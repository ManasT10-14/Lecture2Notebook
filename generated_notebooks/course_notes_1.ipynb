{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2481f3b3",
   "metadata": {},
   "source": [
    "# Week 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddbbb11b",
   "metadata": {},
   "source": [
    "## Lesson 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d9585c0",
   "metadata": {},
   "source": [
    "**Explain the core concepts of the lesson**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38ba5c48",
   "metadata": {},
   "source": [
    "## Core Concepts of Deep Learning\n",
    "\n",
    "Deep learning is a subset of machine learning that uses neural networks with multiple layers to learn hierarchical representations of data. The key concepts in this specialization include:\n",
    "\n",
    "**Neural Networks**: Computational models inspired by biological neurons that learn patterns from data through interconnected layers of nodes.\n",
    "\n",
    "**Deep Neural Networks**: Neural networks with multiple hidden layers that can learn complex, non-linear relationships in data.\n",
    "\n",
    "**Hyperparameter Tuning**: The process of selecting optimal values for parameters that control the learning process, such as learning rate, batch size, and number of layers.\n",
    "\n",
    "**Regularization**: Techniques used to prevent overfitting by constraining model complexity, including L1/L2 regularization and dropout.\n",
    "\n",
    "**Optimization Algorithms**: Methods for updating network weights during training, such as gradient descent and its variants.\n",
    "\n",
    "**Machine Learning Project Strategy**: A systematic approach to structuring ML projects, including problem definition, data preparation, model selection, and evaluation.\n",
    "\n",
    "**Data Splitting**: Dividing data into training, validation, and test sets to properly evaluate model performance.\n",
    "\n",
    "**End-to-end Deep Learning**: Building complete systems that integrate data preprocessing, model training, and deployment.\n",
    "\n",
    "**Convolutional Neural Networks (CNNs)**: Specialized neural networks designed for processing grid-like data, particularly images, using convolutional layers.\n",
    "\n",
    "**Sequence Models**: Neural architectures designed to process sequential data, such as time series or text.\n",
    "\n",
    "**Natural Language Processing (NLP)**: The application of deep learning to understand and generate human language."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9e476a3",
   "metadata": {},
   "source": [
    "**Explain intuitions and mental models for the lesson**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27057ef3",
   "metadata": {},
   "source": [
    "## Intuitions and Mental Models\n",
    "\n",
    "**AI as the New Electricity**: Deep learning and artificial intelligence are poised to transform industries and society in ways comparable to the electrification revolution. Just as electricity became a foundational technology enabling countless innovations, AI will reshape how we work, create, and solve problems across every sector.\n",
    "\n",
    "**Deep Learning as a Rapidly Rising Field**: Deep learning is one of the fastest-growing areas in machine learning and artificial intelligence. The combination of increased computational power, larger datasets, and algorithmic innovations has made deep learning increasingly practical and powerful for solving real-world problems.\n",
    "\n",
    "**Deep Learning Tools Enable New Products and Businesses**: The availability of accessible deep learning frameworks and pre-trained models has democratized AI development. These tools empower developers and entrepreneurs to create novel products and business solutions that were previously impossible, from computer vision applications to natural language understanding systems."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2832d94",
   "metadata": {},
   "source": [
    "**Implement code primitive: Build a neural network**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d2f7daa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class NeuralNetwork:\n",
    "    def __init__(self, layer_sizes):\n",
    "        self.layer_sizes = layer_sizes\n",
    "        self.weights = []\n",
    "        self.biases = []\n",
    "        \n",
    "        for i in range(len(layer_sizes) - 1):\n",
    "            w = np.random.randn(layer_sizes[i], layer_sizes[i+1]) * 0.01\n",
    "            b = np.zeros((1, layer_sizes[i+1]))\n",
    "            self.weights.append(w)\n",
    "            self.biases.append(b)\n",
    "    \n",
    "    def forward(self, X):\n",
    "        self.activations = [X]\n",
    "        A = X\n",
    "        \n",
    "        for i in range(len(self.weights)):\n",
    "            Z = np.dot(A, self.weights[i]) + self.biases[i]\n",
    "            A = np.maximum(0, Z) if i < len(self.weights) - 1 else 1 / (1 + np.exp(-Z))\n",
    "            self.activations.append(A)\n",
    "        \n",
    "        return A\n",
    "    \n",
    "    def backward(self, y, learning_rate):\n",
    "        m = y.shape[0]\n",
    "        dA = self.activations[-1] - y\n",
    "        \n",
    "        for i in range(len(self.weights) - 1, -1, -1):\n",
    "            dW = np.dot(self.activations[i].T, dA) / m\n",
    "            db = np.sum(dA, axis=0, keepdims=True) / m\n",
    "            \n",
    "            if i > 0:\n",
    "                dA = np.dot(dA, self.weights[i].T)\n",
    "                dA[self.activations[i] <= 0] = 0\n",
    "            \n",
    "            self.weights[i] -= learning_rate * dW\n",
    "            self.biases[i] -= learning_rate * db\n",
    "\n",
    "nn = NeuralNetwork([784, 128, 64, 10])\n",
    "X_sample = np.random.randn(32, 784)\n",
    "output = nn.forward(X_sample)\n",
    "print(f\"Output shape: {output.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4667aefc",
   "metadata": {},
   "source": [
    "**Implement code primitive: Train a neural network on data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f027f394",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "digits = load_digits()\n",
    "X = digits.data\n",
    "y = np.eye(10)[digits.target]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "nn = NeuralNetwork([64, 32, 16, 10])\n",
    "\n",
    "epochs = 100\n",
    "learning_rate = 0.01\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    output = nn.forward(X_train)\n",
    "    loss = -np.mean(y_train * np.log(output + 1e-8))\n",
    "    nn.backward(y_train, learning_rate)\n",
    "    \n",
    "    if (epoch + 1) % 20 == 0:\n",
    "        print(f\"Epoch {epoch + 1}/{epochs}, Loss: {loss:.4f}\")\n",
    "\n",
    "test_output = nn.forward(X_test)\n",
    "test_predictions = np.argmax(test_output, axis=1)\n",
    "test_accuracy = np.mean(test_predictions == np.argmax(y_test, axis=1))\n",
    "print(f\"Test Accuracy: {test_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74b37d3f",
   "metadata": {},
   "source": [
    "**Implement code primitive: Build a deep neural network to recognize cats**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93aebbbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class DeepNeuralNetwork:\n",
    "    def __init__(self, layer_sizes):\n",
    "        self.layer_sizes = layer_sizes\n",
    "        self.weights = []\n",
    "        self.biases = []\n",
    "        self.layer_count = len(layer_sizes) - 1\n",
    "        \n",
    "        for i in range(self.layer_count):\n",
    "            w = np.random.randn(layer_sizes[i], layer_sizes[i+1]) * np.sqrt(2.0 / layer_sizes[i])\n",
    "            b = np.zeros((1, layer_sizes[i+1]))\n",
    "            self.weights.append(w)\n",
    "            self.biases.append(b)\n",
    "    \n",
    "    def relu(self, Z):\n",
    "        return np.maximum(0, Z)\n",
    "    \n",
    "    def relu_derivative(self, Z):\n",
    "        return (Z > 0).astype(float)\n",
    "    \n",
    "    def sigmoid(self, Z):\n",
    "        return 1 / (1 + np.exp(-np.clip(Z, -500, 500)))\n",
    "    \n",
    "    def forward(self, X):\n",
    "        self.Z_values = []\n",
    "        self.A_values = [X]\n",
    "        A = X\n",
    "        \n",
    "        for i in range(self.layer_count):\n",
    "            Z = np.dot(A, self.weights[i]) + self.biases[i]\n",
    "            self.Z_values.append(Z)\n",
    "            \n",
    "            if i < self.layer_count - 1:\n",
    "                A = self.relu(Z)\n",
    "            else:\n",
    "                A = self.sigmoid(Z)\n",
    "            \n",
    "            self.A_values.append(A)\n",
    "        \n",
    "        return A\n",
    "    \n",
    "    def backward(self, y, learning_rate):\n",
    "        m = y.shape[0]\n",
    "        dA = self.A_values[-1] - y\n",
    "        \n",
    "        for i in range(self.layer_count - 1, -1, -1):\n",
    "            dZ = dA * (self.sigmoid(self.Z_values[i]) * (1 - self.sigmoid(self.Z_values[i])) if i == self.layer_count - 1 else self.relu_derivative(self.Z_values[i]))\n",
    "            dW = np.dot(self.A_values[i].T, dZ) / m\n",
    "            db = np.sum(dZ, axis=0, keepdims=True) / m\n",
    "            \n",
    "            if i > 0:\n",
    "                dA = np.dot(dZ, self.weights[i].T)\n",
    "            \n",
    "            self.weights[i] -= learning_rate * dW\n",
    "            self.biases[i] -= learning_rate * db\n",
    "\n",
    "cat_detector = DeepNeuralNetwork([12288, 256, 128, 64, 32, 1])\n",
    "X_sample = np.random.randn(10, 12288)\n",
    "y_sample = np.random.randint(0, 2, (10, 1))\n",
    "predictions = cat_detector.forward(X_sample)\n",
    "print(f\"Predictions shape: {predictions.shape}\")\n",
    "print(f\"Sample predictions: {predictions[:5].flatten()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3000e58",
   "metadata": {},
   "source": [
    "**Implement code primitive: Implement convolutional neural networks**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5afc74c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class ConvolutionalLayer:\n",
    "    def __init__(self, num_filters, filter_size, stride=1, padding=0):\n",
    "        self.num_filters = num_filters\n",
    "        self.filter_size = filter_size\n",
    "        self.stride = stride\n",
    "        self.padding = padding\n",
    "        self.filters = np.random.randn(num_filters, filter_size, filter_size) * 0.01\n",
    "        self.biases = np.zeros((num_filters, 1))\n",
    "    \n",
    "    def convolve(self, X, filter_kernel):\n",
    "        X_padded = np.pad(X, ((self.padding, self.padding), (self.padding, self.padding)), mode='constant')\n",
    "        output_size = (X_padded.shape[0] - self.filter_size) // self.stride + 1\n",
    "        output = np.zeros((output_size, output_size))\n",
    "        \n",
    "        for i in range(0, X_padded.shape[0] - self.filter_size + 1, self.stride):\n",
    "            for j in range(0, X_padded.shape[1] - self.filter_size + 1, self.stride):\n",
    "                patch = X_padded[i:i+self.filter_size, j:j+self.filter_size]\n",
    "                output[i//self.stride, j//self.stride] = np.sum(patch * filter_kernel)\n",
    "        \n",
    "        return output\n",
    "    \n",
    "    def forward(self, X):\n",
    "        self.X = X\n",
    "        batch_size = X.shape[0]\n",
    "        output_size = (X.shape[1] - self.filter_size + 2*self.padding) // self.stride + 1\n",
    "        output = np.zeros((batch_size, output_size, output_size, self.num_filters))\n",
    "        \n",
    "        for b in range(batch_size):\n",
    "            for f in range(self.num_filters):\n",
    "                output[b, :, :, f] = self.convolve(X[b], self.filters[f]) + self.biases[f]\n",
    "        \n",
    "        return output\n",
    "\n",
    "class MaxPoolingLayer:\n",
    "    def __init__(self, pool_size=2, stride=2):\n",
    "        self.pool_size = pool_size\n",
    "        self.stride = stride\n",
    "    \n",
    "    def forward(self, X):\n",
    "        batch_size, height, width, channels = X.shape\n",
    "        output_height = (height - self.pool_size) // self.stride + 1\n",
    "        output_width = (width - self.pool_size) // self.stride + 1\n",
    "        output = np.zeros((batch_size, output_height, output_width, channels))\n",
    "        \n",
    "        for b in range(batch_size):\n",
    "            for c in range(channels):\n",
    "                for i in range(output_height):\n",
    "                    for j in range(output_width):\n",
    "                        h_start = i * self.stride\n",
    "                        w_start = j * self.stride\n",
    "                        patch = X[b, h_start:h_start+self.pool_size, w_start:w_start+self.pool_size, c]\n",
    "                        output[b, i, j, c] = np.max(patch)\n",
    "        \n",
    "        return output\n",
    "\n",
    "conv_layer = ConvolutionalLayer(num_filters=32, filter_size=3, stride=1, padding=1)\n",
    "pool_layer = MaxPoolingLayer(pool_size=2, stride=2)\n",
    "\n",
    "X_input = np.random.randn(4, 28, 28, 1)\n",
    "conv_output = conv_layer.forward(X_input)\n",
    "pool_output = pool_layer.forward(conv_output)\n",
    "\n",
    "print(f\"Input shape: {X_input.shape}\")\n",
    "print(f\"Conv output shape: {conv_output.shape}\")\n",
    "print(f\"Pool output shape: {pool_output.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "849a4b99",
   "metadata": {},
   "source": [
    "**Apply sequence models to natural language processing problems**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18ecd439",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class SimpleRNN:\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        \n",
    "        self.Wxh = np.random.randn(input_size, hidden_size) * 0.01\n",
    "        self.Whh = np.random.randn(hidden_size, hidden_size) * 0.01\n",
    "        self.Why = np.random.randn(hidden_size, output_size) * 0.01\n",
    "        self.bh = np.zeros((1, hidden_size))\n",
    "        self.by = np.zeros((1, output_size))\n",
    "    \n",
    "    def forward(self, X):\n",
    "        batch_size, seq_length, _ = X.shape\n",
    "        h = np.zeros((batch_size, self.hidden_size))\n",
    "        outputs = []\n",
    "        \n",
    "        for t in range(seq_length):\n",
    "            h = np.tanh(np.dot(X[:, t, :], self.Wxh) + np.dot(h, self.Whh) + self.bh)\n",
    "            y = np.dot(h, self.Why) + self.by\n",
    "            outputs.append(y)\n",
    "        \n",
    "        return np.array(outputs), h\n",
    "    \n",
    "    def predict(self, X):\n",
    "        outputs, _ = self.forward(X)\n",
    "        return np.argmax(outputs[-1], axis=1)\n",
    "\n",
    "class LSTMCell:\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        self.Wf = np.random.randn(input_size + hidden_size, hidden_size) * 0.01\n",
    "        self.Wi = np.random.randn(input_size + hidden_size, hidden_size) * 0.01\n",
    "        self.Wo = np.random.randn(input_size + hidden_size, hidden_size) * 0.01\n",
    "        self.Wc = np.random.randn(input_size + hidden_size, hidden_size) * 0.01\n",
    "        \n",
    "        self.bf = np.zeros((1, hidden_size))\n",
    "        self.bi = np.zeros((1, hidden_size))\n",
    "        self.bo = np.zeros((1, hidden_size))\n",
    "        self.bc = np.zeros((1, hidden_size))\n",
    "    \n",
    "    def forward(self, X, h_prev, c_prev):\n",
    "        combined = np.concatenate([X, h_prev], axis=1)\n",
    "        \n",
    "        f = 1 / (1 + np.exp(-np.dot(combined, self.Wf) - self.bf))\n",
    "        i = 1 / (1 + np.exp(-np.dot(combined, self.Wi) - self.bi))\n",
    "        o = 1 / (1 + np.exp(-np.dot(combined, self.Wo) - self.bo))\n",
    "        c_tilde = np.tanh(np.dot(combined, self.Wc) + self.bc)\n",
    "        \n",
    "        c = f * c_prev + i * c_tilde\n",
    "        h = o * np.tanh(c)\n",
    "        \n",
    "        return h, c\n",
    "\n",
    "rnn = SimpleRNN(input_size=100, hidden_size=50, output_size=10)\n",
    "X_seq = np.random.randn(8, 20, 100)\n",
    "outputs, final_h = rnn.forward(X_seq)\n",
    "print(f\"RNN outputs shape: {outputs.shape}\")\n",
    "print(f\"Final hidden state shape: {final_h.shape}\")\n",
    "\n",
    "lstm = LSTMCell(input_size=100, hidden_size=50)\n",
    "X_t = np.random.randn(8, 100)\n",
    "h_t, c_t = lstm.forward(X_t, np.zeros((8, 50)), np.zeros((8, 50)))\n",
    "print(f\"LSTM hidden state shape: {h_t.shape}\")\n",
    "print(f\"LSTM cell state shape: {c_t.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11e2b329",
   "metadata": {},
   "source": [
    "**Create a Mermaid diagram: graph TD\n",
    "A[Course 1: Neural Network Foundations] --> B[Course 2: Practical Deep Learning]\n",
    "B --> C[Course 3: ML Project Structuring]\n",
    "C --> D[Course 4: Convolutional Neural Networks]\n",
    "D --> E[Course 5: Sequence Models]**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcdbe4a8",
   "metadata": {},
   "source": [
    "## Deep Learning Specialization Curriculum\n",
    "\n",
    "```mermaid\n",
    "graph TD\n",
    "    A[Course 1: Neural Network Foundations] --> B[Course 2: Practical Deep Learning]\n",
    "    B --> C[Course 3: ML Project Structuring]\n",
    "    C --> D[Course 4: Convolutional Neural Networks]\n",
    "    D --> E[Course 5: Sequence Models]\n",
    "```\n",
    "\n",
    "This specialization provides a comprehensive pathway through deep learning, starting with foundational neural network concepts and progressing through practical applications, project management, specialized architectures, and advanced sequence modeling techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6951b4e2",
   "metadata": {},
   "source": [
    "## Lesson 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f651e512",
   "metadata": {},
   "source": [
    "**Explain the core concepts of the lesson**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "780ed7b4",
   "metadata": {},
   "source": [
    "## Core Concepts\n",
    "\n",
    "A **neural network** is a computational model inspired by how biological neurons work. At its core, a neural network learns to map inputs (features) to outputs (predictions) through layers of interconnected units.\n",
    "\n",
    "Key concepts in this lesson:\n",
    "\n",
    "- **Single Neuron**: The simplest building block of a neural network. It takes one or more inputs and produces an output through a mathematical function.\n",
    "\n",
    "- **ReLU Function (Rectified Linear Unit)**: A common activation function defined as $\\text{ReLU}(z) = \\max(0, z)$. It outputs the input if positive, and zero otherwise. This introduces non-linearity and helps neural networks learn complex patterns.\n",
    "\n",
    "- **Input Features**: The raw data fed into a neural network (e.g., house size, number of bedrooms, zip code, wealth).\n",
    "\n",
    "- **Hidden Units**: Intermediate neurons in a neural network that learn abstract representations of the input features. These units automatically discover useful intermediate concepts like \"family size\" or \"walkability.\"\n",
    "\n",
    "- **Densely Connected Layers**: Layers where every neuron in one layer connects to every neuron in the next layer. This allows information to flow and be transformed through the network.\n",
    "\n",
    "- **Supervised Learning**: The process of training a neural network using labeled data (inputs paired with correct outputs) to learn the mapping from inputs to outputs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7aea409",
   "metadata": {},
   "source": [
    "**Explain intuitions and mental models for the lesson**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec31c28b",
   "metadata": {},
   "source": [
    "## Intuitions and Mental Models\n",
    "\n",
    "**Simple Functions as Neural Networks**: A simple function that maps house size to price can be thought of as the most basic neural network—a single neuron with a ReLU activation. This shows that neural networks are not mysterious; they are just functions that learn from data.\n",
    "\n",
    "**Building Complexity with Simplicity**: Larger neural networks are constructed by combining many simple neurons, much like stacking Lego bricks. Each neuron performs a simple operation, but when combined in layers, they create powerful models capable of learning complex relationships.\n",
    "\n",
    "**Automatic Feature Learning**: One of the most powerful aspects of neural networks is their ability to automatically learn intermediate features from raw inputs. Rather than manually engineering features, a neural network discovers useful representations. For example, from basic house attributes (size, bedrooms, zip code, wealth), a neural network can learn abstract concepts like \"family size,\" \"walkability,\" or \"school quality\" in its hidden layers.\n",
    "\n",
    "**Learning with Data**: With sufficient training data, neural networks excel at discovering accurate functions to map inputs (X) to desired outputs (Y). The network learns by adjusting its internal parameters (weights) to minimize prediction errors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cb12a09",
   "metadata": {},
   "source": [
    "**Present and explain the key equations used in the lesson**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec418c58",
   "metadata": {},
   "source": [
    "## Key Equations\n",
    "\n",
    "**ReLU Activation Function**:\n",
    "\n",
    "$$\\text{ReLU}(z) = \\max(0, z)$$\n",
    "\n",
    "This function is fundamental to modern neural networks. It takes any input $z$ and outputs the maximum of zero and $z$. This means:\n",
    "- If $z > 0$, the output is $z$\n",
    "- If $z \\leq 0$, the output is $0$\n",
    "\n",
    "The ReLU function introduces non-linearity, allowing neural networks to learn complex, non-linear relationships between inputs and outputs. It also helps with efficient training through better gradient flow during backpropagation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4984202",
   "metadata": {},
   "source": [
    "**Implement code primitive: Implement a function that predicts house price based on size, incorporating a floor at zero (similar to a ReLU function).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de7f89e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_price_from_size(size, weight=200, bias=50000):\n",
    "    \"\"\"\n",
    "    Predicts house price based on size using a ReLU-like function.\n",
    "    \n",
    "    Args:\n",
    "        size: House size in square feet\n",
    "        weight: Price per square foot\n",
    "        bias: Base price\n",
    "    \n",
    "    Returns:\n",
    "        Predicted price (floored at zero)\n",
    "    \"\"\"\n",
    "    linear_output = weight * size + bias\n",
    "    price = max(0, linear_output)  # ReLU: floor at zero\n",
    "    return price\n",
    "\n",
    "# Example usage\n",
    "size_1 = 2000\n",
    "price_1 = predict_price_from_size(size_1)\n",
    "print(f\"House size: {size_1} sq ft, Predicted price: ${price_1:,.0f}\")\n",
    "\n",
    "size_2 = 3000\n",
    "price_2 = predict_price_from_size(size_2)\n",
    "print(f\"House size: {size_2} sq ft, Predicted price: ${price_2:,.0f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69da8733",
   "metadata": {},
   "source": [
    "**Implement code primitive: Demonstrate a neural network taking multiple input features (size, number of bedrooms, zip code, wealth) to predict house price.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "314ada73",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class SimpleHousingNeuralNetwork:\n",
    "    def __init__(self):\n",
    "        # Hidden layer weights and biases (4 inputs -> 3 hidden units)\n",
    "        self.w_hidden = np.array([\n",
    "            [0.5, 0.3, 0.2],      # weights from size\n",
    "            [0.4, 0.6, 0.1],      # weights from bedrooms\n",
    "            [0.2, 0.4, 0.7],      # weights from zip code\n",
    "            [0.3, 0.2, 0.5]       # weights from wealth\n",
    "        ])\n",
    "        self.b_hidden = np.array([10000, 5000, 8000])\n",
    "        \n",
    "        # Output layer weights and biases (3 hidden units -> 1 output)\n",
    "        self.w_output = np.array([[300], [250], [200]])\n",
    "        self.b_output = np.array([50000])\n",
    "    \n",
    "    def relu(self, z):\n",
    "        return np.maximum(0, z)\n",
    "    \n",
    "    def predict(self, size, bedrooms, zip_code, wealth):\n",
    "        # Input features\n",
    "        x = np.array([size, bedrooms, zip_code, wealth])\n",
    "        \n",
    "        # Hidden layer\n",
    "        z_hidden = np.dot(x, self.w_hidden) + self.b_hidden\n",
    "        a_hidden = self.relu(z_hidden)\n",
    "        \n",
    "        # Output layer\n",
    "        z_output = np.dot(a_hidden, self.w_output) + self.b_output\n",
    "        price = self.relu(z_output)[0]\n",
    "        \n",
    "        return price\n",
    "\n",
    "# Example usage\n",
    "network = SimpleHousingNeuralNetwork()\n",
    "\n",
    "# Predict price for a house\n",
    "size = 2500\n",
    "bedrooms = 4\n",
    "zip_code = 95000\n",
    "wealth = 100000\n",
    "\n",
    "predicted_price = network.predict(size, bedrooms, zip_code, wealth)\n",
    "print(f\"Input: Size={size} sq ft, Bedrooms={bedrooms}, Zip={zip_code}, Wealth=${wealth}\")\n",
    "print(f\"Predicted Price: ${predicted_price:,.0f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82580187",
   "metadata": {},
   "source": [
    "**Create a Mermaid diagram: graph TD\n",
    "    A[Size] --> B{Single Neuron/ReLU Function}\n",
    "    B --> C[Price]**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54a18c5a",
   "metadata": {},
   "source": [
    "```mermaid\n",
    "graph TD\n",
    "    A[Size] --> B{Single Neuron/ReLU Function}\n",
    "    B --> C[Price]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc80f325",
   "metadata": {},
   "source": [
    "**Create a Mermaid diagram: graph TD\n",
    "    subgraph Input Layer\n",
    "        A[Size]\n",
    "        B[#Bedrooms]\n",
    "        C[Zip Code]\n",
    "        D[Wealth]\n",
    "    end\n",
    "    subgraph Hidden Layer\n",
    "        E[Family Size]\n",
    "        F[Walkability]\n",
    "        G[School Quality]\n",
    "    end\n",
    "    subgraph Output Layer\n",
    "        H[Price]\n",
    "    end\n",
    "    A -- connects to --> E\n",
    "    A -- connects to --> F\n",
    "    A -- connects to --> G\n",
    "    B -- connects to --> E\n",
    "    B -- connects to --> F\n",
    "    B -- connects to --> G\n",
    "    C -- connects to --> E\n",
    "    C -- connects to --> F\n",
    "    C -- connects to --> G\n",
    "    D -- connects to --> E\n",
    "    D -- connects to --> F\n",
    "    D -- connects to --> G\n",
    "    E --> H\n",
    "    F --> H\n",
    "    G --> H**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d5d2725",
   "metadata": {},
   "source": [
    "```mermaid\n",
    "graph TD\n",
    "    subgraph Input Layer\n",
    "        A[Size]\n",
    "        B[#Bedrooms]\n",
    "        C[Zip Code]\n",
    "        D[Wealth]\n",
    "    end\n",
    "    subgraph Hidden Layer\n",
    "        E[Family Size]\n",
    "        F[Walkability]\n",
    "        G[School Quality]\n",
    "    end\n",
    "    subgraph Output Layer\n",
    "        H[Price]\n",
    "    end\n",
    "    A --> E\n",
    "    A --> F\n",
    "    A --> G\n",
    "    B --> E\n",
    "    B --> F\n",
    "    B --> G\n",
    "    C --> E\n",
    "    C --> F\n",
    "    C --> G\n",
    "    D --> E\n",
    "    D --> F\n",
    "    D --> G\n",
    "    E --> H\n",
    "    F --> H\n",
    "    G --> H\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5056bd6",
   "metadata": {},
   "source": [
    "## Lesson 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c1e9b16",
   "metadata": {},
   "source": [
    "**Explain the core concepts of the lesson**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0a0563d",
   "metadata": {},
   "source": [
    "## Core Concepts\n",
    "\n",
    "**Supervised Learning** is the process of training neural networks to learn a mapping function from input data to desired output data. This is the primary application driving economic value from neural networks.\n",
    "\n",
    "In supervised learning, we define:\n",
    "- **Input (x)**: The data we provide to the neural network\n",
    "- **Output (y)**: The desired result or target we want the network to predict\n",
    "\n",
    "The neural network learns to approximate the function $f(x) \\approx y$ by adjusting its internal parameters during training.\n",
    "\n",
    "**Neural Network Architectures** are specialized designs optimized for different types of data:\n",
    "- **Standard Neural Network**: Used for structured data (tabular, numerical features)\n",
    "- **Convolutional Neural Networks (CNN)**: Specialized for image data\n",
    "- **Recurrent Neural Networks (RNN)**: Designed for sequence data (audio, text, time series)\n",
    "- **Hybrid Neural Networks**: Custom architectures combining multiple approaches for complex or mixed data types\n",
    "\n",
    "The choice of architecture depends critically on the nature of the problem and the type of data being processed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "193bda40",
   "metadata": {},
   "source": [
    "**Explain intuitions and mental models for the lesson**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87466eba",
   "metadata": {},
   "source": [
    "## Intuitions and Mental Models\n",
    "\n",
    "**Supervised Learning as Function Approximation**: Think of supervised learning as teaching a neural network to mimic a hidden function. You provide examples of inputs and their corresponding outputs, and the network learns the pattern connecting them. This is fundamentally different from unsupervised learning, where no target outputs are provided.\n",
    "\n",
    "**The Importance of Defining x and y**: Before applying supervised learning to any problem, you must carefully define what your inputs (x) and outputs (y) represent. Poor definitions lead to poor results, regardless of how powerful your neural network is. This is a critical first step in any machine learning project.\n",
    "\n",
    "**Architecture Follows Data Type**: Neural networks are not one-size-fits-all. Different data types have different structures and properties:\n",
    "- Structured data is already organized in a way standard networks can process\n",
    "- Images have spatial structure that CNNs exploit through convolution operations\n",
    "- Sequences have temporal or sequential dependencies that RNNs capture through recurrence\n",
    "\n",
    "**Unlocking Unstructured Data**: Neural networks have revolutionized how computers process unstructured data (images, audio, text). This capability has opened entirely new application domains and is a major source of modern AI's economic impact."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a042876",
   "metadata": {},
   "source": [
    "**Create a Mermaid diagram: graph TD\n",
    "    A[Input X] --> B{Neural Network}\n",
    "    B --> C[Output Y]\n",
    "    style A fill:#f9f,stroke:#333,stroke-width:2px\n",
    "    style B fill:#bbf,stroke:#333,stroke-width:2px\n",
    "    style C fill:#f9f,stroke:#333,stroke-width:2px**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bf52957",
   "metadata": {},
   "source": [
    "## Supervised Learning Framework\n",
    "\n",
    "```mermaid\n",
    "graph TD\n",
    "    A[Input X] --> B{Neural Network}\n",
    "    B --> C[Output Y]\n",
    "    style A fill:#f9f,stroke:#333,stroke-width:2px\n",
    "    style B fill:#bbf,stroke:#333,stroke-width:2px\n",
    "    style C fill:#f9f,stroke:#333,stroke-width:2px\n",
    "```\n",
    "\n",
    "This diagram illustrates the fundamental supervised learning pipeline: the neural network takes input data (x) and produces predicted output data (y)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2358fe2",
   "metadata": {},
   "source": [
    "**Create a Mermaid diagram: graph TD\n",
    "    subgraph Data Type to NN Architecture\n",
    "        A[Structured Data] --> SNN(Standard Neural Network)\n",
    "        B[Image Data] --> CNN(Convolutional Neural Network)\n",
    "        C[Sequence Data (Audio, Text)] --> RNN(Recurrent Neural Network)\n",
    "        D[Complex/Hybrid Data] --> HNN(Custom/Hybrid Neural Network)\n",
    "    end\n",
    "    style A fill:#f9f,stroke:#333,stroke-width:2px\n",
    "    style B fill:#f9f,stroke:#333,stroke-width:2px\n",
    "    style C fill:#f9f,stroke:#333,stroke-width:2px\n",
    "    style D fill:#f9f,stroke:#333,stroke-width:2px\n",
    "    style SNN fill:#bbf,stroke:#333,stroke-width:2px\n",
    "    style CNN fill:#bbf,stroke:#333,stroke-width:2px\n",
    "    style RNN fill:#bbf,stroke:#333,stroke-width:2px\n",
    "    style HNN fill:#bbf,stroke:#333,stroke-width:2px**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a4b1ff1",
   "metadata": {},
   "source": [
    "## Matching Data Types to Neural Network Architectures\n",
    "\n",
    "```mermaid\n",
    "graph TD\n",
    "    subgraph Data Type to NN Architecture\n",
    "        A[Structured Data] --> SNN(Standard Neural Network)\n",
    "        B[Image Data] --> CNN(Convolutional Neural Network)\n",
    "        C[Sequence Data (Audio, Text)] --> RNN(Recurrent Neural Network)\n",
    "        D[Complex/Hybrid Data] --> HNN(Custom/Hybrid Neural Network)\n",
    "    end\n",
    "    style A fill:#f9f,stroke:#333,stroke-width:2px\n",
    "    style B fill:#f9f,stroke:#333,stroke-width:2px\n",
    "    style C fill:#f9f,stroke:#333,stroke-width:2px\n",
    "    style D fill:#f9f,stroke:#333,stroke-width:2px\n",
    "    style SNN fill:#bbf,stroke:#333,stroke-width:2px\n",
    "    style CNN fill:#bbf,stroke:#333,stroke-width:2px\n",
    "    style RNN fill:#bbf,stroke:#333,stroke-width:2px\n",
    "    style HNN fill:#bbf,stroke:#333,stroke-width:2px\n",
    "```\n",
    "\n",
    "The most effective neural network architecture depends on the nature of your data. This diagram shows the primary mapping between data types and specialized architectures designed to handle them efficiently."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5e712a8",
   "metadata": {},
   "source": [
    "## Lesson 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99b4d16d",
   "metadata": {},
   "source": [
    "**Explain the core concepts of the lesson**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95da57f6",
   "metadata": {},
   "source": [
    "## Core Concepts\n",
    "\n",
    "Deep learning's rise has been driven by three fundamental factors:\n",
    "\n",
    "1. **Data Scale**: The availability of massive amounts of labeled data has enabled neural networks to learn complex patterns that traditional algorithms cannot capture.\n",
    "\n",
    "2. **Computation Scale**: Advances in computational power (GPUs, TPUs) have made it feasible to train very large neural networks on large datasets.\n",
    "\n",
    "3. **Neural Network Size**: Larger neural networks have the capacity to leverage vast amounts of data to continuously improve performance, unlike traditional algorithms which plateau.\n",
    "\n",
    "These three factors work together synergistically. Traditional machine learning algorithms eventually reach a performance plateau regardless of how much additional data is provided. In contrast, large neural networks can continue to improve their performance as more data becomes available. This fundamental difference is a key driver of deep learning's success in modern applications."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "512fe17e",
   "metadata": {},
   "source": [
    "**Explain intuitions and mental models for the lesson**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48d3e326",
   "metadata": {},
   "source": [
    "## Intuitions and Mental Models\n",
    "\n",
    "**The Plateau Problem**: Traditional learning algorithms hit a ceiling. No matter how much data you feed them, their performance stops improving. It's like trying to fill a cup that's already full—adding more water doesn't help.\n",
    "\n",
    "**The Scaling Advantage**: Large neural networks are different. They're like containers with no ceiling. As you provide more data, they continue to improve. The more data you have, the better they become.\n",
    "\n",
    "**The Virtuous Cycle**: Deep learning's success comes from combining three elements:\n",
    "- Abundant data (the fuel)\n",
    "- Large neural networks (the engine)\n",
    "- Computational power (the infrastructure)\n",
    "\n",
    "When these three align, neural networks can learn representations that traditional algorithms simply cannot discover.\n",
    "\n",
    "**Rapid Experimentation**: Faster computation enables quicker iteration. You can try an idea, run an experiment, analyze results, and refine your approach in hours instead of days. This speed of experimentation accelerates discovery and innovation.\n",
    "\n",
    "**Small Changes, Big Impact**: Even seemingly minor algorithmic changes—like switching from sigmoid to ReLU activation functions—can dramatically improve both training speed and model performance. These innovations compound over time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "872cbd5b",
   "metadata": {},
   "source": [
    "**Present and explain the key equations used in the lesson**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d928585",
   "metadata": {},
   "source": [
    "## Key Equations and Notation\n",
    "\n",
    "**Training Set Size**\n",
    "\n",
    "$$m = \\text{number of training examples}$$\n",
    "\n",
    "The variable $m$ represents the size of your training dataset. This is a fundamental parameter in deep learning because the relationship between $m$ and model performance differs dramatically between traditional algorithms and neural networks:\n",
    "\n",
    "- **Traditional algorithms**: Performance plateaus as $m$ increases beyond a certain point\n",
    "- **Large neural networks**: Performance continues to improve as $m$ increases\n",
    "\n",
    "This difference in how performance scales with $m$ is central to understanding why deep learning has become so dominant in modern machine learning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "454bf1ba",
   "metadata": {},
   "source": [
    "**Implement code primitive: Implementing a specific neural network architecture for a given task.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46601469",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class SimpleNeuralNetwork:\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        self.W1 = np.random.randn(input_size, hidden_size) * 0.01\n",
    "        self.b1 = np.zeros((1, hidden_size))\n",
    "        self.W2 = np.random.randn(hidden_size, output_size) * 0.01\n",
    "        self.b2 = np.zeros((1, output_size))\n",
    "    \n",
    "    def forward(self, X):\n",
    "        self.z1 = np.dot(X, self.W1) + self.b1\n",
    "        self.a1 = np.maximum(0, self.z1)\n",
    "        self.z2 = np.dot(self.a1, self.W2) + self.b2\n",
    "        self.a2 = 1 / (1 + np.exp(-self.z2))\n",
    "        return self.a2\n",
    "    \n",
    "    def backward(self, X, y, learning_rate=0.01):\n",
    "        m = X.shape[0]\n",
    "        dz2 = self.a2 - y\n",
    "        dW2 = np.dot(self.a1.T, dz2) / m\n",
    "        db2 = np.sum(dz2, axis=0, keepdims=True) / m\n",
    "        da1 = np.dot(dz2, self.W2.T)\n",
    "        dz1 = da1 * (self.z1 > 0)\n",
    "        dW1 = np.dot(X.T, dz1) / m\n",
    "        db1 = np.sum(dz1, axis=0, keepdims=True) / m\n",
    "        \n",
    "        self.W2 -= learning_rate * dW2\n",
    "        self.b2 -= learning_rate * db2\n",
    "        self.W1 -= learning_rate * dW1\n",
    "        self.b1 -= learning_rate * db1\n",
    "\n",
    "nn = SimpleNeuralNetwork(input_size=10, hidden_size=5, output_size=1)\n",
    "X_sample = np.random.randn(100, 10)\n",
    "output = nn.forward(X_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fd01507",
   "metadata": {},
   "source": [
    "**Implement code primitive: Changing the activation function (e.g., from sigmoid to ReLU) within a neural network's code to improve training speed and performance.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3624c663",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class NeuralNetworkWithActivations:\n",
    "    def __init__(self, input_size, hidden_size, output_size, activation='relu'):\n",
    "        self.activation = activation\n",
    "        self.W1 = np.random.randn(input_size, hidden_size) * 0.01\n",
    "        self.b1 = np.zeros((1, hidden_size))\n",
    "        self.W2 = np.random.randn(hidden_size, output_size) * 0.01\n",
    "        self.b2 = np.zeros((1, output_size))\n",
    "    \n",
    "    def relu(self, z):\n",
    "        return np.maximum(0, z)\n",
    "    \n",
    "    def sigmoid(self, z):\n",
    "        return 1 / (1 + np.exp(-np.clip(z, -500, 500)))\n",
    "    \n",
    "    def forward(self, X):\n",
    "        self.z1 = np.dot(X, self.W1) + self.b1\n",
    "        \n",
    "        if self.activation == 'relu':\n",
    "            self.a1 = self.relu(self.z1)\n",
    "        elif self.activation == 'sigmoid':\n",
    "            self.a1 = self.sigmoid(self.z1)\n",
    "        \n",
    "        self.z2 = np.dot(self.a1, self.W2) + self.b2\n",
    "        self.a2 = self.sigmoid(self.z2)\n",
    "        return self.a2\n",
    "\n",
    "nn_relu = NeuralNetworkWithActivations(input_size=10, hidden_size=5, output_size=1, activation='relu')\n",
    "nn_sigmoid = NeuralNetworkWithActivations(input_size=10, hidden_size=5, output_size=1, activation='sigmoid')\n",
    "\n",
    "X_sample = np.random.randn(100, 10)\n",
    "output_relu = nn_relu.forward(X_sample)\n",
    "output_sigmoid = nn_sigmoid.forward(X_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ca797bb",
   "metadata": {},
   "source": [
    "**Create a Mermaid diagram: graph TD\n",
    "    A[Amount of Data] --> B{Traditional Algorithm};\n",
    "    B --> C{Performance Plateaus};\n",
    "    A --> D{Small Neural Network};\n",
    "    D --> E{Performance Improves (Limited)};\n",
    "    A --> F{Large Neural Network};\n",
    "    F --> G{Performance Keeps Improving};**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d38655c",
   "metadata": {},
   "source": [
    "## Performance Scaling with Data\n",
    "\n",
    "The following diagram illustrates how performance scales differently for traditional algorithms versus neural networks as the amount of data increases:\n",
    "\n",
    "```mermaid\n",
    "graph TD\n",
    "    A[Amount of Data] --> B{Traditional Algorithm}\n",
    "    B --> C{Performance Plateaus}\n",
    "    A --> D{Small Neural Network}\n",
    "    D --> E{Performance Improves Limited}\n",
    "    A --> F{Large Neural Network}\n",
    "    F --> G{Performance Keeps Improving}\n",
    "```\n",
    "\n",
    "This visualization captures a fundamental insight: traditional algorithms reach a performance ceiling, while large neural networks continue to benefit from additional data. This difference is a primary driver of deep learning's dominance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c72b3b2",
   "metadata": {},
   "source": [
    "**Create a Mermaid diagram: graph TD\n",
    "    A[Have Idea] --> B[Implement Idea (Code)];\n",
    "    B --> C[Run Experiment];\n",
    "    C --> D[Analyze Results];\n",
    "    D --> E{Good Performance?};\n",
    "    E -- No --> A;\n",
    "    E -- Yes --> F[Deploy/Conclude];**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03abce55",
   "metadata": {},
   "source": [
    "## The Iterative Experimentation Cycle\n",
    "\n",
    "Modern deep learning development follows a rapid iterative cycle. Faster computational power enables researchers and practitioners to cycle through this loop quickly, accelerating innovation:\n",
    "\n",
    "```mermaid\n",
    "graph TD\n",
    "    A[Have Idea] --> B[Implement Idea Code]\n",
    "    B --> C[Run Experiment]\n",
    "    C --> D[Analyze Results]\n",
    "    D --> E{Good Performance?}\n",
    "    E -->|No| A\n",
    "    E -->|Yes| F[Deploy/Conclude]\n",
    "```\n",
    "\n",
    "This cycle of ideation, implementation, experimentation, and analysis is fundamental to deep learning development. The speed at which you can complete each iteration directly impacts how quickly you can discover effective solutions. Advances in computation have dramatically reduced the time per iteration, enabling more experiments and faster progress."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd3b1f14",
   "metadata": {},
   "source": [
    "## Lesson 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e47d889a",
   "metadata": {},
   "source": [
    "**Explain the core concepts of the lesson**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3308bf9",
   "metadata": {},
   "source": [
    "## Core Concepts\n",
    "\n",
    "This lesson introduces the foundational building blocks of deep learning. The key concepts you will learn are:\n",
    "\n",
    "**Deep Neural Networks**: Multi-layered networks that learn hierarchical representations of data through stacked layers of neurons.\n",
    "\n",
    "**Neural Network Programming**: The practical implementation of neural network architectures using code, enabling you to build and deploy models.\n",
    "\n",
    "**Forward Propagation**: The process of computing predictions by passing input data through the network layers sequentially, from input to output.\n",
    "\n",
    "**Back Propagation**: The algorithm for computing gradients of the loss function with respect to network parameters, enabling efficient learning through gradient descent.\n",
    "\n",
    "**Single Hidden Layer Networks**: Neural networks with one intermediate layer between input and output, serving as a foundation for understanding deeper architectures.\n",
    "\n",
    "**Many Layer Networks**: Deep neural networks with multiple hidden layers that can learn more complex patterns and representations.\n",
    "\n",
    "These concepts form the essential foundation for understanding and implementing deep learning models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4256bdf",
   "metadata": {},
   "source": [
    "**Explain intuitions and mental models for the lesson**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2989fa8e",
   "metadata": {},
   "source": [
    "## Intuitions and Mental Models\n",
    "\n",
    "**Building Blocks of Deep Learning**: This course teaches the foundational building blocks of deep learning. Think of neural networks as modular components that you can combine and extend to solve increasingly complex problems.\n",
    "\n",
    "**Learning Through Implementation**: The primary goal of the first course is to learn to build and deploy a deep neural network. Rather than just understanding theory, you will gain practical experience by constructing networks from scratch.\n",
    "\n",
    "**Hands-On Programming Solidifies Understanding**: Hands-on programming exercises help solidify understanding of algorithms by implementing them. When you code forward propagation and back propagation yourself, you develop intuition for how these algorithms work and why they matter.\n",
    "\n",
    "**From Simple to Complex**: Start with single hidden layer networks to understand the basics, then extend to many-layer networks as you build confidence and understanding.\n",
    "\n",
    "**The Learning Loop**: Forward propagation computes predictions, back propagation computes how to improve those predictions, and this cycle repeats to train the network."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88aa88e9",
   "metadata": {},
   "source": [
    "**Implement code primitive: Implement neural network algorithms efficiently.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f8062ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def sigmoid(z):\n",
    "    \"\"\"Sigmoid activation function\"\"\"\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "def relu(z):\n",
    "    \"\"\"ReLU activation function\"\"\"\n",
    "    return np.maximum(0, z)\n",
    "\n",
    "def sigmoid_derivative(z):\n",
    "    \"\"\"Derivative of sigmoid function\"\"\"\n",
    "    s = sigmoid(z)\n",
    "    return s * (1 - s)\n",
    "\n",
    "def relu_derivative(z):\n",
    "    \"\"\"Derivative of ReLU function\"\"\"\n",
    "    return (z > 0).astype(float)\n",
    "\n",
    "print(\"Neural network activation functions implemented efficiently.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a546606",
   "metadata": {},
   "source": [
    "**Implement code primitive: Code a single hidden layer neural network.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3b55269",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SingleHiddenLayerNetwork:\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        \"\"\"Initialize network parameters\"\"\"\n",
    "        self.W1 = np.random.randn(hidden_size, input_size) * 0.01\n",
    "        self.b1 = np.zeros((hidden_size, 1))\n",
    "        self.W2 = np.random.randn(output_size, hidden_size) * 0.01\n",
    "        self.b2 = np.zeros((output_size, 1))\n",
    "    \n",
    "    def forward(self, X):\n",
    "        \"\"\"Forward propagation through single hidden layer\"\"\"\n",
    "        self.Z1 = np.dot(self.W1, X) + self.b1\n",
    "        self.A1 = relu(self.Z1)\n",
    "        self.Z2 = np.dot(self.W2, self.A1) + self.b2\n",
    "        self.A2 = sigmoid(self.Z2)\n",
    "        return self.A2\n",
    "\n",
    "print(\"Single hidden layer neural network implemented.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03b971b5",
   "metadata": {},
   "source": [
    "**Implement code primitive: Build a deep neural network with many layers.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eb7e6cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepNeuralNetwork:\n",
    "    def __init__(self, layer_dims):\n",
    "        \"\"\"Initialize deep network with multiple layers\n",
    "        layer_dims: list of layer dimensions [input_size, hidden1, hidden2, ..., output_size]\n",
    "        \"\"\"\n",
    "        self.layer_dims = layer_dims\n",
    "        self.parameters = {}\n",
    "        self.L = len(layer_dims)\n",
    "        \n",
    "        for l in range(1, self.L):\n",
    "            self.parameters[f'W{l}'] = np.random.randn(layer_dims[l], layer_dims[l-1]) * 0.01\n",
    "            self.parameters[f'b{l}'] = np.zeros((layer_dims[l], 1))\n",
    "    \n",
    "    def forward(self, X):\n",
    "        \"\"\"Forward propagation through all layers\"\"\"\n",
    "        self.cache = {}\n",
    "        A = X\n",
    "        \n",
    "        for l in range(1, self.L):\n",
    "            Z = np.dot(self.parameters[f'W{l}'], A) + self.parameters[f'b{l}']\n",
    "            A = relu(Z) if l < self.L - 1 else sigmoid(Z)\n",
    "            self.cache[f'Z{l}'] = Z\n",
    "            self.cache[f'A{l}'] = A\n",
    "        \n",
    "        return A\n",
    "\n",
    "print(\"Deep neural network with many layers implemented.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76679feb",
   "metadata": {},
   "source": [
    "**Implement code primitive: Implement forward propagation steps.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95d578dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_propagation(X, parameters, layer_dims):\n",
    "    \"\"\"Complete forward propagation through the network\"\"\"\n",
    "    cache = {}\n",
    "    A = X\n",
    "    cache['A0'] = X\n",
    "    \n",
    "    L = len(layer_dims)\n",
    "    \n",
    "    for l in range(1, L):\n",
    "        W = parameters[f'W{l}']\n",
    "        b = parameters[f'b{l}']\n",
    "        \n",
    "        Z = np.dot(W, A) + b\n",
    "        \n",
    "        if l < L - 1:\n",
    "            A = relu(Z)\n",
    "        else:\n",
    "            A = sigmoid(Z)\n",
    "        \n",
    "        cache[f'Z{l}'] = Z\n",
    "        cache[f'A{l}'] = A\n",
    "    \n",
    "    return A, cache\n",
    "\n",
    "print(\"Forward propagation steps implemented.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99d32131",
   "metadata": {},
   "source": [
    "**Implement code primitive: Implement back propagation steps.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61f7c361",
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_propagation(Y, cache, parameters, layer_dims, learning_rate=0.01):\n",
    "    \"\"\"Complete back propagation to compute gradients and update parameters\"\"\"\n",
    "    m = Y.shape[1]\n",
    "    L = len(layer_dims)\n",
    "    gradients = {}\n",
    "    \n",
    "    dA = -(Y / cache[f'A{L-1}'] - (1 - Y) / (1 - cache[f'A{L-1}']))\n",
    "    \n",
    "    for l in range(L - 1, 0, -1):\n",
    "        A_prev = cache[f'A{l-1}']\n",
    "        Z = cache[f'Z{l}']\n",
    "        \n",
    "        dZ = dA * (relu_derivative(Z) if l < L - 1 else sigmoid_derivative(Z))\n",
    "        \n",
    "        dW = np.dot(dZ, A_prev.T) / m\n",
    "        db = np.sum(dZ, axis=1, keepdims=True) / m\n",
    "        \n",
    "        gradients[f'dW{l}'] = dW\n",
    "        gradients[f'db{l}'] = db\n",
    "        \n",
    "        if l > 1:\n",
    "            dA = np.dot(parameters[f'W{l}'].T, dZ)\n",
    "        \n",
    "        parameters[f'W{l}'] -= learning_rate * dW\n",
    "        parameters[f'b{l}'] -= learning_rate * db\n",
    "    \n",
    "    return parameters, gradients\n",
    "\n",
    "print(\"Back propagation steps implemented.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c3873e8",
   "metadata": {},
   "source": [
    "## Lesson 6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bae3626",
   "metadata": {},
   "source": [
    "**Explain the core concepts of the lesson**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51a05812",
   "metadata": {},
   "source": [
    "## Core Concepts\n",
    "\n",
    "Geoffrey Hinton's research has fundamentally shaped modern deep learning through several key innovations:\n",
    "\n",
    "**Backpropagation Algorithm**: The foundational technique for training neural networks by computing gradients through the chain rule, enabling efficient learning in multi-layer networks.\n",
    "\n",
    "**Word Embeddings**: Semantic feature vectors that represent words in a continuous vector space, capturing relationships and meanings from text data.\n",
    "\n",
    "**Boltzmann Machines**: Probabilistic models with densely connected networks that learn hidden representations through energy-based learning.\n",
    "\n",
    "**Restricted Boltzmann Machines (RBMs)**: Simplified Boltzmann machines with a bipartite structure (visible and hidden units) that learn single layers of features efficiently.\n",
    "\n",
    "**Deep Belief Networks (DBNs)**: Generative models constructed by stacking trained RBMs layer-wise, enabling efficient approximate inference in deep architectures.\n",
    "\n",
    "**Variational Bayes**: A method for approximating true posteriors in complex probabilistic models through variational inference.\n",
    "\n",
    "**Rectified Linear Units (ReLUs)**: Activation functions defined as $\\text{ReLU}(x) = \\max(0, x)$ that improve training of deep networks.\n",
    "\n",
    "**Recirculation Algorithm**: An autoencoder learning approach that minimizes activity variation without explicit backpropagation.\n",
    "\n",
    "**Fast Weights**: Neural network mechanisms enabling rapid adaptation and short-term memory for recursive processing.\n",
    "\n",
    "**Capsule Networks**: Neural architectures where groups of neurons represent multi-dimensional feature entities with routing mechanisms for improved generalization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d523c9c0",
   "metadata": {},
   "source": [
    "**Explain intuitions and mental models for the lesson**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa277b36",
   "metadata": {},
   "source": [
    "## Intuitions and Mental Models\n",
    "\n",
    "**Distributed Representations**: Brain memories are distributed throughout the brain, similar to how holograms store information. This principle underlies modern neural networks where knowledge is encoded across many neurons rather than localized in single units.\n",
    "\n",
    "**Computational Power Drives Progress**: The power of deep learning has been significantly driven by the increasing speed of computers, especially GPUs. Algorithms that were theoretically sound but computationally infeasible became practical with hardware advances.\n",
    "\n",
    "**Biological Plausibility**: Learning algorithms like backpropagation, if effective, could have been implemented by biological evolution due to strong selective pressure. This suggests that successful learning algorithms align with principles that nature has discovered.\n",
    "\n",
    "**Features as Concepts**: A concept can be understood as a collection of features, unifying traditional psychological and AI views. Rather than discrete symbolic representations, concepts emerge from combinations of learned features.\n",
    "\n",
    "**State Vectors Over Symbols**: Transforming raw measurements into a 'state vector' where actions become linear simplifies modeling and manipulation. This continuous representation enables efficient computation and learning.\n",
    "\n",
    "**Thoughts as Neural Activity**: Thoughts are best represented as large vectors of neural activity with causal powers, not as symbolic expressions. This perspective emphasizes the importance of learned representations over hand-crafted symbols.\n",
    "\n",
    "**Intuition-Driven Research**: Trusting one's intuition about a research idea, even when others disagree, can be key to groundbreaking discoveries. Many of Hinton's innovations were pursued despite initial skepticism from the community."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71991d0e",
   "metadata": {},
   "source": [
    "**Present and explain the key equations used in the lesson**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c22d0ac0",
   "metadata": {},
   "source": [
    "## Key Equations\n",
    "\n",
    "**Rectified Linear Unit (ReLU)**:\n",
    "\n",
    "$$\\text{ReLU}(x) = \\max(0, x)$$\n",
    "\n",
    "This activation function outputs the input directly if positive, and zero otherwise. ReLUs have become the standard activation function in modern deep neural networks because they:\n",
    "- Enable efficient gradient flow during backpropagation\n",
    "- Reduce the vanishing gradient problem in deep networks\n",
    "- Provide sparse representations where many neurons are inactive\n",
    "- Simplify computation compared to sigmoid or tanh activations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4163c023",
   "metadata": {},
   "source": [
    "**Implement code primitive: Implement backpropagation for discriminative learning tasks, such as predicting words in a sequence.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69780743",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class SequencePredictor:\n",
    "    def __init__(self, vocab_size, hidden_size, learning_rate=0.01):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "        self.W_input = np.random.randn(vocab_size, hidden_size) * 0.01\n",
    "        self.W_hidden = np.random.randn(hidden_size, hidden_size) * 0.01\n",
    "        self.W_output = np.random.randn(hidden_size, vocab_size) * 0.01\n",
    "        self.b_hidden = np.zeros((1, hidden_size))\n",
    "        self.b_output = np.zeros((1, vocab_size))\n",
    "    \n",
    "    def forward(self, x, h_prev):\n",
    "        h = np.tanh(np.dot(x, self.W_input) + np.dot(h_prev, self.W_hidden) + self.b_hidden)\n",
    "        logits = np.dot(h, self.W_output) + self.b_output\n",
    "        probs = np.exp(logits) / np.sum(np.exp(logits), axis=1, keepdims=True)\n",
    "        return h, probs\n",
    "    \n",
    "    def backward(self, x, h, h_prev, probs, target):\n",
    "        batch_size = x.shape[0]\n",
    "        \n",
    "        d_logits = probs.copy()\n",
    "        d_logits[np.arange(batch_size), target] -= 1\n",
    "        d_logits /= batch_size\n",
    "        \n",
    "        dW_output = np.dot(h.T, d_logits)\n",
    "        db_output = np.sum(d_logits, axis=0, keepdims=True)\n",
    "        \n",
    "        dh = np.dot(d_logits, self.W_output.T)\n",
    "        dh_raw = dh * (1 - h ** 2)\n",
    "        \n",
    "        dW_input = np.dot(x.T, dh_raw)\n",
    "        dW_hidden = np.dot(h_prev.T, dh_raw)\n",
    "        db_hidden = np.sum(dh_raw, axis=0, keepdims=True)\n",
    "        \n",
    "        self.W_output -= self.learning_rate * dW_output\n",
    "        self.b_output -= self.learning_rate * db_output\n",
    "        self.W_input -= self.learning_rate * dW_input\n",
    "        self.W_hidden -= self.learning_rate * dW_hidden\n",
    "        self.b_hidden -= self.learning_rate * db_hidden\n",
    "        \n",
    "        return np.dot(dh_raw, self.W_hidden.T)\n",
    "    \n",
    "    def train_step(self, x_batch, h_prev, target_batch):\n",
    "        h, probs = self.forward(x_batch, h_prev)\n",
    "        self.backward(x_batch, h, h_prev, probs, target_batch)\n",
    "        return h"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dae28c1",
   "metadata": {},
   "source": [
    "**Implement code primitive: Train models to learn semantic feature vectors (embeddings) for words from text data.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "976bb451",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class WordEmbedding:\n",
    "    def __init__(self, vocab_size, embedding_dim, learning_rate=0.01):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.learning_rate = learning_rate\n",
    "        self.embeddings = np.random.randn(vocab_size, embedding_dim) * 0.01\n",
    "    \n",
    "    def get_embedding(self, word_id):\n",
    "        return self.embeddings[word_id]\n",
    "    \n",
    "    def forward(self, context_ids, target_id):\n",
    "        context_vectors = self.embeddings[context_ids]\n",
    "        context_mean = np.mean(context_vectors, axis=0, keepdims=True)\n",
    "        \n",
    "        scores = np.dot(context_mean, self.embeddings.T)\n",
    "        probs = np.exp(scores) / np.sum(np.exp(scores))\n",
    "        \n",
    "        return context_mean, probs\n",
    "    \n",
    "    def backward(self, context_ids, target_id, context_mean, probs):\n",
    "        d_probs = probs.copy()\n",
    "        d_probs[0, target_id] -= 1\n",
    "        \n",
    "        d_embeddings_output = np.dot(d_probs, self.embeddings)\n",
    "        d_context_mean = np.dot(d_probs, self.embeddings.T)\n",
    "        \n",
    "        context_vectors = self.embeddings[context_ids]\n",
    "        d_context_vectors = d_context_mean / len(context_ids)\n",
    "        \n",
    "        for i, word_id in enumerate(context_ids):\n",
    "            self.embeddings[word_id] -= self.learning_rate * d_context_vectors\n",
    "        \n",
    "        self.embeddings -= self.learning_rate * np.dot(d_embeddings_output.T, np.ones((1, self.embedding_dim)))\n",
    "    \n",
    "    def train_step(self, context_ids, target_id):\n",
    "        context_mean, probs = self.forward(context_ids, target_id)\n",
    "        self.backward(context_ids, target_id, context_mean, probs)\n",
    "        loss = -np.log(probs[0, target_id] + 1e-10)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57f0e649",
   "metadata": {},
   "source": [
    "**Implement code primitive: Implement a Boltzmann machine learning algorithm for densely connected networks with hidden representations.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daf9e091",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class BoltzmannMachine:\n",
    "    def __init__(self, n_visible, n_hidden, learning_rate=0.01):\n",
    "        self.n_visible = n_visible\n",
    "        self.n_hidden = n_hidden\n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "        self.W = np.random.randn(n_visible, n_hidden) * 0.01\n",
    "        self.b_v = np.zeros(n_visible)\n",
    "        self.b_h = np.zeros(n_hidden)\n",
    "    \n",
    "    def sigmoid(self, x):\n",
    "        return 1 / (1 + np.exp(-np.clip(x, -500, 500)))\n",
    "    \n",
    "    def sample_hidden(self, v):\n",
    "        h_prob = self.sigmoid(np.dot(v, self.W) + self.b_h)\n",
    "        h = (np.random.rand(*h_prob.shape) < h_prob).astype(float)\n",
    "        return h, h_prob\n",
    "    \n",
    "    def sample_visible(self, h):\n",
    "        v_prob = self.sigmoid(np.dot(h, self.W.T) + self.b_v)\n",
    "        v = (np.random.rand(*v_prob.shape) < v_prob).astype(float)\n",
    "        return v, v_prob\n",
    "    \n",
    "    def gibbs_step(self, v):\n",
    "        h, h_prob = self.sample_hidden(v)\n",
    "        v_new, v_prob = self.sample_visible(h)\n",
    "        return v_new, h, h_prob, v_prob\n",
    "    \n",
    "    def train_step(self, v_data, n_gibbs=1):\n",
    "        h_data, h_prob_data = self.sample_hidden(v_data)\n",
    "        \n",
    "        v_model = v_data.copy()\n",
    "        for _ in range(n_gibbs):\n",
    "            v_model, h_model, h_prob_model, v_prob_model = self.gibbs_step(v_model)\n",
    "        \n",
    "        positive_gradient = np.dot(v_data.T, h_prob_data)\n",
    "        negative_gradient = np.dot(v_model.T, h_prob_model)\n",
    "        \n",
    "        self.W += self.learning_rate * (positive_gradient - negative_gradient) / v_data.shape[0]\n",
    "        self.b_v += self.learning_rate * np.mean(v_data - v_model, axis=0)\n",
    "        self.b_h += self.learning_rate * np.mean(h_prob_data - h_prob_model, axis=0)\n",
    "        \n",
    "        return np.mean((v_data - v_model) ** 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5be2e9e",
   "metadata": {},
   "source": [
    "**Implement code primitive: Develop Restricted Boltzmann Machines (RBMs) for learning single layers of features.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad20ccdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class RBM:\n",
    "    def __init__(self, n_visible, n_hidden, learning_rate=0.01):\n",
    "        self.n_visible = n_visible\n",
    "        self.n_hidden = n_hidden\n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "        self.W = np.random.randn(n_visible, n_hidden) * 0.01\n",
    "        self.b_v = np.zeros(n_visible)\n",
    "        self.b_h = np.zeros(n_hidden)\n",
    "    \n",
    "    def sigmoid(self, x):\n",
    "        return 1 / (1 + np.exp(-np.clip(x, -500, 500)))\n",
    "    \n",
    "    def forward(self, v):\n",
    "        h_prob = self.sigmoid(np.dot(v, self.W) + self.b_h)\n",
    "        h = (np.random.rand(*h_prob.shape) < h_prob).astype(float)\n",
    "        return h, h_prob\n",
    "    \n",
    "    def backward(self, h):\n",
    "        v_prob = self.sigmoid(np.dot(h, self.W.T) + self.b_v)\n",
    "        v = (np.random.rand(*v_prob.shape) < v_prob).astype(float)\n",
    "        return v, v_prob\n",
    "    \n",
    "    def contrastive_divergence(self, v_data, k=1):\n",
    "        h_data, h_prob_data = self.forward(v_data)\n",
    "        \n",
    "        h = h_data\n",
    "        for _ in range(k):\n",
    "            v, v_prob = self.backward(h)\n",
    "            h, h_prob = self.forward(v)\n",
    "        \n",
    "        positive = np.dot(v_data.T, h_prob_data)\n",
    "        negative = np.dot(v.T, h_prob)\n",
    "        \n",
    "        self.W += self.learning_rate * (positive - negative) / v_data.shape[0]\n",
    "        self.b_v += self.learning_rate * np.mean(v_data - v, axis=0)\n",
    "        self.b_h += self.learning_rate * np.mean(h_prob_data - h_prob, axis=0)\n",
    "        \n",
    "        return np.mean((v_data - v) ** 2)\n",
    "    \n",
    "    def transform(self, v):\n",
    "        h, h_prob = self.forward(v)\n",
    "        return h_prob"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ae04655",
   "metadata": {},
   "source": [
    "**Implement code primitive: Construct Deep Belief Networks (DBNs) by stacking trained RBMs layer-wise and performing efficient approximate inference.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c399a02",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class DBN:\n",
    "    def __init__(self, layer_sizes, learning_rate=0.01):\n",
    "        self.layer_sizes = layer_sizes\n",
    "        self.learning_rate = learning_rate\n",
    "        self.rbms = []\n",
    "        \n",
    "        for i in range(len(layer_sizes) - 1):\n",
    "            rbm = RBM(layer_sizes[i], layer_sizes[i + 1], learning_rate)\n",
    "            self.rbms.append(rbm)\n",
    "    \n",
    "    def sigmoid(self, x):\n",
    "        return 1 / (1 + np.exp(-np.clip(x, -500, 500)))\n",
    "    \n",
    "    def pretrain(self, data, epochs=10, k=1):\n",
    "        current_data = data\n",
    "        \n",
    "        for layer_idx, rbm in enumerate(self.rbms):\n",
    "            for epoch in range(epochs):\n",
    "                error = rbm.contrastive_divergence(current_data, k=k)\n",
    "            \n",
    "            current_data = rbm.transform(current_data)\n",
    "    \n",
    "    def forward(self, v):\n",
    "        activations = [v]\n",
    "        current = v\n",
    "        \n",
    "        for rbm in self.rbms:\n",
    "            h_prob = self.sigmoid(np.dot(current, rbm.W) + rbm.b_h)\n",
    "            activations.append(h_prob)\n",
    "            current = h_prob\n",
    "        \n",
    "        return activations\n",
    "    \n",
    "    def backward(self, h_top):\n",
    "        current = h_top\n",
    "        \n",
    "        for rbm in reversed(self.rbms):\n",
    "            v_prob = self.sigmoid(np.dot(current, rbm.W.T) + rbm.b_v)\n",
    "            current = v_prob\n",
    "        \n",
    "        return current\n",
    "    \n",
    "    def inference(self, v, n_steps=10):\n",
    "        current = v\n",
    "        \n",
    "        for _ in range(n_steps):\n",
    "            activations = self.forward(current)\n",
    "            h_top = activations[-1]\n",
    "            current = self.backward(h_top)\n",
    "        \n",
    "        return current"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c51be7fb",
   "metadata": {},
   "source": [
    "**Implement code primitive: Implement variational Bayesian learning to approximate true posteriors in complex models.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b40bf2ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.special import digamma, loggamma\n",
    "\n",
    "class VariationalBayesian:\n",
    "    def __init__(self, n_features, n_components, learning_rate=0.01):\n",
    "        self.n_features = n_features\n",
    "        self.n_components = n_components\n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "        self.alpha = np.ones(n_components)\n",
    "        self.beta = np.ones((n_components, n_features))\n",
    "        self.gamma = np.random.rand(n_components)\n",
    "    \n",
    "    def elbo(self, data):\n",
    "        n_samples = data.shape[0]\n",
    "        \n",
    "        log_likelihood = 0\n",
    "        for k in range(self.n_components):\n",
    "            log_beta_k = digamma(self.beta[k]) - digamma(np.sum(self.beta[k]))\n",
    "            log_likelihood += np.sum(data * log_beta_k)\n",
    "        \n",
    "        kl_alpha = np.sum((self.alpha - 1) * (digamma(self.gamma) - digamma(np.sum(self.gamma))))\n",
    "        kl_alpha += np.sum(loggamma(np.sum(self.alpha)) - np.sum(loggamma(self.alpha)))\n",
    "        kl_alpha -= np.sum(loggamma(np.sum(self.gamma)) - np.sum(loggamma(self.gamma)))\n",
    "        \n",
    "        return log_likelihood + kl_alpha\n",
    "    \n",
    "    def update(self, data):\n",
    "        n_samples = data.shape[0]\n",
    "        \n",
    "        for k in range(self.n_components):\n",
    "            self.gamma[k] = self.alpha[k] + np.sum(data)\n",
    "            self.beta[k] = self.beta[k] + data.T\n",
    "    \n",
    "    def fit(self, data, n_iterations=10):\n",
    "        for iteration in range(n_iterations):\n",
    "            self.update(data)\n",
    "            elbo_val = self.elbo(data)\n",
    "    \n",
    "    def predict(self, data):\n",
    "        log_prob = np.zeros((data.shape[0], self.n_components))\n",
    "        \n",
    "        for k in range(self.n_components):\n",
    "            log_prob[:, k] = digamma(self.gamma[k]) - digamma(np.sum(self.gamma))\n",
    "            log_prob[:, k] += np.dot(data, digamma(self.beta[k]) - digamma(np.sum(self.beta[k])))\n",
    "        \n",
    "        return np.argmax(log_prob, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "956dca31",
   "metadata": {},
   "source": [
    "**Implement code primitive: Utilize Rectified Linear Units (ReLUs) as activation functions in neural networks.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5508cd1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class ReLUNetwork:\n",
    "    def __init__(self, layer_sizes, learning_rate=0.01):\n",
    "        self.layer_sizes = layer_sizes\n",
    "        self.learning_rate = learning_rate\n",
    "        self.weights = []\n",
    "        self.biases = []\n",
    "        \n",
    "        for i in range(len(layer_sizes) - 1):\n",
    "            w = np.random.randn(layer_sizes[i], layer_sizes[i + 1]) * np.sqrt(2.0 / layer_sizes[i])\n",
    "            b = np.zeros((1, layer_sizes[i + 1]))\n",
    "            self.weights.append(w)\n",
    "            self.biases.append(b)\n",
    "    \n",
    "    def relu(self, x):\n",
    "        return np.maximum(0, x)\n",
    "    \n",
    "    def relu_derivative(self, x):\n",
    "        return (x > 0).astype(float)\n",
    "    \n",
    "    def softmax(self, x):\n",
    "        exp_x = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
    "        return exp_x / np.sum(exp_x, axis=1, keepdims=True)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        self.activations = [x]\n",
    "        self.z_values = []\n",
    "        \n",
    "        for i in range(len(self.weights) - 1):\n",
    "            z = np.dot(self.activations[-1], self.weights[i]) + self.biases[i]\n",
    "            self.z_values.append(z)\n",
    "            a = self.relu(z)\n",
    "            self.activations.append(a)\n",
    "        \n",
    "        z_final = np.dot(self.activations[-1], self.weights[-1]) + self.biases[-1]\n",
    "        self.z_values.append(z_final)\n",
    "        a_final = self.softmax(z_final)\n",
    "        self.activations.append(a_final)\n",
    "        \n",
    "        return a_final\n",
    "    \n",
    "    def backward(self, y_true):\n",
    "        m = y_true.shape[0]\n",
    "        \n",
    "        delta = self.activations[-1] - y_true\n",
    "        \n",
    "        for i in range(len(self.weights) - 1, -1, -1):\n",
    "            dw = np.dot(self.activations[i].T, delta) / m\n",
    "            db = np.sum(delta, axis=0, keepdims=True) / m\n",
    "            \n",
    "            self.weights[i] -= self.learning_rate * dw\n",
    "            self.biases[i] -= self.learning_rate * db\n",
    "            \n",
    "            if i > 0:\n",
    "                delta = np.dot(delta, self.weights[i].T) * self.relu_derivative(self.z_values[i - 1])\n",
    "    \n",
    "    def train_step(self, x, y_true):\n",
    "        output = self.forward(x)\n",
    "        self.backward(y_true)\n",
    "        loss = -np.mean(np.sum(y_true * np.log(output + 1e-10), axis=1))\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34ccbed4",
   "metadata": {},
   "source": [
    "**Implement code primitive: Initialize deep neural networks with identity matrices to facilitate training of very deep architectures.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76b77a06",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class DeepNetworkWithIdentityInit:\n",
    "    def __init__(self, layer_sizes, learning_rate=0.01):\n",
    "        self.layer_sizes = layer_sizes\n",
    "        self.learning_rate = learning_rate\n",
    "        self.weights = []\n",
    "        self.biases = []\n",
    "        \n",
    "        for i in range(len(layer_sizes) - 1):\n",
    "            if layer_sizes[i] == layer_sizes[i + 1]:\n",
    "                w = np.eye(layer_sizes[i])\n",
    "            else:\n",
    "                min_dim = min(layer_sizes[i], layer_sizes[i + 1])\n",
    "                w = np.random.randn(layer_sizes[i], layer_sizes[i + 1]) * 0.01\n",
    "                w[:min_dim, :min_dim] += np.eye(min_dim)\n",
    "            \n",
    "            b = np.zeros((1, layer_sizes[i + 1]))\n",
    "            self.weights.append(w)\n",
    "            self.biases.append(b)\n",
    "    \n",
    "    def relu(self, x):\n",
    "        return np.maximum(0, x)\n",
    "    \n",
    "    def relu_derivative(self, x):\n",
    "        return (x > 0).astype(float)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        self.activations = [x]\n",
    "        self.z_values = []\n",
    "        \n",
    "        for i in range(len(self.weights)):\n",
    "            z = np.dot(self.activations[-1], self.weights[i]) + self.biases[i]\n",
    "            self.z_values.append(z)\n",
    "            a = self.relu(z)\n",
    "            self.activations.append(a)\n",
    "        \n",
    "        return self.activations[-1]\n",
    "    \n",
    "    def backward(self, y_true):\n",
    "        m = y_true.shape[0]\n",
    "        \n",
    "        delta = self.activations[-1] - y_true\n",
    "        \n",
    "        for i in range(len(self.weights) - 1, -1, -1):\n",
    "            dw = np.dot(self.activations[i].T, delta) / m\n",
    "            db = np.sum(delta, axis=0, keepdims=True) / m\n",
    "            \n",
    "            self.weights[i] -= self.learning_rate * dw\n",
    "            self.biases[i] -= self.learning_rate * db\n",
    "            \n",
    "            if i > 0:\n",
    "                delta = np.dot(delta, self.weights[i].T) * self.relu_derivative(self.z_values[i - 1])\n",
    "    \n",
    "    def train_step(self, x, y_true):\n",
    "        output = self.forward(x)\n",
    "        self.backward(y_true)\n",
    "        loss = np.mean((output - y_true) ** 2)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c6bc3b0",
   "metadata": {},
   "source": [
    "**Implement code primitive: Implement the recirculation algorithm for autoencoders to learn without explicit backpropagation by minimizing activity variation.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "712b76fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class RecirculationAutoencoder:\n",
    "    def __init__(self, input_size, hidden_size, learning_rate=0.01):\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "        self.W_encode = np.random.randn(input_size, hidden_size) * 0.01\n",
    "        self.W_decode = np.random.randn(hidden_size, input_size) * 0.01\n",
    "        self.b_hidden = np.zeros((1, hidden_size))\n",
    "        self.b_output = np.zeros((1, input_size))\n",
    "    \n",
    "    def sigmoid(self, x):\n",
    "        return 1 / (1 + np.exp(-np.clip(x, -500, 500)))\n",
    "    \n",
    "    def encode(self, x):\n",
    "        h = self.sigmoid(np.dot(x, self.W_encode) + self.b_hidden)\n",
    "        return h\n",
    "    \n",
    "    def decode(self, h):\n",
    "        x_recon = self.sigmoid(np.dot(h, self.W_decode) + self.b_output)\n",
    "        return x_recon\n",
    "    \n",
    "    def recirculate(self, x, n_iterations=3):\n",
    "        h = self.encode(x)\n",
    "        \n",
    "        for _ in range(n_iterations):\n",
    "            x_recon = self.decode(h)\n",
    "            h_new = self.encode(x_recon)\n",
    "            h = 0.5 * h + 0.5 * h_new\n",
    "        \n",
    "        return h\n",
    "    \n",
    "    def train_step(self, x):\n",
    "        h_initial = self.encode(x)\n",
    "        h_recirculated = self.recirculate(x, n_iterations=3)\n",
    "        \n",
    "        activity_variation = np.mean((h_initial - h_recirculated) ** 2)\n",
    "        \n",
    "        x_recon = self.decode(h_recirculated)\n",
    "        reconstruction_error = np.mean((x - x_recon) ** 2)\n",
    "        \n",
    "        dh = 2 * (h_recirculated - h_initial) / x.shape[0]\n",
    "        \n",
    "        dW_decode = np.dot(h_recirculated.T, (x - x_recon)) / x.shape[0]\n",
    "        db_output = np.mean(x - x_recon, axis=0, keepdims=True)\n",
    "        \n",
    "        self.W_decode += self.learning_rate * dW_decode\n",
    "        self.b_output += self.learning_rate * db_output\n",
    "        \n",
    "        dW_encode = np.dot(x.T, dh) / x.shape[0]\n",
    "        db_hidden = np.mean(dh, axis=0, keepdims=True)\n",
    "        \n",
    "        self.W_encode += self.learning_rate * dW_encode\n",
    "        self.b_hidden += self.learning_rate * db_hidden\n",
    "        \n",
    "        return activity_variation + reconstruction_error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f72b2fb",
   "metadata": {},
   "source": [
    "**Implement code primitive: Develop neural networks that use 'fast weights' for rapid adaptation and short-term memory, enabling recursion.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bd653f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class FastWeightNetwork:\n",
    "    def __init__(self, input_size, hidden_size, learning_rate=0.01, fast_weight_decay=0.95):\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.learning_rate = learning_rate\n",
    "        self.fast_weight_decay = fast_weight_decay\n",
    "        \n",
    "        self.W_slow = np.random.randn(input_size, hidden_size) * 0.01\n",
    "        self.W_fast = np.zeros((input_size, hidden_size))\n",
    "        self.W_recurrent = np.random.randn(hidden_size, hidden_size) * 0.01\n",
    "        self.b_hidden = np.zeros((1, hidden_size))\n",
    "    \n",
    "    def relu(self, x):\n",
    "        return np.maximum(0, x)\n",
    "    \n",
    "    def forward(self, x, h_prev):\n",
    "        W_combined = self.W_slow + self.W_fast\n",
    "        \n",
    "        h = self.relu(np.dot(x, W_combined) + np.dot(h_prev, self.W_recurrent) + self.b_hidden)\n",
    "        \n",
    "        return h\n",
    "    \n",
    "    def update_fast_weights(self, x, h, learning_rate_fast=0.1):\n",
    "        dW_fast = np.dot(x.T, h) / x.shape[0]\n",
    "        self.W_fast = self.fast_weight_decay * self.W_fast + learning_rate_fast * dW_fast\n",
    "    \n",
    "    def update_slow_weights(self, x, h, target):\n",
    "        error = h - target\n",
    "        dW_slow = np.dot(x.T, error) / x.shape[0]\n",
    "        self.W_slow -= self.learning_rate * dW_slow\n",
    "    \n",
    "    def train_step(self, x_sequence, target_sequence):\n",
    "        h = np.zeros((x_sequence.shape[0], self.hidden_size))\n",
    "        total_loss = 0\n",
    "        \n",
    "        for t in range(x_sequence.shape[1]):\n",
    "            x_t = x_sequence[:, t:t+1]\n",
    "            target_t = target_sequence[:, t:t+1]\n",
    "            \n",
    "            h_prev = h if t > 0 else np.zeros((x_sequence.shape[0], self.hidden_size))\n",
    "            h = self.forward(x_t, h_prev)\n",
    "            \n",
    "            self.update_fast_weights(x_t, h)\n",
    "            self.update_slow_weights(x_t, h, target_t)\n",
    "            \n",
    "            loss = np.mean((h - target_t) ** 2)\n",
    "            total_loss += loss\n",
    "        \n",
    "        return total_loss / x_sequence.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bc44f6c",
   "metadata": {},
   "source": [
    "**Implement code primitive: Implement capsule networks where groups of neurons represent multi-dimensional feature entities.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cc36b1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class CapsuleLayer:\n",
    "    def __init__(self, n_capsules, capsule_dim, input_dim, learning_rate=0.01):\n",
    "        self.n_capsules = n_capsules\n",
    "        self.capsule_dim = capsule_dim\n",
    "        self.input_dim = input_dim\n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "        self.W = np.random.randn(input_dim, n_capsules, capsule_dim) * 0.01\n",
    "        self.b = np.zeros((n_capsules, capsule_dim))\n",
    "    \n",
    "    def squash(self, x):\n",
    "        norm = np.linalg.norm(x, axis=-1, keepdims=True)\n",
    "        return (norm ** 2 / (1 + norm ** 2)) * (x / (norm + 1e-10))\n",
    "    \n",
    "    def forward(self, u):\n",
    "        batch_size = u.shape[0]\n",
    "        \n",
    "        u_hat = np.zeros((batch_size, self.n_capsules, self.capsule_dim))\n",
    "        for i in range(self.n_capsules):\n",
    "            u_hat[:, i, :] = np.dot(u, self.W[:, i, :]) + self.b[i]\n",
    "        \n",
    "        b_ij = np.zeros((batch_size, self.n_capsules))\n",
    "        \n",
    "        for iteration in range(3):\n",
    "            c_ij = np.exp(b_ij) / np.sum(np.exp(b_ij), axis=1, keepdims=True)\n",
    "            \n",
    "            s_j = np.zeros((batch_size, self.n_capsules, self.capsule_dim))\n",
    "            for j in range(self.n_capsules):\n",
    "                s_j[:, j, :] = np.sum(c_ij[:, j:j+1] * u_hat, axis=1)\n",
    "            \n",
    "            v_j = np.zeros((batch_size, self.n_capsules, self.capsule_dim))\n",
    "            for j in range(self.n_capsules):\n",
    "                v_j[:, j, :] = self.squash(s_j[:, j, :])\n",
    "            \n",
    "            if iteration < 2:\n",
    "                for i in range(self.n_capsules):\n",
    "                    for j in range(self.n_capsules):\n",
    "                        b_ij[:, j] += np.sum(u_hat[:, i, :] * v_j[:, j, :], axis=1)\n",
    "        \n",
    "        return v_j\n",
    "    \n",
    "    def backward(self, grad_output):\n",
    "        batch_size = grad_output.shape[0]\n",
    "        \n",
    "        dW = np.zeros_like(self.W)\n",
    "        db = np.zeros_like(self.b)\n",
    "        \n",
    "        for i in range(self.n_capsules):\n",
    "            for j in range(self.n_capsules):\n",
    "                dW[:, j, :] += np.outer(grad_output[:, j, :], grad_output[:, j, :])\n",
    "        \n",
    "        self.W -= self.learning_rate * dW / batch_size\n",
    "        self.b -= self.learning_rate * db / batch_size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c23003a7",
   "metadata": {},
   "source": [
    "**Implement code primitive: Develop 'routing by agreement' mechanisms in capsule networks for improved generalization, segmentation, and viewpoint handling.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12b4c8f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class RoutingByAgreement:\n",
    "    def __init__(self, n_lower_capsules, n_upper_capsules, capsule_dim, learning_rate=0.01):\n",
    "        self.n_lower_capsules = n_lower_capsules\n",
    "        self.n_upper_capsules = n_upper_capsules\n",
    "        self.capsule_dim = capsule_dim\n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "        self.W = np.random.randn(n_lower_capsules, n_upper_capsules, capsule_dim, capsule_dim) * 0.01\n",
    "    \n",
    "    def squash(self, x):\n",
    "        norm = np.linalg.norm(x, axis=-1, keepdims=True)\n",
    "        return (norm ** 2 / (1 + norm ** 2)) * (x / (norm + 1e-10))\n",
    "    \n",
    "    def routing_algorithm(self, u_lower, n_iterations=3):\n",
    "        batch_size = u_lower.shape[0]\n",
    "        \n",
    "        b_ij = np.zeros((batch_size, self.n_lower_capsules, self.n_upper_capsules))\n",
    "        \n",
    "        for iteration in range(n_iterations):\n",
    "            c_ij = np.exp(b_ij) / np.sum(np.exp(b_ij), axis=2, keepdims=True)\n",
    "            \n",
    "            s_j = np.zeros((batch_size, self.n_upper_capsules, self.capsule_dim))\n",
    "            \n",
    "            for j in range(self.n_upper_capsules):\n",
    "                for i in range(self.n_lower_capsules):\n",
    "                    u_hat_ij = np.dot(u_lower[:, i, :], self.W[i, j, :, :])\n",
    "                    s_j[:, j, :] += c_ij[:, i, j:j+1] * u_hat_ij\n",
    "            \n",
    "            v_j = np.zeros((batch_size, self.n_upper_capsules, self.capsule_dim))\n",
    "            for j in range(self.n_upper_capsules):\n",
    "                v_j[:, j, :] = self.squash(s_j[:, j, :])\n",
    "            \n",
    "            if iteration < n_iterations - 1:\n",
    "                for i in range(self.n_lower_capsules):\n",
    "                    for j in range(self.n_upper_capsules):\n",
    "                        u_hat_ij = np.dot(u_lower[:, i, :], self.W[i, j, :, :])\n",
    "                        agreement = np.sum(u_hat_ij * v_j[:, j, :], axis=1)\n",
    "                        b_ij[:, i, j] += agreement\n",
    "        \n",
    "        return v_j\n",
    "    \n",
    "    def forward(self, u_lower):\n",
    "        v_upper = self.routing_algorithm(u_lower, n_iterations=3)\n",
    "        return v_upper\n",
    "    \n",
    "    def backward(self, grad_output, u_lower):\n",
    "        batch_size = u_lower.shape[0]\n",
    "        \n",
    "        dW = np.zeros_like(self.W)\n",
    "        \n",
    "        for i in range(self.n_lower_capsules):\n",
    "            for j in range(self.n_upper_capsules):\n",
    "                u_hat_ij = np.dot(u_lower[:, i, :], self.W[i, j, :, :])\n",
    "                dW[i, j, :, :] += np.dot(u_lower[:, i, :].T, grad_output[:, j, :]) / batch_size\n",
    "        \n",
    "        self.W -= self.learning_rate * dW"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "603c396f",
   "metadata": {},
   "source": [
    "**Create a Mermaid diagram: graph TD\n",
    "    A[Input Data] --> B{RBM 1 (Layer 1)};\n",
    "    B --> C{RBM 2 (Layer 2)};\n",
    "    C --> D{RBM 3 (Layer 3)};\n",
    "    D --> E[Deep Belief Network (DBN)];**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "079a0b44",
   "metadata": {},
   "source": [
    "## Deep Belief Network Architecture\n",
    "\n",
    "```mermaid\n",
    "graph TD\n",
    "    A[Input Data] --> B{RBM 1 Layer 1}\n",
    "    B --> C{RBM 2 Layer 2}\n",
    "    C --> D{RBM 3 Layer 3}\n",
    "    D --> E[Deep Belief Network DBN]\n",
    "```\n",
    "\n",
    "Deep Belief Networks are constructed by stacking multiple Restricted Boltzmann Machines layer-wise. Each RBM learns a layer of features from the output of the previous layer, enabling the network to learn hierarchical representations of the input data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e6c297c",
   "metadata": {},
   "source": [
    "**Create a Mermaid diagram: sequenceDiagram\n",
    "    participant NN as Neural Network\n",
    "    participant Loss as Loss Function\n",
    "    NN->>NN: Forward Pass (Input to Output)\n",
    "    NN->>Loss: Calculate Loss\n",
    "    Loss->>NN: Backward Pass (Compute Gradients)\n",
    "    NN->>NN: Update Weights**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1226ce86",
   "metadata": {},
   "source": [
    "## Backpropagation Training Loop\n",
    "\n",
    "```mermaid\n",
    "sequenceDiagram\n",
    "    participant NN as Neural Network\n",
    "    participant Loss as Loss Function\n",
    "    NN->>NN: Forward Pass Input to Output\n",
    "    NN->>Loss: Calculate Loss\n",
    "    Loss->>NN: Backward Pass Compute Gradients\n",
    "    NN->>NN: Update Weights\n",
    "```\n",
    "\n",
    "Backpropagation enables efficient training of deep neural networks by computing gradients through the chain rule. The forward pass propagates input through the network to produce predictions, the loss function measures prediction error, and the backward pass computes gradients for each parameter, allowing weights to be updated in the direction that reduces loss."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4a55d4f",
   "metadata": {},
   "source": [
    "**Create a Mermaid diagram: graph TD\n",
    "    A[Input Units] -- Send data --> B[Hidden Units];\n",
    "    B -- Send data back --> A;\n",
    "    A -- Iterate --> B;\n",
    "    B -- Learn to reduce variation --> A;**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2573214",
   "metadata": {},
   "source": [
    "## Recirculation Algorithm\n",
    "\n",
    "```mermaid\n",
    "graph TD\n",
    "    A[Input Units] -- Send data --> B[Hidden Units]\n",
    "    B -- Send data back --> A\n",
    "    A -- Iterate --> B\n",
    "    B -- Learn to reduce variation --> A\n",
    "```\n",
    "\n",
    "The recirculation algorithm enables autoencoders to learn without explicit backpropagation by iteratively passing information between input and hidden units. The network learns by minimizing the variation in activity patterns across recirculation iterations, allowing it to discover efficient representations of the input data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0e5b7f5",
   "metadata": {},
   "source": [
    "**Create a Mermaid diagram: graph TD\n",
    "    M[Mouth Capsule] --> V1(Vote for Face Parameters);\n",
    "    N[Nose Capsule] --> V2(Vote for Face Parameters);\n",
    "    V1 -- Agree? --> F[Face Capsule];\n",
    "    V2 -- Agree? --> F;**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e7e5d01",
   "metadata": {},
   "source": [
    "## Routing by Agreement in Capsule Networks\n",
    "\n",
    "```mermaid\n",
    "graph TD\n",
    "    M[Mouth Capsule] --> V1(Vote for Face Parameters)\n",
    "    N[Nose Capsule] --> V2(Vote for Face Parameters)\n",
    "    V1 -- Agree? --> F[Face Capsule]\n",
    "    V2 -- Agree? --> F\n",
    "```\n",
    "\n",
    "Routing by agreement is a mechanism in capsule networks where lower-level capsules vote on the parameters of higher-level capsules. When multiple lower-level capsules agree on their votes, the connection strength to the higher-level capsule is strengthened. This enables the network to learn part-whole relationships and improve generalization, segmentation, and viewpoint handling."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67c5bb46",
   "metadata": {},
   "source": [
    "# Week 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4ba474d",
   "metadata": {},
   "source": [
    "## Lesson 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b495f4ca",
   "metadata": {},
   "source": [
    "**Explain the core concepts of the lesson**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16003742",
   "metadata": {},
   "source": [
    "## Core Concepts\n",
    "\n",
    "This lesson introduces the fundamental programming concepts and notation used in neural networks, with a focus on how to organize and represent data efficiently.\n",
    "\n",
    "**Key Concepts:**\n",
    "\n",
    "1. **Vectorization**: Processing entire training sets without explicit for loops is crucial for neural network efficiency. Instead of iterating through individual examples, we organize data into matrices and perform operations on them simultaneously.\n",
    "\n",
    "2. **Binary Classification**: A foundational task where the goal is to predict one of two possible outcomes (e.g., yes/no, cat/not cat).\n",
    "\n",
    "3. **Logistic Regression Algorithm**: A simple yet powerful algorithm used as a foundational example to understand neural network programming concepts.\n",
    "\n",
    "4. **Image Representation (RGB)**: Images are represented using three separate matrices—one for each color channel (Red, Green, Blue).\n",
    "\n",
    "5. **Feature Vector (x)**: All pixel intensity values from an image's RGB channels are unrolled into a single column vector, which serves as the input to the model.\n",
    "\n",
    "6. **Input Feature Dimension ($n_x$)**: The total number of features in a single training example, calculated as width × height × number of channels.\n",
    "\n",
    "7. **Training Example Notation**: Each training example is denoted as $(x, y)$, where $x$ is the feature vector and $y$ is the corresponding label.\n",
    "\n",
    "8. **Training Set Size (m)**: The number of training examples in the dataset.\n",
    "\n",
    "9. **Input Data Matrix (X)**: Individual training feature vectors are stacked as columns to form a matrix of dimensions $n_x \\times m$.\n",
    "\n",
    "10. **Output Label Matrix (Y)**: Individual training labels are stacked as columns to form a matrix of dimensions $1 \\times m$.\n",
    "\n",
    "11. **Column Stacking Convention**: The standard practice of organizing training data by stacking individual examples into columns within matrices simplifies neural network implementations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8568e6d3",
   "metadata": {},
   "source": [
    "**Explain intuitions and mental models for the lesson**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0325fa9d",
   "metadata": {},
   "source": [
    "## Intuitions and Mental Models\n",
    "\n",
    "**Why Vectorization Matters:**\n",
    "Processing entire training sets without explicit for loops is crucial for neural network efficiency. When you vectorize operations, you leverage optimized linear algebra libraries that can process multiple examples simultaneously, making your code run orders of magnitude faster than iterating through examples one by one.\n",
    "\n",
    "**Forward and Backward Passes:**\n",
    "Neural network computations are typically organized into distinct forward and backward passes. The forward pass computes predictions from input data, while the backward pass computes gradients for learning. Understanding this structure helps you organize your code logically and efficiently.\n",
    "\n",
    "**Logistic Regression as a Foundation:**\n",
    "Logistic regression is used as a foundational example to understand neural network programming concepts. It's simple enough to understand quickly, yet it demonstrates the same organizational principles (vectorization, matrix operations, forward/backward passes) that apply to more complex neural networks.\n",
    "\n",
    "**Data Organization Through Column Stacking:**\n",
    "Organizing training data by stacking individual examples into columns within matrices (X and Y) simplifies neural network implementations. This convention allows you to write compact, vectorized code that processes all training examples at once, rather than writing loops to handle each example individually.\n",
    "\n",
    "**Mental Model for Data Flow:**\n",
    "Think of your training data as flowing through your model in a vectorized manner: all $m$ training examples are processed together, with each example contributing one column to the input matrix X and one column to the output matrix Y. This parallel processing is what makes neural networks computationally efficient at scale."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d9feb9f",
   "metadata": {},
   "source": [
    "**Present and explain the key equations used in the lesson**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d669d67b",
   "metadata": {},
   "source": [
    "## Key Equations\n",
    "\n",
    "**Input Feature Dimension:**\n",
    "\n",
    "For an image represented with RGB channels, the total number of features in a single training example is:\n",
    "\n",
    "$$n_x = \\text{image\\_width} \\times \\text{image\\_height} \\times \\text{num\\_channels}$$\n",
    "\n",
    "**Example: 64×64 RGB Image:**\n",
    "\n",
    "$$n_x = 64 \\times 64 \\times 3 = 12288$$\n",
    "\n",
    "This means each image is flattened into a feature vector of 12,288 elements.\n",
    "\n",
    "**Input Data Matrix Dimension:**\n",
    "\n",
    "When stacking $m$ training examples as columns, the input data matrix X has dimensions:\n",
    "\n",
    "$$X \\text{ (Input Data Matrix) dimension}: n_x \\times m$$\n",
    "\n",
    "Each column represents one training example's feature vector.\n",
    "\n",
    "**Output Label Matrix Dimension:**\n",
    "\n",
    "When stacking $m$ training labels as columns, the output label matrix Y has dimensions:\n",
    "\n",
    "$$Y \\text{ (Output Label Matrix) dimension}: 1 \\times m$$\n",
    "\n",
    "Each column contains the label for the corresponding training example in X."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "766dc2b6",
   "metadata": {},
   "source": [
    "**Implement code primitive: Represent an image using three separate matrices for Red, Green, and Blue color channels.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aea35126",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Create a sample 64x64 RGB image\n",
    "image_height = 64\n",
    "image_width = 64\n",
    "\n",
    "# Initialize three separate matrices for R, G, B channels\n",
    "R = np.random.randint(0, 256, size=(image_height, image_width), dtype=np.uint8)\n",
    "G = np.random.randint(0, 256, size=(image_height, image_width), dtype=np.uint8)\n",
    "B = np.random.randint(0, 256, size=(image_height, image_width), dtype=np.uint8)\n",
    "\n",
    "print(f\"Red channel shape: {R.shape}\")\n",
    "print(f\"Green channel shape: {G.shape}\")\n",
    "print(f\"Blue channel shape: {B.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad1d656f",
   "metadata": {},
   "source": [
    "**Implement code primitive: Unroll all pixel intensity values from an image's RGB channels into a single input feature vector (x).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74a2e719",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unroll all pixel intensity values from R, G, B channels into a single feature vector\n",
    "x = np.concatenate([R.flatten(), G.flatten(), B.flatten()])\n",
    "\n",
    "print(f\"Feature vector shape: {x.shape}\")\n",
    "print(f\"Feature vector dimension (n_x): {len(x)}\")\n",
    "print(f\"Expected n_x = 64 * 64 * 3 = {64 * 64 * 3}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fef07bb",
   "metadata": {},
   "source": [
    "**Implement code primitive: Construct a training data matrix (capital X) by stacking individual training feature vectors as columns.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "324bf9a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create multiple training examples and stack them as columns\n",
    "m = 10  # number of training examples\n",
    "n_x = 64 * 64 * 3  # feature dimension\n",
    "\n",
    "# Initialize training data matrix X\n",
    "X = np.zeros((n_x, m))\n",
    "\n",
    "# Fill X with training examples (each column is one training example)\n",
    "for i in range(m):\n",
    "    # Generate a random feature vector for each training example\n",
    "    X[:, i] = np.random.randint(0, 256, size=n_x)\n",
    "\n",
    "print(f\"Training data matrix X shape: {X.shape}\")\n",
    "print(f\"X is a {X.shape[0]} by {X.shape[1]} matrix\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f24f9255",
   "metadata": {},
   "source": [
    "**Implement code primitive: Construct a training label matrix (capital Y) by stacking individual training labels as columns.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c8e2e3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create training labels and stack them as columns\n",
    "m = 10  # number of training examples\n",
    "\n",
    "# Initialize training label matrix Y\n",
    "Y = np.zeros((1, m))\n",
    "\n",
    "# Fill Y with training labels (each column is one training label)\n",
    "for i in range(m):\n",
    "    # Generate a random binary label (0 or 1) for each training example\n",
    "    Y[0, i] = np.random.randint(0, 2)\n",
    "\n",
    "print(f\"Training label matrix Y shape: {Y.shape}\")\n",
    "print(f\"Y is a {Y.shape[0]} by {Y.shape[1]} matrix\")\n",
    "print(f\"Labels: {Y}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfc0f020",
   "metadata": {},
   "source": [
    "**Implement code primitive: Determine the shape of a matrix (e.g., using .shape in Python).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "571a2938",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine the shape of matrices using .shape\n",
    "print(f\"Shape of X: {X.shape}\")\n",
    "print(f\"Shape of Y: {Y.shape}\")\n",
    "print(f\"Shape of R: {R.shape}\")\n",
    "\n",
    "# Extract individual dimensions\n",
    "n_x_actual, m_actual = X.shape\n",
    "print(f\"\\nX has {n_x_actual} features and {m_actual} training examples\")\n",
    "\n",
    "y_rows, y_cols = Y.shape\n",
    "print(f\"Y has {y_rows} row(s) and {y_cols} column(s)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "825c593e",
   "metadata": {},
   "source": [
    "**Create a Mermaid diagram: graph LR\n",
    "    subgraph Image Pixels\n",
    "        R[R Channel Matrix]\n",
    "        G[G Channel Matrix]\n",
    "        B[B Channel Matrix]\n",
    "    end\n",
    "    R --> Unroll\n",
    "    G --> Unroll\n",
    "    B --> Unroll\n",
    "    Unroll --> x(Feature Vector x)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68e27385",
   "metadata": {},
   "source": [
    "```mermaid\n",
    "graph LR\n",
    "    subgraph Image Pixels\n",
    "        R[R Channel Matrix]\n",
    "        G[G Channel Matrix]\n",
    "        B[B Channel Matrix]\n",
    "    end\n",
    "    R --> Unroll\n",
    "    G --> Unroll\n",
    "    B --> Unroll\n",
    "    Unroll --> x(Feature Vector x)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79ca1d2f",
   "metadata": {},
   "source": [
    "**Create a Mermaid diagram: graph TD\n",
    "    subgraph Training Examples\n",
    "        x1(x^(1))\n",
    "        x2(x^(2))\n",
    "        xm(x^(m))\n",
    "    end\n",
    "    x1 -- Column 1 --> X[Matrix X]\n",
    "    x2 -- Column 2 --> X\n",
    "    xm -- Column m --> X\n",
    "    X --- n_x_by_m[n_x by m matrix]**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f3c01a2",
   "metadata": {},
   "source": [
    "```mermaid\n",
    "graph TD\n",
    "    subgraph Training Examples\n",
    "        x1(x^(1))\n",
    "        x2(x^(2))\n",
    "        xm(x^(m))\n",
    "    end\n",
    "    x1 -- Column 1 --> X[Matrix X]\n",
    "    x2 -- Column 2 --> X\n",
    "    xm -- Column m --> X\n",
    "    X --- n_x_by_m[n_x by m matrix]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed53dcd9",
   "metadata": {},
   "source": [
    "**Create a Mermaid diagram: graph TD\n",
    "    subgraph Training Labels\n",
    "        y1(y^(1))\n",
    "        y2(y^(2))\n",
    "        ym(y^(m))\n",
    "    end\n",
    "    y1 -- Column 1 --> Y[Matrix Y]\n",
    "    y2 -- Column 2 --> Y\n",
    "    ym -- Column m --> Y\n",
    "    Y --- 1_by_m[1 by m matrix]**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9223ecbe",
   "metadata": {},
   "source": [
    "```mermaid\n",
    "graph TD\n",
    "    subgraph Training Labels\n",
    "        y1(y^(1))\n",
    "        y2(y^(2))\n",
    "        ym(y^(m))\n",
    "    end\n",
    "    y1 -- Column 1 --> Y[Matrix Y]\n",
    "    y2 -- Column 2 --> Y\n",
    "    ym -- Column m --> Y\n",
    "    Y --- 1_by_m[1 by m matrix]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c5787d6",
   "metadata": {},
   "source": [
    "## Lesson 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2b2a16d",
   "metadata": {},
   "source": [
    "**Explain the core concepts of the lesson**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2482d29",
   "metadata": {},
   "source": [
    "## Core Concepts of Logistic Regression\n",
    "\n",
    "Logistic regression is a learning algorithm designed for **binary classification problems**, where the output label $Y$ belongs to the set $\\{0, 1\\}$.\n",
    "\n",
    "The fundamental concepts are:\n",
    "\n",
    "- **Input Features** ($X$): The data we use to make predictions\n",
    "- **Output Label** ($Y$): The target we want to predict, which is either 0 or 1\n",
    "- **Parameters** ($W$ and $b$): Weights and bias that the model learns from data\n",
    "- **Prediction** ($\\hat{Y}$): The model's estimate of the probability that $Y = 1$ given the input $X$\n",
    "- **Probability Estimation**: The prediction must always be a valid probability, meaning it must be between 0 and 1\n",
    "\n",
    "The key insight is that logistic regression models the probability of the positive class (label 1) using a special mathematical function called the sigmoid function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a515e670",
   "metadata": {},
   "source": [
    "**Explain intuitions and mental models for the lesson**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18dc8980",
   "metadata": {},
   "source": [
    "## Intuitions Behind Logistic Regression\n",
    "\n",
    "**Why not use linear regression for classification?**\n",
    "\n",
    "In linear regression, we can fit a straight line to data, but the output can be any real number—it can be negative or greater than 1. For binary classification, we need predictions that represent probabilities, which must always be between 0 and 1. A linear function simply cannot guarantee this constraint.\n",
    "\n",
    "**The sigmoid function to the rescue:**\n",
    "\n",
    "The sigmoid function is a mathematical transformation that takes any real number as input and outputs a value strictly between 0 and 1. This makes it perfect for modeling probabilities:\n",
    "\n",
    "- When the input is a very large positive number, the sigmoid output approaches 1\n",
    "- When the input is a very large negative number, the sigmoid output approaches 0\n",
    "- When the input is 0, the sigmoid output is exactly 0.5\n",
    "\n",
    "**The logistic regression pipeline:**\n",
    "\n",
    "1. Compute a linear combination of features: $Z = W^T X + b$\n",
    "2. Apply the sigmoid function to transform $Z$ into a probability: $\\hat{Y} = \\sigma(Z)$\n",
    "3. Interpret $\\hat{Y}$ as the probability that $Y = 1$ given the input $X$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7abc458e",
   "metadata": {},
   "source": [
    "**Present and explain the key equations used in the lesson**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f51d20d4",
   "metadata": {},
   "source": [
    "## Key Equations in Logistic Regression\n",
    "\n",
    "**Output label constraint:**\n",
    "$$Y \\in \\{0, 1\\}$$\n",
    "\n",
    "The output is binary—either 0 or 1.\n",
    "\n",
    "**Prediction as probability:**\n",
    "$$\\hat{Y} = P(Y=1|X)$$\n",
    "\n",
    "The prediction represents the probability that $Y$ equals 1 given the input features $X$.\n",
    "\n",
    "**Linear combination:**\n",
    "$$Z = W^T X + b$$\n",
    "\n",
    "We compute a weighted sum of the input features plus a bias term.\n",
    "\n",
    "**Sigmoid transformation:**\n",
    "$$\\hat{Y} = \\sigma(Z)$$\n",
    "\n",
    "We apply the sigmoid function to the linear combination.\n",
    "\n",
    "**Sigmoid function definition:**\n",
    "$$\\sigma(Z) = \\frac{1}{1 + e^{-Z}}$$\n",
    "\n",
    "The sigmoid function maps any real number to a value between 0 and 1, making it suitable for probability estimation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6dc85f1",
   "metadata": {},
   "source": [
    "**Implement code primitive: Implement the logistic regression model to compute the prediction (Y_hat) given input features (X) and parameters (W, b).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "751c6213",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def sigmoid(Z):\n",
    "    \"\"\"\n",
    "    Compute the sigmoid function.\n",
    "    \n",
    "    Parameters:\n",
    "    Z: Linear combination (scalar or array)\n",
    "    \n",
    "    Returns:\n",
    "    Sigmoid output (scalar or array with values between 0 and 1)\n",
    "    \"\"\"\n",
    "    return 1 / (1 + np.exp(-Z))\n",
    "\n",
    "def logistic_regression_predict(X, W, b):\n",
    "    \"\"\"\n",
    "    Compute the logistic regression prediction.\n",
    "    \n",
    "    Parameters:\n",
    "    X: Input features (shape: (n_features, n_samples))\n",
    "    W: Weights (shape: (n_features,))\n",
    "    b: Bias (scalar)\n",
    "    \n",
    "    Returns:\n",
    "    Y_hat: Predicted probabilities (shape: (n_samples,))\n",
    "    \"\"\"\n",
    "    Z = np.dot(W, X) + b\n",
    "    Y_hat = sigmoid(Z)\n",
    "    return Y_hat\n",
    "\n",
    "# Example usage\n",
    "X = np.array([[1, 2, 3], [4, 5, 6]])  # 2 features, 3 samples\n",
    "W = np.array([0.5, -0.3])  # 2 weights\n",
    "b = 0.1  # bias\n",
    "\n",
    "Y_hat = logistic_regression_predict(X, W, b)\n",
    "print(\"Predictions:\", Y_hat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aac532e",
   "metadata": {},
   "source": [
    "**Implement code primitive: Develop a process to learn optimal parameters (W and b) such that Y_hat accurately estimates the probability of Y being 1.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bbcd3a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def sigmoid(Z):\n",
    "    return 1 / (1 + np.exp(-Z))\n",
    "\n",
    "def compute_cost(Y_hat, Y):\n",
    "    \"\"\"\n",
    "    Compute the binary cross-entropy cost function.\n",
    "    \n",
    "    Parameters:\n",
    "    Y_hat: Predicted probabilities (shape: (n_samples,))\n",
    "    Y: True labels (shape: (n_samples,))\n",
    "    \n",
    "    Returns:\n",
    "    cost: Average cost across all samples\n",
    "    \"\"\"\n",
    "    m = Y.shape[0]\n",
    "    cost = -np.mean(Y * np.log(Y_hat + 1e-8) + (1 - Y) * np.log(1 - Y_hat + 1e-8))\n",
    "    return cost\n",
    "\n",
    "def gradient_descent(X, Y, W, b, learning_rate=0.01, iterations=100):\n",
    "    \"\"\"\n",
    "    Learn optimal parameters W and b using gradient descent.\n",
    "    \n",
    "    Parameters:\n",
    "    X: Input features (shape: (n_features, n_samples))\n",
    "    Y: True labels (shape: (n_samples,))\n",
    "    W: Initial weights (shape: (n_features,))\n",
    "    b: Initial bias (scalar)\n",
    "    learning_rate: Step size for parameter updates\n",
    "    iterations: Number of gradient descent iterations\n",
    "    \n",
    "    Returns:\n",
    "    W: Learned weights\n",
    "    b: Learned bias\n",
    "    costs: List of costs at each iteration\n",
    "    \"\"\"\n",
    "    m = Y.shape[0]\n",
    "    costs = []\n",
    "    \n",
    "    for i in range(iterations):\n",
    "        Z = np.dot(W, X) + b\n",
    "        Y_hat = sigmoid(Z)\n",
    "        \n",
    "        dZ = Y_hat - Y\n",
    "        dW = np.dot(dZ, X.T) / m\n",
    "        db = np.mean(dZ)\n",
    "        \n",
    "        W = W - learning_rate * dW\n",
    "        b = b - learning_rate * db\n",
    "        \n",
    "        cost = compute_cost(Y_hat, Y)\n",
    "        costs.append(cost)\n",
    "    \n",
    "    return W, b, costs\n",
    "\n",
    "# Example usage\n",
    "X = np.array([[1, 2, 3], [4, 5, 6]])  # 2 features, 3 samples\n",
    "Y = np.array([0, 1, 1])  # True labels\n",
    "W = np.zeros(2)  # Initialize weights\n",
    "b = 0.0  # Initialize bias\n",
    "\n",
    "W_learned, b_learned, costs = gradient_descent(X, Y, W, b, learning_rate=0.1, iterations=100)\n",
    "print(\"Learned W:\", W_learned)\n",
    "print(\"Learned b:\", b_learned)\n",
    "print(\"Final cost:\", costs[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8baf6512",
   "metadata": {},
   "source": [
    "## Lesson 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e9ed742",
   "metadata": {},
   "source": [
    "**Explain the core concepts of the lesson**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84b76d56",
   "metadata": {},
   "source": [
    "## Core Concepts\n",
    "\n",
    "Logistic regression is a fundamental classification algorithm that relies on two critical components: the **loss function** and the **cost function**.\n",
    "\n",
    "**Loss Function**: A loss function measures how well a model's prediction aligns with the true label for a single training example. For logistic regression, we use a specialized loss function that penalizes incorrect predictions in a way that leads to a convex optimization problem.\n",
    "\n",
    "**Cost Function**: The cost function aggregates the loss over all training examples, providing an overall measure of the model's performance on the entire training set. It is the objective function that we minimize during training.\n",
    "\n",
    "**Training Example Indexing**: In logistic regression, we denote the $i$-th training example as $(\\mathbf{x}^{(i)}, y^{(i)})$, where $\\mathbf{x}^{(i)}$ is the input feature vector and $y^{(i)}$ is the true binary label.\n",
    "\n",
    "**Convex vs. Non-convex Optimization**: Using squared error as a loss function in logistic regression results in a non-convex optimization problem with multiple local optima, making it difficult for gradient descent to find the global optimum. The logistic regression loss function is specifically designed to ensure a convex optimization problem, allowing gradient descent to reliably find the global minimum."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48cf908c",
   "metadata": {},
   "source": [
    "**Explain intuitions and mental models for the lesson**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40d798d5",
   "metadata": {},
   "source": [
    "## Intuitions and Mental Models\n",
    "\n",
    "**Why Loss Matters**: Think of the loss function as a report card for a single prediction. It tells us how far off our prediction is from the truth. A good loss function should heavily penalize confident wrong predictions while being lenient on uncertain predictions.\n",
    "\n",
    "**The Problem with Squared Error**: If we use squared error loss (common in linear regression), we create a landscape with many valleys and hills. Gradient descent might get stuck in a local valley and never reach the global optimum. This is the non-convex problem.\n",
    "\n",
    "**The Logistic Regression Loss Solution**: The logistic regression loss function is cleverly designed to create a smooth, bowl-shaped landscape (convex). No matter where we start, gradient descent will slide down to the single global minimum. This is guaranteed to work.\n",
    "\n",
    "**Predicted Output Design**: The logistic regression loss function is designed to make the predicted output $\\hat{y}$ close to 1 when the true label $y$ is 1, and close to 0 when $y$ is 0. When $\\hat{y}$ matches $y$, the loss is small; when they differ, the loss grows.\n",
    "\n",
    "**From Single to Aggregate**: The cost function takes all the individual losses and averages them. This gives us a single number that summarizes how well our model performs across the entire training set. We then optimize this single number to improve overall performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e291a5c",
   "metadata": {},
   "source": [
    "**Present and explain the key equations used in the lesson**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a46673e5",
   "metadata": {},
   "source": [
    "## Key Equations\n",
    "\n",
    "**Linear Combination**: For a given input $\\mathbf{x}^{(i)}$ and parameters $\\mathbf{w}$ and $b$, we compute:\n",
    "$$z^{(i)} = \\mathbf{w}^T\\mathbf{x}^{(i)} + b$$\n",
    "\n",
    "**Sigmoid Activation Function**: The sigmoid function maps any real number to a probability between 0 and 1:\n",
    "$$\\sigma(z) = \\frac{1}{1 + e^{-z}}$$\n",
    "\n",
    "**Predicted Output**: Combining the linear combination with the sigmoid activation:\n",
    "$$\\hat{y}^{(i)} = \\sigma(z^{(i)})$$\n",
    "\n",
    "Or equivalently:\n",
    "$$\\hat{y} = \\sigma(\\mathbf{w}^T\\mathbf{x} + b)$$\n",
    "\n",
    "**Logistic Regression Loss Function**: For a single training example, the loss is:\n",
    "$$L(\\hat{y}, y) = -(y \\log(\\hat{y}) + (1-y) \\log(1-\\hat{y}))$$\n",
    "\n",
    "This function ensures that:\n",
    "- When $y = 1$: loss = $-\\log(\\hat{y})$ (penalizes low predictions)\n",
    "- When $y = 0$: loss = $-\\log(1-\\hat{y})$ (penalizes high predictions)\n",
    "\n",
    "**Cost Function**: The average loss across all $m$ training examples:\n",
    "$$J(\\mathbf{w}, b) = \\frac{1}{m} \\sum_{i=1}^{m} L(\\hat{y}^{(i)}, y^{(i)})$$\n",
    "\n",
    "Expanded form:\n",
    "$$J(\\mathbf{w}, b) = -\\frac{1}{m} \\sum_{i=1}^{m} [y^{(i)} \\log(\\hat{y}^{(i)}) + (1-y^{(i)}) \\log(1-\\hat{y}^{(i)})]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c32ca38",
   "metadata": {},
   "source": [
    "**Implement code primitive: Compute the linear combination $z$ for a given input $x$ and parameters $W, b$.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab7fcdad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def compute_z(x, W, b):\n",
    "    \"\"\"\n",
    "    Compute the linear combination z = W^T * x + b\n",
    "    \n",
    "    Args:\n",
    "        x: Input feature vector (shape: (n_features,))\n",
    "        W: Weight vector (shape: (n_features,))\n",
    "        b: Bias term (scalar)\n",
    "    \n",
    "    Returns:\n",
    "        z: Linear combination (scalar)\n",
    "    \"\"\"\n",
    "    z = np.dot(W, x) + b\n",
    "    return z"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d60b4635",
   "metadata": {},
   "source": [
    "**Implement code primitive: Compute the sigmoid activation function.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d11635e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    \"\"\"\n",
    "    Compute the sigmoid activation function: sigma(z) = 1 / (1 + e^(-z))\n",
    "    \n",
    "    Args:\n",
    "        z: Input value or array (scalar or numpy array)\n",
    "    \n",
    "    Returns:\n",
    "        Sigmoid output (same shape as input, values between 0 and 1)\n",
    "    \"\"\"\n",
    "    sigma = 1 / (1 + np.exp(-z))\n",
    "    return sigma"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8ccf145",
   "metadata": {},
   "source": [
    "**Implement code primitive: Calculate the predicted output $\\hat{y}$ for a given training example.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b635399",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(x, W, b):\n",
    "    \"\"\"\n",
    "    Calculate the predicted output y_hat for a given training example.\n",
    "    \n",
    "    Args:\n",
    "        x: Input feature vector (shape: (n_features,))\n",
    "        W: Weight vector (shape: (n_features,))\n",
    "        b: Bias term (scalar)\n",
    "    \n",
    "    Returns:\n",
    "        y_hat: Predicted probability (scalar, between 0 and 1)\n",
    "    \"\"\"\n",
    "    z = compute_z(x, W, b)\n",
    "    y_hat = sigmoid(z)\n",
    "    return y_hat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05aa1c46",
   "metadata": {},
   "source": [
    "**Implement code primitive: Implement the logistic regression loss function for a single training example.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11d96dfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(y_hat, y):\n",
    "    \"\"\"\n",
    "    Compute the logistic regression loss for a single training example.\n",
    "    L(y_hat, y) = -(y * log(y_hat) + (1 - y) * log(1 - y_hat))\n",
    "    \n",
    "    Args:\n",
    "        y_hat: Predicted probability (scalar, between 0 and 1)\n",
    "        y: True label (scalar, 0 or 1)\n",
    "    \n",
    "    Returns:\n",
    "        L: Loss value (scalar)\n",
    "    \"\"\"\n",
    "    epsilon = 1e-15\n",
    "    y_hat = np.clip(y_hat, epsilon, 1 - epsilon)\n",
    "    L = -(y * np.log(y_hat) + (1 - y) * np.log(1 - y_hat))\n",
    "    return L"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4da584f",
   "metadata": {},
   "source": [
    "**Implement code primitive: Calculate the overall cost function for the entire training set.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e6458ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost(X, Y, W, b):\n",
    "    \"\"\"\n",
    "    Calculate the cost function for the entire training set.\n",
    "    J(W, b) = (1/m) * sum(L(y_hat^(i), y^(i))) for i = 1 to m\n",
    "    \n",
    "    Args:\n",
    "        X: Training feature matrix (shape: (m, n_features))\n",
    "        Y: Training labels vector (shape: (m,))\n",
    "        W: Weight vector (shape: (n_features,))\n",
    "        b: Bias term (scalar)\n",
    "    \n",
    "    Returns:\n",
    "        J: Cost value (scalar)\n",
    "    \"\"\"\n",
    "    m = X.shape[0]\n",
    "    total_loss = 0\n",
    "    \n",
    "    for i in range(m):\n",
    "        y_hat_i = predict(X[i], W, b)\n",
    "        total_loss += loss(y_hat_i, Y[i])\n",
    "    \n",
    "    J = total_loss / m\n",
    "    return J"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0776b3de",
   "metadata": {},
   "source": [
    "**Implement code primitive: Set up an optimization process to find parameters $W$ and $b$ that minimize the cost function $J$.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6ce0fda",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(X, Y, W, b, learning_rate=0.01, iterations=1000):\n",
    "    \"\"\"\n",
    "    Optimize parameters W and b using gradient descent to minimize cost function J.\n",
    "    \n",
    "    Args:\n",
    "        X: Training feature matrix (shape: (m, n_features))\n",
    "        Y: Training labels vector (shape: (m,))\n",
    "        W: Initial weight vector (shape: (n_features,))\n",
    "        b: Initial bias term (scalar)\n",
    "        learning_rate: Step size for gradient descent (default: 0.01)\n",
    "        iterations: Number of iterations (default: 1000)\n",
    "    \n",
    "    Returns:\n",
    "        W: Optimized weight vector\n",
    "        b: Optimized bias term\n",
    "        costs: List of cost values at each iteration\n",
    "    \"\"\"\n",
    "    m = X.shape[0]\n",
    "    costs = []\n",
    "    \n",
    "    for iteration in range(iterations):\n",
    "        dW = np.zeros_like(W)\n",
    "        db = 0\n",
    "        \n",
    "        for i in range(m):\n",
    "            y_hat_i = predict(X[i], W, b)\n",
    "            error = y_hat_i - Y[i]\n",
    "            dW += error * X[i]\n",
    "            db += error\n",
    "        \n",
    "        dW /= m\n",
    "        db /= m\n",
    "        \n",
    "        W = W - learning_rate * dW\n",
    "        b = b - learning_rate * db\n",
    "        \n",
    "        J = cost(X, Y, W, b)\n",
    "        costs.append(J)\n",
    "    \n",
    "    return W, b, costs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a98561a5",
   "metadata": {},
   "source": [
    "## Lesson 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f0401d6",
   "metadata": {},
   "source": [
    "**Explain the core concepts of the lesson**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7df895bf",
   "metadata": {},
   "source": [
    "## Core Concepts of Gradient Descent for Logistic Regression\n",
    "\n",
    "Gradient descent is an optimization algorithm used to find the parameters that minimize a cost function. In the context of logistic regression, we aim to find the optimal values of parameters $w$ and $b$ that make our model's predictions as accurate as possible.\n",
    "\n",
    "**Key Concepts:**\n",
    "\n",
    "- **Cost Function Minimization**: The goal is to find parameters that make the cost function as small as possible. The cost function measures how well our model performs on the training data.\n",
    "\n",
    "- **Convex Cost Function**: The cost function for logistic regression is convex, meaning it has a single global minimum. This guarantees that gradient descent will find the optimal solution regardless of where we initialize the parameters.\n",
    "\n",
    "- **Non-convex Functions**: In contrast, some functions have multiple local optima, making optimization more challenging.\n",
    "\n",
    "- **Global Optimum vs. Local Optima**: A global optimum is the best solution across the entire parameter space, while local optima are solutions that are better than nearby points but not necessarily the best overall.\n",
    "\n",
    "- **Parameter Initialization**: We start the algorithm by initializing parameters $w$ and $b$ to specific values, typically zero.\n",
    "\n",
    "- **Learning Rate**: The learning rate $\\alpha$ controls the step size we take in each iteration. A larger learning rate means bigger steps, while a smaller learning rate means smaller, more careful steps.\n",
    "\n",
    "- **Derivative and Partial Derivative**: The derivative indicates the slope of the cost function. For functions with multiple parameters, we use partial derivatives to understand how the cost changes with respect to each parameter individually."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91b21ca7",
   "metadata": {},
   "source": [
    "**Explain intuitions and mental models for the lesson**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ed374c7",
   "metadata": {},
   "source": [
    "## Intuitions and Mental Models\n",
    "\n",
    "**The Landscape Analogy:**\n",
    "Imagine the cost function as a landscape with hills and valleys. Gradient descent is like walking downhill to find the lowest point. At each step, we look around to find the steepest downward direction and take a step in that direction.\n",
    "\n",
    "**Why Convexity Matters:**\n",
    "A convex cost function is like a single bowl—no matter where you start on the bowl's surface, if you always walk downhill, you'll eventually reach the same lowest point at the bottom. This is why logistic regression's convex cost function is so powerful: we're guaranteed to find the global optimum.\n",
    "\n",
    "**Following the Slope:**\n",
    "Gradient descent takes steps in the direction that decreases the cost most steeply. The derivative tells us the slope of the function at our current position. A steep slope means we should take a larger conceptual step (scaled by the learning rate), while a gentle slope means we're getting close to the minimum.\n",
    "\n",
    "**The Role of Derivatives:**\n",
    "The derivative indicates which direction to step to go downhill. For a function with multiple parameters like $J(w, b)$, we compute partial derivatives with respect to each parameter. These partial derivatives tell us how sensitive the cost is to changes in each parameter, guiding our updates."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6401b1b2",
   "metadata": {},
   "source": [
    "**Present and explain the key equations used in the lesson**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4153dce8",
   "metadata": {},
   "source": [
    "## Key Equations\n",
    "\n",
    "**Cost Function for Logistic Regression:**\n",
    "$$J(w, b) = -\\frac{1}{m} \\sum_{i=1}^m [y^{(i)}\\log(a^{(i)}) + (1-y^{(i)})\\log(1-a^{(i)})]$$\n",
    "\n",
    "where $m$ is the number of training examples, $y^{(i)}$ is the true label, and $a^{(i)}$ is the predicted probability.\n",
    "\n",
    "**Parameter Update Rules:**\n",
    "\n",
    "For parameter $w$:\n",
    "$$w := w - \\alpha \\frac{\\partial J(w,b)}{\\partial w}$$\n",
    "\n",
    "For parameter $b$:\n",
    "$$b := b - \\alpha \\frac{\\partial J(w,b)}{\\partial b}$$\n",
    "\n",
    "where $\\alpha$ is the learning rate, and $\\frac{\\partial J(w,b)}{\\partial w}$ and $\\frac{\\partial J(w,b)}{\\partial b}$ are the partial derivatives of the cost function with respect to $w$ and $b$ respectively.\n",
    "\n",
    "**Simplified Notation:**\n",
    "\n",
    "We often denote the partial derivatives as:\n",
    "- $dw = \\frac{\\partial J(w,b)}{\\partial w}$\n",
    "- $db = \\frac{\\partial J(w,b)}{\\partial b}$\n",
    "\n",
    "So the update rules become:\n",
    "$$w := w - \\alpha \\cdot dw$$\n",
    "$$b := b - \\alpha \\cdot db$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beac4763",
   "metadata": {},
   "source": [
    "**Implement code primitive: Initialize parameters w and b, typically to zero.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1812a6b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "w = 0\n",
    "b = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aef6ce8",
   "metadata": {},
   "source": [
    "**Implement code primitive: Implement an iterative update for parameter w using a learning rate and its derivative.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a788011",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.01\n",
    "w = w - learning_rate * dw"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "326e0a85",
   "metadata": {},
   "source": [
    "**Implement code primitive: Implement an iterative update for parameter b using a learning rate and its derivative.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4283ac6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.01\n",
    "b = b - learning_rate * db"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2374180",
   "metadata": {},
   "source": [
    "**Implement code primitive: Represent the derivative terms dJ/dw and dJ/db as variables `dw` and `db` in code.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7321a3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dw = 0.0\n",
    "db = 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad1dbe8c",
   "metadata": {},
   "source": [
    "**Create a Mermaid diagram: graph TD\n",
    "    A[Start Gradient Descent] --> B(Initialize W, B);\n",
    "    B --> C{Repeat until convergence};\n",
    "    C --> D[Compute dW = \\partial J/\\partial W];\n",
    "    C --> E[Compute dB = \\partial J/\\partial B];\n",
    "    D --> F[Update W = W - \\alpha * dW];\n",
    "    E --> G[Update B = B - \\alpha * dB];\n",
    "    F --> C;\n",
    "    G --> C;\n",
    "    C --> H[End: Optimal W, B found];**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66841971",
   "metadata": {},
   "source": [
    "```mermaid\n",
    "graph TD\n",
    "    A[Start Gradient Descent] --> B(Initialize W, B);\n",
    "    B --> C{Repeat until convergence};\n",
    "    C --> D[Compute dW = ∂J/∂W];\n",
    "    C --> E[Compute dB = ∂J/∂B];\n",
    "    D --> F[Update W = W - α * dW];\n",
    "    E --> G[Update B = B - α * dB];\n",
    "    F --> C;\n",
    "    G --> C;\n",
    "    C --> H[End: Optimal W, B found];\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1ea1e48",
   "metadata": {},
   "source": [
    "## Lesson 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15582c5a",
   "metadata": {},
   "source": [
    "**Explain the core concepts of the lesson**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7d47b95",
   "metadata": {},
   "source": [
    "## Core Concepts\n",
    "\n",
    "A **derivative** is fundamentally a measure of how a function changes. Specifically, it tells us the **slope** of a function—how much the output changes when we nudge the input by a tiny amount.\n",
    "\n",
    "For a linear function like $f(a) = 3a$, the slope is constant everywhere. This means that no matter where you are on the line, nudging the input by a small amount always causes the output to change by three times that amount.\n",
    "\n",
    "Key concepts:\n",
    "- **Slope of a function**: The rate at which the output changes relative to the input\n",
    "- **Rate of change**: How sensitive the output is to changes in the input\n",
    "- **Nudging a variable**: Making a small change to the input to observe the corresponding change in output\n",
    "- **Infinitesimal change**: In calculus, we consider the limit as the nudge approaches zero, but for intuition, a small nudge like 0.001 is sufficient\n",
    "- **Constant slope**: For linear functions, the slope is the same everywhere"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d12cc1b",
   "metadata": {},
   "source": [
    "**Explain intuitions and mental models for the lesson**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6883d87b",
   "metadata": {},
   "source": [
    "## Intuitions and Mental Models\n",
    "\n",
    "**The Slope as a Triangle**: Imagine a right triangle on the graph of a function. The height of the triangle represents the vertical change in the output (how much $f(a)$ changed), and the width represents the horizontal change in the input (how much $a$ changed). The slope is simply the ratio of height to width:\n",
    "\n",
    "$$\\text{slope} = \\frac{\\text{height}}{\\text{width}}$$\n",
    "\n",
    "**Constant Slope for Linear Functions**: For a straight line like $f(a) = 3a$, this ratio is always 3. Whether you look at the change from $a=0$ to $a=1$, or from $a=2$ to $a=3$, the output always changes by 3 times the input change.\n",
    "\n",
    "**Sensitivity Interpretation**: The derivative tells you how sensitive a function is to changes in its input. A steep slope (large derivative) means the output is very sensitive to input changes. A flat slope (small derivative) means the output is insensitive.\n",
    "\n",
    "**Practical Relevance**: In deep learning, you don't need to master calculus deeply. The key insight is that derivatives measure the sensitivity of outputs to input changes—this is essential for understanding how neural networks learn through gradient descent."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59cd6744",
   "metadata": {},
   "source": [
    "**Present and explain the key equations used in the lesson**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "759c1f98",
   "metadata": {},
   "source": [
    "## Key Equations\n",
    "\n",
    "The linear function we'll examine is:\n",
    "\n",
    "$$f(a) = 3a$$\n",
    "\n",
    "The derivative of this function is:\n",
    "\n",
    "$$\\frac{df(a)}{da} = 3$$\n",
    "\n",
    "Alternatively, this can be written as:\n",
    "\n",
    "$$\\frac{d}{da}f(a) = 3$$\n",
    "\n",
    "Both notations mean the same thing: the rate of change of $f(a)$ with respect to $a$ is 3.\n",
    "\n",
    "The general formula for slope as a ratio is:\n",
    "\n",
    "$$\\text{slope} = \\frac{\\text{height}}{\\text{width}} = \\frac{\\Delta f(a)}{\\Delta a}$$\n",
    "\n",
    "where $\\Delta f(a)$ is the change in the function output and $\\Delta a$ is the change in the input."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0612c197",
   "metadata": {},
   "source": [
    "**Implement code primitive: Evaluate a linear function f(a) = 3a at a specific point (e.g., a=2)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22ba8663",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(a):\n",
    "    return 3 * a\n",
    "\n",
    "a = 2\n",
    "result = f(a)\n",
    "print(f\"f({a}) = {result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac8a8460",
   "metadata": {},
   "source": [
    "**Implement code primitive: Compute the change in f(a) when a is nudged by a small amount (e.g., from a=2 to a=2.001)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d886fdfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "a1 = 2\n",
    "a2 = 2.001\n",
    "\n",
    "f_a1 = f(a1)\n",
    "f_a2 = f(a2)\n",
    "\n",
    "change_in_a = a2 - a1\n",
    "change_in_f = f_a2 - f_a1\n",
    "\n",
    "print(f\"f({a1}) = {f_a1}\")\n",
    "print(f\"f({a2}) = {f_a2}\")\n",
    "print(f\"Change in a: {change_in_a}\")\n",
    "print(f\"Change in f(a): {change_in_f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18fe9113",
   "metadata": {},
   "source": [
    "**Implement code primitive: Calculate the ratio of output change to input change to demonstrate slope**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64ec02db",
   "metadata": {},
   "outputs": [],
   "source": [
    "slope = change_in_f / change_in_a\n",
    "print(f\"Slope = {change_in_f} / {change_in_a} = {slope}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "914da7ab",
   "metadata": {},
   "source": [
    "**Implement code primitive: Verify that slope remains constant across different points on a linear function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73db7f37",
   "metadata": {},
   "outputs": [],
   "source": [
    "points = [0, 1, 5, 10]\n",
    "nudge = 0.001\n",
    "\n",
    "print(\"Verifying constant slope across different points:\")\n",
    "for a in points:\n",
    "    f_a = f(a)\n",
    "    f_a_nudged = f(a + nudge)\n",
    "    slope_at_a = (f_a_nudged - f_a) / nudge\n",
    "    print(f\"At a={a}: slope = {slope_at_a}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81505e13",
   "metadata": {},
   "source": [
    "**Create a Mermaid diagram: A coordinate plot showing the linear function f(a) = 3a with two points marked (a=2, f(a)=6) and (a=2.001, f(a)=6.003), with a right triangle highlighting the height (0.003) and width (0.001) to illustrate the slope calculation**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "572d6c5e",
   "metadata": {},
   "source": [
    "```mermaid\n",
    "graph TD\n",
    "    subgraph plot[\"Linear Function f(a) = 3a\"]\n",
    "        A[\"Point 1: (a=2, f(a)=6)\"]\n",
    "        B[\"Point 2: (a=2.001, f(a)=6.003)\"]\n",
    "        C[\"Width (Δa) = 0.001\"]\n",
    "        D[\"Height (Δf) = 0.003\"]\n",
    "        E[\"Slope = Height/Width = 0.003/0.001 = 3\"]\n",
    "    end\n",
    "    A --> C\n",
    "    B --> D\n",
    "    C --> E\n",
    "    D --> E\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ba8bbd6",
   "metadata": {},
   "source": [
    "## Lesson 6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52b98bf7",
   "metadata": {},
   "source": [
    "**Explain the core concepts of the lesson**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90cca770",
   "metadata": {},
   "source": [
    "## Core Concepts\n",
    "\n",
    "A **derivative** measures how much a function's output changes when you make a tiny change to its input. It represents the **slope of a function at a specific point**.\n",
    "\n",
    "Key ideas:\n",
    "- **Slope of a function**: The rate at which the output changes relative to the input. For straight lines, the slope is constant everywhere. For curved functions, the slope varies at different points.\n",
    "- **Rate of change**: How fast a function's output is changing at a particular input value.\n",
    "- **Infinitesimal nudge**: A conceptually infinitely small change to the input, used to define derivatives precisely.\n",
    "- **Variable-slope functions**: Functions like $f(a) = a^2$ where the slope is different at different points.\n",
    "- **Constant-slope functions**: Linear functions where the slope is the same everywhere.\n",
    "- **Derivative notation**: Written as $\\frac{d}{da}f(a)$, which means \"the derivative of $f$ with respect to $a$.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f46a5bc6",
   "metadata": {},
   "source": [
    "**Explain intuitions and mental models for the lesson**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d344f921",
   "metadata": {},
   "source": [
    "## Intuitions and Mental Models\n",
    "\n",
    "**The core intuition**: A derivative is the slope of a function at a point. Imagine zooming in on a curved graph until it looks like a straight line—the slope of that line is the derivative.\n",
    "\n",
    "**Why slopes vary**: Different points on a curved function have different slopes. For example, on $f(a) = a^2$, the function is flatter near $a = 0$ and steeper as $a$ increases.\n",
    "\n",
    "**The nudge principle**: When you nudge the input by a tiny amount $\\Delta a$, the output changes by approximately the derivative times that nudge. This is the practical way to estimate derivatives.\n",
    "\n",
    "**Infinitesimal vs. finite nudges**: Derivatives are defined using infinitesimally small nudges (conceptually zero, but not actually zero). When we use finite nudges like $0.001$, we get small approximation errors. The smaller the nudge, the better the approximation.\n",
    "\n",
    "**Using formulas**: Rather than computing derivatives from scratch each time, you can look up derivative formulas in a calculus table. Common formulas include $\\frac{d}{da}(a^2) = 2a$ and $\\frac{d}{da}(a^3) = 3a^2$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a52da23",
   "metadata": {},
   "source": [
    "**Present and explain the key equations used in the lesson**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39bd717e",
   "metadata": {},
   "source": [
    "## Key Equations\n",
    "\n",
    "**Definition of a derivative**:\n",
    "$$\\frac{d}{da}f(a) = \\text{slope at point } a$$\n",
    "\n",
    "**Common derivative formulas**:\n",
    "$$\\frac{d}{da}(a^2) = 2a$$\n",
    "\n",
    "$$\\frac{d}{da}(a^3) = 3a^2$$\n",
    "\n",
    "$$\\frac{d}{da}(\\log(a)) = \\frac{1}{a}$$\n",
    "\n",
    "**The nudge principle**:\n",
    "$$\\Delta f(a) \\approx \\frac{d}{da}f(a) \\cdot \\Delta a$$\n",
    "\n",
    "This equation tells us that the change in output ($\\Delta f(a)$) is approximately equal to the derivative at that point times the change in input ($\\Delta a$)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be33f55f",
   "metadata": {},
   "source": [
    "**Implement code primitive: Compute the output of a function at a given point (e.g., f(a) = a² at a=2)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a19c5514",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(a):\n",
    "    return a**2\n",
    "\n",
    "a = 2\n",
    "output = f(a)\n",
    "print(f\"f({a}) = {output}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adaaa064",
   "metadata": {},
   "source": [
    "**Implement code primitive: Nudge an input variable by a small amount and observe the change in output**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d24c04e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(a):\n",
    "    return a**2\n",
    "\n",
    "a = 2\n",
    "nudge = 0.001\n",
    "\n",
    "output_before = f(a)\n",
    "output_after = f(a + nudge)\n",
    "output_change = output_after - output_before\n",
    "\n",
    "print(f\"f({a}) = {output_before}\")\n",
    "print(f\"f({a + nudge}) = {output_after}\")\n",
    "print(f\"Change in output: {output_change}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fba3fd8a",
   "metadata": {},
   "source": [
    "**Implement code primitive: Calculate the ratio of output change to input change to estimate slope**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d48fd00",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(a):\n",
    "    return a**2\n",
    "\n",
    "a = 2\n",
    "nudge = 0.001\n",
    "\n",
    "output_before = f(a)\n",
    "output_after = f(a + nudge)\n",
    "output_change = output_after - output_before\n",
    "\n",
    "slope_estimate = output_change / nudge\n",
    "\n",
    "print(f\"Output change: {output_change}\")\n",
    "print(f\"Input change: {nudge}\")\n",
    "print(f\"Estimated slope (derivative): {slope_estimate}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bdcfa97",
   "metadata": {},
   "source": [
    "**Implement code primitive: Verify derivative formulas by comparing predicted changes (derivative × nudge) with actual computed changes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67cc390f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(a):\n",
    "    return a**2\n",
    "\n",
    "def derivative_f(a):\n",
    "    return 2 * a\n",
    "\n",
    "a = 2\n",
    "nudge = 0.001\n",
    "\n",
    "output_before = f(a)\n",
    "output_after = f(a + nudge)\n",
    "actual_change = output_after - output_before\n",
    "\n",
    "predicted_change = derivative_f(a) * nudge\n",
    "\n",
    "print(f\"Actual change in output: {actual_change}\")\n",
    "print(f\"Predicted change (derivative × nudge): {predicted_change}\")\n",
    "print(f\"Difference: {abs(actual_change - predicted_change)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a316ca3",
   "metadata": {},
   "source": [
    "**Create a Mermaid diagram: A flowchart showing the process of estimating a derivative: pick a point, nudge the input slightly, compute the output change, calculate the ratio of changes**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b10b3f7",
   "metadata": {},
   "source": [
    "## Process of Estimating a Derivative\n",
    "\n",
    "```mermaid\n",
    "graph TD\n",
    "    A[\"Pick a point a\"] --> B[\"Nudge input: compute f(a + Δa)\"]\n",
    "    B --> C[\"Compute output change: Δf = f(a + Δa) - f(a)\"]\n",
    "    C --> D[\"Calculate ratio: Δf / Δa\"]\n",
    "    D --> E[\"Ratio ≈ derivative at point a\"]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a806ca87",
   "metadata": {},
   "source": [
    "**Create a Mermaid diagram: A diagram illustrating how the slope (height-to-width ratio of a small triangle) varies at different points on a curved function like f(a) = a²**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "738ae2d3",
   "metadata": {},
   "source": [
    "## Slope Varies at Different Points\n",
    "\n",
    "```mermaid\n",
    "graph TD\n",
    "    A[\"Function: f(a) = a²\"] --> B[\"At a = 0: slope = 0 (flat)\"]\n",
    "    A --> C[\"At a = 1: slope = 2 (moderate)\"]\n",
    "    A --> D[\"At a = 2: slope = 4 (steep)\"]\n",
    "    A --> E[\"At a = 3: slope = 6 (steeper)\"]\n",
    "    B --> F[\"Slope = height/width of small triangle\"]\n",
    "    C --> F\n",
    "    D --> F\n",
    "    E --> F\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50718107",
   "metadata": {},
   "source": [
    "## Lesson 7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aec93d1",
   "metadata": {},
   "source": [
    "**Explain the core concepts of the lesson**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fb9a474",
   "metadata": {},
   "source": [
    "## Core Concepts\n",
    "\n",
    "A **computation graph** is a visual representation of how a calculation is organized, showing the flow of data from inputs to outputs through intermediate steps.\n",
    "\n",
    "Key concepts in computation graphs:\n",
    "\n",
    "- **Forward Pass**: The process of computing the output value by moving from left to right through the graph, starting from inputs and progressing through intermediate variables to the final output.\n",
    "\n",
    "- **Backward Pass**: The process of computing derivatives by moving from right to left through the graph, which enables efficient calculation of gradients for optimization.\n",
    "\n",
    "- **Intermediate Variables**: Variables computed during the forward pass that represent intermediate results in the calculation (e.g., $u$, $V$).\n",
    "\n",
    "- **Output Variable**: The final result of the computation (e.g., $J$), which is often the cost function in machine learning contexts.\n",
    "\n",
    "- **Gradients**: The derivatives of the output with respect to each input, computed during the backward pass.\n",
    "\n",
    "- **Derivatives**: Mathematical measures of how the output changes with respect to inputs, essential for optimization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f45451a3",
   "metadata": {},
   "source": [
    "**Explain intuitions and mental models for the lesson**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7327d10c",
   "metadata": {},
   "source": [
    "## Intuitions and Mental Models\n",
    "\n",
    "**Visual Organization of Computation**: A computation graph visually organizes the steps of a calculation from initial inputs to a final output. Think of it as a flowchart where each node represents a computation and each edge represents data flowing from one computation to the next. This visual structure makes it easy to understand the dependencies between variables.\n",
    "\n",
    "**Forward Pass as Left-to-Right Evaluation**: The forward pass computes the value of the output by moving from left to right through the graph. Start with the input values, compute each intermediate variable in sequence, and finally arrive at the output. This is the natural way we think about evaluating a mathematical expression.\n",
    "\n",
    "**Backward Pass as Right-to-Left Differentiation**: The backward pass computes derivatives by moving from right to left, which is a natural organization for optimization. Starting from the output, we work backwards through the graph, applying the chain rule to compute how changes in each variable affect the final output. This organization is the foundation of backpropagation in neural networks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "008f7494",
   "metadata": {},
   "source": [
    "**Present and explain the key equations used in the lesson**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "260d2fa8",
   "metadata": {},
   "source": [
    "## Key Equations\n",
    "\n",
    "The computation graph organizes the following equations:\n",
    "\n",
    "**Intermediate variable $u$:**\n",
    "$$u = bc$$\n",
    "\n",
    "**Intermediate variable $V$:**\n",
    "$$V = a + u$$\n",
    "\n",
    "**Final output $J$:**\n",
    "$$J = 3V$$\n",
    "\n",
    "**Combined form:**\n",
    "$$J = 3(a + bc)$$\n",
    "\n",
    "These equations define the forward pass computation. Each equation represents a node in the computation graph, and the variables flow from left to right as inputs are transformed into the final output."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ecb51c3",
   "metadata": {},
   "source": [
    "**Implement code primitive: Compute an intermediate variable 'u' as the product of 'b' and 'c'.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d4c4cbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "b = 2\n",
    "c = 3\n",
    "u = b * c\n",
    "print(f\"u = {u}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19d20744",
   "metadata": {},
   "source": [
    "**Implement code primitive: Compute an intermediate variable 'V' as the sum of 'a' and 'u'.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2757538b",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = 5\n",
    "V = a + u\n",
    "print(f\"V = {V}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fe0c3a4",
   "metadata": {},
   "source": [
    "**Implement code primitive: Compute the final output 'J' as three times 'V'.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff015177",
   "metadata": {},
   "outputs": [],
   "source": [
    "J = 3 * V\n",
    "print(f\"J = {J}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dbb27ec",
   "metadata": {},
   "source": [
    "**Create a Mermaid diagram: graph TD\n",
    "    A[a] --> U_node\n",
    "    B[b] --> U_node\n",
    "    C[c] --> U_node\n",
    "    U_node(u = b * c) --> V_node\n",
    "    A --> V_node\n",
    "    V_node(V = a + u) --> J_node\n",
    "    J_node(J = 3 * V)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ac2ddf7",
   "metadata": {},
   "source": [
    "## Computation Graph Visualization\n",
    "\n",
    "```mermaid\n",
    "graph TD\n",
    "    A[a] --> U_node\n",
    "    B[b] --> U_node\n",
    "    C[c] --> U_node\n",
    "    U_node(u = b * c) --> V_node\n",
    "    A --> V_node\n",
    "    V_node(V = a + u) --> J_node\n",
    "    J_node(J = 3 * V)\n",
    "```\n",
    "\n",
    "This diagram shows the flow of computation from inputs ($a$, $b$, $c$) through intermediate variables ($u$, $V$) to the final output ($J$). The forward pass evaluates from left to right, while the backward pass would compute gradients from right to left."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c2d45ad",
   "metadata": {},
   "source": [
    "## Lesson 8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4eaf124",
   "metadata": {},
   "source": [
    "**Explain the core concepts of the lesson**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17cf0cc1",
   "metadata": {},
   "source": [
    "## Core Concepts\n",
    "\n",
    "**Computation Graph**: A visual representation of how variables flow and transform through a series of calculations. Each node represents a variable or operation, and edges show dependencies between them.\n",
    "\n",
    "**Backpropagation**: An algorithm that computes derivatives by traversing a computation graph from the output backwards to the inputs. It efficiently calculates how changes in inputs affect the final output.\n",
    "\n",
    "**Chain Rule**: A fundamental calculus principle that allows us to compute derivatives of composite functions. If variable A affects B and B affects C, the total effect of A on C is the product of the individual effects: $\\frac{dC}{dA} = \\frac{dC}{dB} \\cdot \\frac{dB}{dA}$.\n",
    "\n",
    "**Partial Derivatives**: The rate of change of a function with respect to one variable while holding others constant. In a computation graph, we compute partial derivatives at each step.\n",
    "\n",
    "**Right-to-Left Computation**: The direction of backpropagation—we start from the final output and work backwards through the graph, computing derivatives as we go.\n",
    "\n",
    "**Intermediate Variables**: Variables that appear in the middle of a computation, neither at the input nor at the final output. Their derivatives are computed as stepping stones to reach the input derivatives.\n",
    "\n",
    "**Gradient Notation**: A shorthand way to represent derivatives. We use $dv$ to mean \"the derivative of the final output with respect to variable $v$\" rather than writing the full expression."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d54a9452",
   "metadata": {},
   "source": [
    "**Explain intuitions and mental models for the lesson**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bda0491",
   "metadata": {},
   "source": [
    "## Intuitions and Mental Models\n",
    "\n",
    "**The Flow Perspective**: Think of a computation graph as a river system. Water (the input) flows through channels (operations) and eventually reaches the ocean (the output). To understand how changes upstream affect the ocean, we trace the flow backwards from the ocean to the source.\n",
    "\n",
    "**Breaking Down Complexity**: The chain rule lets you break down complex derivatives into simpler pieces. Instead of computing one giant derivative, you compute small local derivatives at each step and multiply them together. This is like understanding a long causal chain by examining each link individually.\n",
    "\n",
    "**Efficiency Through Reuse**: Computing derivatives efficiently means working backwards through the graph, reusing previously computed derivatives to calculate new ones. Once you know how the output changes with respect to one intermediate variable, you can use that result to find how it changes with respect to earlier variables. This avoids redundant calculations and is much faster than computing each derivative independently.\n",
    "\n",
    "**Shorthand Notation**: In code, we use shorthand notation like $dv$ to mean the derivative of the final output with respect to variable $v$, rather than writing out the full derivative expression each time. This keeps the code clean and readable while maintaining mathematical precision."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39f2192f",
   "metadata": {},
   "source": [
    "**Present and explain the key equations used in the lesson**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30ba4474",
   "metadata": {},
   "source": [
    "## Key Equations\n",
    "\n",
    "Consider a computation graph where the final output $J$ depends on intermediate variables $u$ and $v$, which in turn depend on inputs $a$, $b$, and $c$.\n",
    "\n",
    "**Starting Point**: The derivative of the output with respect to itself is always 1, but in this example we compute:\n",
    "\n",
    "$$\\frac{dJ}{dv} = 3$$\n",
    "\n",
    "**Applying the Chain Rule**: To find how $J$ changes with respect to earlier variables, we multiply the derivative of $J$ with respect to the intermediate variable by the derivative of that intermediate variable with respect to the input:\n",
    "\n",
    "$$\\frac{dJ}{da} = \\frac{dJ}{dv} \\cdot \\frac{dv}{da}$$\n",
    "\n",
    "$$\\frac{dJ}{du} = \\frac{dJ}{dv} \\cdot \\frac{dv}{du}$$\n",
    "\n",
    "**Continuing Backwards**: As we move further back through the graph, we continue applying the chain rule:\n",
    "\n",
    "$$\\frac{dJ}{db} = \\frac{dJ}{du} \\cdot \\frac{du}{db}$$\n",
    "\n",
    "$$\\frac{dJ}{dc} = \\frac{dJ}{du} \\cdot \\frac{du}{dc}$$\n",
    "\n",
    "Each equation shows how to combine derivatives from the previous step with local derivatives to compute derivatives with respect to earlier variables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07c947d8",
   "metadata": {},
   "source": [
    "**Implement code primitive: Implement variable naming convention where 'dvar' represents the derivative of the final output variable J with respect to intermediate variable 'var'**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63e4aaec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variable naming convention: dvar = derivative of J with respect to var\n",
    "\n",
    "# Example computation\n",
    "a = 2\n",
    "b = 3\n",
    "c = 5\n",
    "\n",
    "# Forward pass\n",
    "u = a + b\n",
    "v = u * c\n",
    "J = 3 * v\n",
    "\n",
    "# Backward pass using naming convention\n",
    "dJ_dv = 3  # derivative of J with respect to v\n",
    "dJ_du = dJ_dv * c  # derivative of J with respect to u\n",
    "dJ_da = dJ_du * 1  # derivative of J with respect to a (du/da = 1)\n",
    "dJ_db = dJ_du * 1  # derivative of J with respect to b (du/db = 1)\n",
    "dJ_dc = dJ_dv * u  # derivative of J with respect to c (dv/dc = u)\n",
    "\n",
    "print(f\"dJ/dv = {dJ_dv}\")\n",
    "print(f\"dJ/du = {dJ_du}\")\n",
    "print(f\"dJ/da = {dJ_da}\")\n",
    "print(f\"dJ/db = {dJ_db}\")\n",
    "print(f\"dJ/dc = {dJ_dc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5c3a771",
   "metadata": {},
   "source": [
    "**Implement code primitive: Compute derivatives step-by-step by traversing the computation graph from right to left, storing intermediate derivative values**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e68c2979",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forward pass: compute J from inputs\n",
    "a = 2\n",
    "b = 3\n",
    "c = 5\n",
    "\n",
    "u = a + b\n",
    "v = u * c\n",
    "J = 3 * v\n",
    "\n",
    "print(f\"Forward pass:\")\n",
    "print(f\"u = a + b = {u}\")\n",
    "print(f\"v = u * c = {v}\")\n",
    "print(f\"J = 3 * v = {J}\")\n",
    "print()\n",
    "\n",
    "# Backward pass: traverse right to left, storing intermediate derivatives\n",
    "print(f\"Backward pass (right to left):\")\n",
    "\n",
    "# Step 1: Start from the output\n",
    "dJ_dv = 3\n",
    "print(f\"dJ/dv = {dJ_dv}\")\n",
    "\n",
    "# Step 2: Move to u\n",
    "dJ_du = dJ_dv * c\n",
    "print(f\"dJ/du = dJ/dv * dv/du = {dJ_dv} * {c} = {dJ_du}\")\n",
    "\n",
    "# Step 3: Move to a and b\n",
    "dJ_da = dJ_du * 1\n",
    "dJ_db = dJ_du * 1\n",
    "print(f\"dJ/da = dJ/du * du/da = {dJ_du} * 1 = {dJ_da}\")\n",
    "print(f\"dJ/db = dJ/du * du/db = {dJ_du} * 1 = {dJ_db}\")\n",
    "\n",
    "# Step 4: Move to c\n",
    "dJ_dc = dJ_dv * u\n",
    "print(f\"dJ/dc = dJ/dv * dv/dc = {dJ_dv} * {u} = {dJ_dc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3cdc324",
   "metadata": {},
   "source": [
    "**Implement code primitive: Use the chain rule to combine derivatives: multiply the derivative of the output with respect to an intermediate variable by the derivative of that intermediate variable with respect to the input**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95be11db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chain rule: dJ/dx = dJ/dy * dy/dx\n",
    "\n",
    "# Forward pass\n",
    "a = 2\n",
    "b = 3\n",
    "c = 5\n",
    "\n",
    "u = a + b\n",
    "v = u * c\n",
    "J = 3 * v\n",
    "\n",
    "# Backward pass: apply chain rule at each step\n",
    "print(\"Applying the chain rule:\")\n",
    "print()\n",
    "\n",
    "# dJ/dv is given\n",
    "dJ_dv = 3\n",
    "print(f\"dJ/dv = {dJ_dv}\")\n",
    "print()\n",
    "\n",
    "# Chain rule: dJ/du = dJ/dv * dv/du\n",
    "dv_du = c  # derivative of v with respect to u\n",
    "dJ_du = dJ_dv * dv_du\n",
    "print(f\"dJ/du = dJ/dv * dv/du = {dJ_dv} * {dv_du} = {dJ_du}\")\n",
    "print()\n",
    "\n",
    "# Chain rule: dJ/da = dJ/du * du/da\n",
    "du_da = 1  # derivative of u with respect to a\n",
    "dJ_da = dJ_du * du_da\n",
    "print(f\"dJ/da = dJ/du * du/da = {dJ_du} * {du_da} = {dJ_da}\")\n",
    "print()\n",
    "\n",
    "# Chain rule: dJ/db = dJ/du * du/db\n",
    "du_db = 1  # derivative of u with respect to b\n",
    "dJ_db = dJ_du * du_db\n",
    "print(f\"dJ/db = dJ/du * du/db = {dJ_du} * {du_db} = {dJ_db}\")\n",
    "print()\n",
    "\n",
    "# Chain rule: dJ/dc = dJ/dv * dv/dc\n",
    "dv_dc = u  # derivative of v with respect to c\n",
    "dJ_dc = dJ_dv * dv_dc\n",
    "print(f\"dJ/dc = dJ/dv * dv/dc = {dJ_dv} * {dv_dc} = {dJ_dc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc84e1b4",
   "metadata": {},
   "source": [
    "**Create a Mermaid diagram: A flowchart showing the forward pass (left to right) computing J from inputs a, b, c, u, v, and the backward pass (right to left) computing derivatives dJ/dv, dJ/da, dJ/du, dJ/db, dJ/dc**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ed00dcc",
   "metadata": {},
   "source": [
    "## Computation Graph: Forward and Backward Pass\n",
    "\n",
    "```mermaid\n",
    "graph TD\n",
    "    A[\"a\"] --> U[\"u = a + b\"]\n",
    "    B[\"b\"] --> U\n",
    "    U --> V[\"v = u * c\"]\n",
    "    C[\"c\"] --> V\n",
    "    V --> J[\"J = 3 * v\"]\n",
    "    \n",
    "    J --> dJ_dv[\"dJ/dv = 3\"]\n",
    "    dJ_dv --> dJ_du[\"dJ/du = dJ/dv * c\"]\n",
    "    dJ_dv --> dJ_dc[\"dJ/dc = dJ/dv * u\"]\n",
    "    dJ_du --> dJ_da[\"dJ/da = dJ/du * 1\"]\n",
    "    dJ_du --> dJ_db[\"dJ/db = dJ/du * 1\"]\n",
    "    \n",
    "    style A fill:#e1f5ff\n",
    "    style B fill:#e1f5ff\n",
    "    style C fill:#e1f5ff\n",
    "    style U fill:#fff3e0\n",
    "    style V fill:#fff3e0\n",
    "    style J fill:#f3e5f5\n",
    "    style dJ_dv fill:#f3e5f5\n",
    "    style dJ_du fill:#fff3e0\n",
    "    style dJ_dc fill:#fff3e0\n",
    "    style dJ_da fill:#e1f5ff\n",
    "    style dJ_db fill:#e1f5ff\n",
    "```\n",
    "\n",
    "The diagram shows two flows:\n",
    "- **Forward pass (top)**: Inputs $a$, $b$, $c$ flow left to right through intermediate variables $u$ and $v$ to produce output $J$.\n",
    "- **Backward pass (bottom)**: Starting from $J$, derivatives flow right to left, with each derivative computed using the chain rule from previously computed derivatives."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d0d8b7c",
   "metadata": {},
   "source": [
    "## Lesson 9"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33561690",
   "metadata": {},
   "source": [
    "**Explain the core concepts of the lesson**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fdc2e3a",
   "metadata": {},
   "source": [
    "## Core Concepts\n",
    "\n",
    "This lesson focuses on understanding how gradient descent works for logistic regression by computing derivatives for a single training example.\n",
    "\n",
    "**Logistic Regression Model**: A binary classification algorithm that uses a linear combination of inputs followed by a sigmoid activation function to produce a probability prediction.\n",
    "\n",
    "**Computation Graph**: A visual representation of how data flows through the model during forward propagation and how gradients flow backward during backpropagation.\n",
    "\n",
    "**Forward Propagation**: The process of computing predictions by passing inputs through the model. For logistic regression with two features:\n",
    "- Compute the linear combination: $Z = W_1X_1 + W_2X_2 + B$\n",
    "- Apply sigmoid activation: $A = \\sigma(Z)$\n",
    "- Calculate loss: $L(A, Y) = -(Y\\log(A) + (1-Y)\\log(1-A))$\n",
    "\n",
    "**Backward Propagation**: The process of computing how much each parameter contributes to the loss by applying the chain rule to traverse the computation graph in reverse.\n",
    "\n",
    "**Gradient Descent**: An optimization algorithm that updates model parameters iteratively by moving them in the direction opposite to the gradient, scaled by a learning rate $\\alpha$.\n",
    "\n",
    "**Partial Derivatives**: The rate of change of the loss with respect to each parameter. These derivatives guide parameter updates during training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8440403b",
   "metadata": {},
   "source": [
    "**Explain intuitions and mental models for the lesson**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e0940ba",
   "metadata": {},
   "source": [
    "## Intuitions and Mental Models\n",
    "\n",
    "**Computation Graphs as Blueprints**: Think of a computation graph as a blueprint showing how data flows through your model. Each node represents a calculation, and each edge represents data flowing from one calculation to the next. This visual structure makes it much easier to understand where gradients come from and how they propagate backward.\n",
    "\n",
    "**Forward vs. Backward**: During forward propagation, you're asking \"What prediction does my model make?\" During backward propagation, you're asking \"How much did each parameter contribute to the error?\" These two passes work together to improve the model.\n",
    "\n",
    "**The Chain Rule as a Multiplier**: The chain rule breaks down complex derivatives into simpler pieces. If you want to know how a parameter affects the loss, you multiply the derivatives along the path from that parameter to the loss. For example, to find how $W_1$ affects the loss, you compute: $\\frac{dL}{dW_1} = \\frac{dL}{dZ} \\cdot \\frac{dZ}{dW_1}$.\n",
    "\n",
    "**Gradient Descent as a Compass**: The gradient points in the direction of steepest increase in loss. Gradient descent moves parameters in the opposite direction (negative gradient) to reduce loss. The learning rate $\\alpha$ controls how large each step is—too large and you might overshoot the minimum, too small and training becomes very slow."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccb9b6da",
   "metadata": {},
   "source": [
    "**Present and explain the key equations used in the lesson**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b76f9059",
   "metadata": {},
   "source": [
    "## Key Equations\n",
    "\n",
    "**Forward Propagation**:\n",
    "\n",
    "$$Z = W_1X_1 + W_2X_2 + B$$\n",
    "\n",
    "$$A = \\sigma(Z) = \\frac{1}{1 + e^{-Z}}$$\n",
    "\n",
    "$$L(A, Y) = -(Y\\log(A) + (1-Y)\\log(1-A))$$\n",
    "\n",
    "**Backward Propagation (Derivatives)**:\n",
    "\n",
    "The derivative of loss with respect to the activation:\n",
    "$$\\frac{dL}{dA} = -\\frac{Y}{A} + \\frac{1-Y}{1-A}$$\n",
    "\n",
    "The derivative of activation with respect to the linear combination:\n",
    "$$\\frac{dA}{dZ} = A(1-A)$$\n",
    "\n",
    "Combining these using the chain rule:\n",
    "$$\\frac{dL}{dZ} = \\frac{dL}{dA} \\cdot \\frac{dA}{dZ} = A - Y$$\n",
    "\n",
    "Derivatives with respect to weights and bias:\n",
    "$$\\frac{dL}{dW_1} = X_1 \\cdot \\frac{dL}{dZ}$$\n",
    "\n",
    "$$\\frac{dL}{dW_2} = X_2 \\cdot \\frac{dL}{dZ}$$\n",
    "\n",
    "$$\\frac{dL}{dB} = \\frac{dL}{dZ}$$\n",
    "\n",
    "**Parameter Updates (Gradient Descent)**:\n",
    "\n",
    "$$W_1 := W_1 - \\alpha \\cdot \\frac{dL}{dW_1}$$\n",
    "\n",
    "$$W_2 := W_2 - \\alpha \\cdot \\frac{dL}{dW_2}$$\n",
    "\n",
    "$$B := B - \\alpha \\cdot \\frac{dL}{dB}$$\n",
    "\n",
    "where $\\alpha$ is the learning rate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c189039",
   "metadata": {},
   "source": [
    "**Implement code primitive: Implement the forward pass to compute Z, A (predicted output), and the loss L for a single training example.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a706e914",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def forward_pass(X1, X2, Y, W1, W2, B):\n",
    "    \"\"\"\n",
    "    Compute Z, A (predicted output), and loss L for a single training example.\n",
    "    \n",
    "    Args:\n",
    "        X1, X2: Input features\n",
    "        Y: Ground truth label (0 or 1)\n",
    "        W1, W2: Weights\n",
    "        B: Bias\n",
    "    \n",
    "    Returns:\n",
    "        Z: Linear combination\n",
    "        A: Sigmoid activation (prediction)\n",
    "        L: Binary cross-entropy loss\n",
    "    \"\"\"\n",
    "    Z = W1 * X1 + W2 * X2 + B\n",
    "    A = 1 / (1 + np.exp(-Z))\n",
    "    L = -(Y * np.log(A) + (1 - Y) * np.log(1 - A))\n",
    "    \n",
    "    return Z, A, L\n",
    "\n",
    "# Example usage\n",
    "X1, X2, Y = 2.0, 3.0, 1\n",
    "W1, W2, B = 0.5, -0.3, 0.1\n",
    "\n",
    "Z, A, L = forward_pass(X1, X2, Y, W1, W2, B)\n",
    "print(f\"Z = {Z:.4f}\")\n",
    "print(f\"A = {A:.4f}\")\n",
    "print(f\"L = {L:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb2f45b7",
   "metadata": {},
   "source": [
    "**Implement code primitive: Implement the backward pass to compute partial derivatives dL/dA, dL/dZ, dL/dW1, dL/dW2, and dL/dB for a single training example.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0aca902",
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_pass(X1, X2, Y, A, Z):\n",
    "    \"\"\"\n",
    "    Compute partial derivatives for a single training example.\n",
    "    \n",
    "    Args:\n",
    "        X1, X2: Input features\n",
    "        Y: Ground truth label\n",
    "        A: Sigmoid activation (prediction)\n",
    "        Z: Linear combination\n",
    "    \n",
    "    Returns:\n",
    "        dL_dA: Derivative of loss with respect to A\n",
    "        dL_dZ: Derivative of loss with respect to Z\n",
    "        dL_dW1: Derivative of loss with respect to W1\n",
    "        dL_dW2: Derivative of loss with respect to W2\n",
    "        dL_dB: Derivative of loss with respect to B\n",
    "    \"\"\"\n",
    "    dL_dA = -Y / A + (1 - Y) / (1 - A)\n",
    "    dA_dZ = A * (1 - A)\n",
    "    dL_dZ = dL_dA * dA_dZ\n",
    "    \n",
    "    dL_dW1 = X1 * dL_dZ\n",
    "    dL_dW2 = X2 * dL_dZ\n",
    "    dL_dB = dL_dZ\n",
    "    \n",
    "    return dL_dA, dL_dZ, dL_dW1, dL_dW2, dL_dB\n",
    "\n",
    "# Example usage (using values from forward pass)\n",
    "dL_dA, dL_dZ, dL_dW1, dL_dW2, dL_dB = backward_pass(X1, X2, Y, A, Z)\n",
    "print(f\"dL/dA = {dL_dA:.4f}\")\n",
    "print(f\"dL/dZ = {dL_dZ:.4f}\")\n",
    "print(f\"dL/dW1 = {dL_dW1:.4f}\")\n",
    "print(f\"dL/dW2 = {dL_dW2:.4f}\")\n",
    "print(f\"dL/dB = {dL_dB:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bd6c384",
   "metadata": {},
   "source": [
    "**Implement code primitive: Implement the parameter updates for W1, W2, and B using the calculated derivatives and a specified learning rate.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "401d3822",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters(W1, W2, B, dL_dW1, dL_dW2, dL_dB, learning_rate):\n",
    "    \"\"\"\n",
    "    Update parameters using gradient descent.\n",
    "    \n",
    "    Args:\n",
    "        W1, W2, B: Current parameters\n",
    "        dL_dW1, dL_dW2, dL_dB: Gradients\n",
    "        learning_rate: Learning rate (alpha)\n",
    "    \n",
    "    Returns:\n",
    "        W1_new, W2_new, B_new: Updated parameters\n",
    "    \"\"\"\n",
    "    W1_new = W1 - learning_rate * dL_dW1\n",
    "    W2_new = W2 - learning_rate * dL_dW2\n",
    "    B_new = B - learning_rate * dL_dB\n",
    "    \n",
    "    return W1_new, W2_new, B_new\n",
    "\n",
    "# Example usage\n",
    "learning_rate = 0.1\n",
    "W1_new, W2_new, B_new = update_parameters(W1, W2, B, dL_dW1, dL_dW2, dL_dB, learning_rate)\n",
    "print(f\"W1: {W1:.4f} -> {W1_new:.4f}\")\n",
    "print(f\"W2: {W2:.4f} -> {W2_new:.4f}\")\n",
    "print(f\"B: {B:.4f} -> {B_new:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffa11cf1",
   "metadata": {},
   "source": [
    "**Create a Mermaid diagram: graph TD\n",
    "    X1 --> Z_calc\n",
    "    X2 --> Z_calc\n",
    "    W1 --> Z_calc\n",
    "    W2 --> Z_calc\n",
    "    B --> Z_calc\n",
    "    Z_calc[Z = W1*X1 + W2*X2 + B] --> A_calc\n",
    "    A_calc[A = sigmoid(Z)] --> L_calc\n",
    "    Y(Ground Truth Y) --> L_calc\n",
    "    L_calc[L = Loss(A, Y)]**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94c61457",
   "metadata": {},
   "source": [
    "## Computation Graph for Logistic Regression\n",
    "\n",
    "```mermaid\n",
    "graph TD\n",
    "    X1 --> Z_calc\n",
    "    X2 --> Z_calc\n",
    "    W1 --> Z_calc\n",
    "    W2 --> Z_calc\n",
    "    B --> Z_calc\n",
    "    Z_calc[Z = W1*X1 + W2*X2 + B] --> A_calc\n",
    "    A_calc[A = sigmoid(Z)] --> L_calc\n",
    "    Y(Ground Truth Y) --> L_calc\n",
    "    L_calc[L = Loss(A, Y)]\n",
    "```\n",
    "\n",
    "This computation graph shows the forward propagation flow. During backward propagation, gradients flow in the reverse direction, allowing us to compute how each parameter contributes to the loss."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eb39be5",
   "metadata": {},
   "source": [
    "## Lesson 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53a29495",
   "metadata": {},
   "source": [
    "**Explain the core concepts of the lesson**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6684232b",
   "metadata": {},
   "source": [
    "## Core Concepts\n",
    "\n",
    "This lesson covers the fundamental components of implementing gradient descent for logistic regression with multiple training examples:\n",
    "\n",
    "**Cost Function for Multiple Examples**: The overall cost $J(w,b)$ aggregates the loss across all $m$ training examples:\n",
    "$$J(w,b) = -\\frac{1}{m} \\sum_{i=1}^{m} [y^{(i)} \\log a^{(i)} + (1 - y^{(i)}) \\log (1 - a^{(i)})]$$\n",
    "\n",
    "**Loss Function for Single Example**: For each training example, we compute the individual loss, which contributes to the overall cost.\n",
    "\n",
    "**Gradient Descent Algorithm**: An iterative optimization method that updates parameters by moving in the direction opposite to the gradient, scaled by a learning rate $\\alpha$.\n",
    "\n",
    "**Parameter Initialization**: Before training, parameters $w_1$, $w_2$, and $b$ are initialized (typically to zero).\n",
    "\n",
    "**Gradient Accumulators**: During the forward and backward pass through all examples, we accumulate gradients in variables like $dw_1$, $dw_2$, and $db$.\n",
    "\n",
    "**Derivative Averaging**: The accumulated gradients are divided by $m$ to obtain the average gradient, which represents the direction of steepest descent for the cost function.\n",
    "\n",
    "**Learning Rate**: A hyperparameter $\\alpha$ that controls the step size during parameter updates."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b230a19b",
   "metadata": {},
   "source": [
    "**Explain intuitions and mental models for the lesson**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "027b0ee8",
   "metadata": {},
   "source": [
    "## Intuitions and Mental Models\n",
    "\n",
    "**Averaging Gradients Across Examples**: To compute the overall gradient for the cost function across all training examples, you can average the derivatives calculated for each individual example. This is because the cost function itself is the average loss across all examples, so its gradient is naturally the average of individual gradients.\n",
    "\n",
    "**The Problem with Explicit For-Loops**: Using explicit for-loops in deep learning code makes algorithms inefficient, especially when working with large datasets. When you have millions of training examples, iterating through them one by one with Python loops becomes a computational bottleneck.\n",
    "\n",
    "**Vectorization as the Solution**: Vectorization is a critical technique in deep learning to eliminate explicit for-loops and significantly improve computational efficiency for large-scale data. By leveraging matrix operations and libraries like NumPy, we can process entire batches of examples simultaneously, achieving orders of magnitude speedup."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "556ef4c2",
   "metadata": {},
   "source": [
    "**Present and explain the key equations used in the lesson**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0ee2239",
   "metadata": {},
   "source": [
    "## Key Equations\n",
    "\n",
    "**Forward Propagation**:\n",
    "- Compute the linear combination: $z^{(i)} = w^T x^{(i)} + b$\n",
    "- Apply sigmoid activation: $a^{(i)} = \\sigma(z^{(i)})$\n",
    "\n",
    "**Cost Function**:\n",
    "$$J(w,b) = -\\frac{1}{m} \\sum_{i=1}^{m} [y^{(i)} \\log a^{(i)} + (1 - y^{(i)}) \\log (1 - a^{(i)})]$$\n",
    "\n",
    "**Backward Propagation**:\n",
    "- Compute the error for each example: $dz^{(i)} = a^{(i)} - y^{(i)}$\n",
    "- Compute gradients for each parameter:\n",
    "$$\\frac{\\partial J}{\\partial w_1} = \\frac{1}{m} \\sum_{i=1}^{m} x_1^{(i)} dz^{(i)}$$\n",
    "$$\\frac{\\partial J}{\\partial w_2} = \\frac{1}{m} \\sum_{i=1}^{m} x_2^{(i)} dz^{(i)}$$\n",
    "$$\\frac{\\partial J}{\\partial b} = \\frac{1}{m} \\sum_{i=1}^{m} dz^{(i)}$$\n",
    "\n",
    "**Parameter Update**:\n",
    "$$w_1 := w_1 - \\alpha \\cdot \\frac{\\partial J}{\\partial w_1}$$\n",
    "$$w_2 := w_2 - \\alpha \\cdot \\frac{\\partial J}{\\partial w_2}$$\n",
    "$$b := b - \\alpha \\cdot \\frac{\\partial J}{\\partial b}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60217a4b",
   "metadata": {},
   "source": [
    "**Implement code primitive: Initialize cost J and gradient accumulators (dw1, dw2, db) to zero.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b5b8576",
   "metadata": {},
   "outputs": [],
   "source": [
    "J = 0\n",
    "dw1 = 0\n",
    "dw2 = 0\n",
    "db = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed2a38c7",
   "metadata": {},
   "source": [
    "**Iterate through all 'm' training examples using a for loop.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "845f8f17",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(m):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10694436",
   "metadata": {},
   "source": [
    "**Implement code primitive: Inside the loop, compute z and the prediction a for the current example.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51e9bd2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "z = w1 * X[i, 0] + w2 * X[i, 1] + b\n",
    "a = 1 / (1 + np.exp(-z))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec3e63e9",
   "metadata": {},
   "source": [
    "**Implement code primitive: Accumulate the loss for the current example into J.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19b1ef1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "J += -(y[i] * np.log(a) + (1 - y[i]) * np.log(1 - a))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a90df22b",
   "metadata": {},
   "source": [
    "**Implement code primitive: Compute dz for the current example.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4493adb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "dz = a - y[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd37e54c",
   "metadata": {},
   "source": [
    "**Implement code primitive: Accumulate individual gradients dw1, dw2, and db based on dz and features.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe4026ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "dw1 += X[i, 0] * dz\n",
    "dw2 += X[i, 1] * dz\n",
    "db += dz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43019cf3",
   "metadata": {},
   "source": [
    "**Implement code primitive: After the loop, divide accumulated gradients (dw1, dw2, db) by 'm' to get the average gradients.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f15185b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "J = J / m\n",
    "dw1 = dw1 / m\n",
    "dw2 = dw2 / m\n",
    "db = db / m"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c3bcf4d",
   "metadata": {},
   "source": [
    "**Implement code primitive: Update parameters (w1, w2, b) using the learning rate and the computed average gradients.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1715239",
   "metadata": {},
   "outputs": [],
   "source": [
    "w1 = w1 - alpha * dw1\n",
    "w2 = w2 - alpha * dw2\n",
    "b = b - alpha * db"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "087cdddb",
   "metadata": {},
   "source": [
    "**Create a Mermaid diagram: A flowchart illustrating the iterative steps of gradient descent for logistic regression, including parameter initialization, the loop for processing each training example (forward propagation, loss accumulation, backpropagation for individual gradients), averaging of gradients, and parameter updates. The flowchart should emphasize the single step of gradient descent logic.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c085d86d",
   "metadata": {},
   "source": [
    "## Gradient Descent Flowchart\n",
    "\n",
    "```mermaid\n",
    "graph TD\n",
    "    A[\"Initialize: w1, w2, b, J=0, dw1=0, dw2=0, db=0\"] --> B[\"For each training example i=1 to m\"]\n",
    "    B --> C[\"Compute z = w1*x1 + w2*x2 + b\"]\n",
    "    C --> D[\"Compute a = sigmoid(z)\"]\n",
    "    D --> E[\"Accumulate loss: J += loss(a, y)\"]\n",
    "    E --> F[\"Compute dz = a - y\"]\n",
    "    F --> G[\"Accumulate gradients: dw1, dw2, db\"]\n",
    "    G --> H{\"More examples?\"}\n",
    "    H -->|Yes| B\n",
    "    H -->|No| I[\"Average gradients: dw1/m, dw2/m, db/m\"]\n",
    "    I --> J[\"Average cost: J/m\"]\n",
    "    J --> K[\"Update parameters: w1, w2, b\"]\n",
    "    K --> L[\"End of one gradient descent step\"]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7738cae",
   "metadata": {},
   "source": [
    "## Lesson 11"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23f494ab",
   "metadata": {},
   "source": [
    "**Explain the core concepts of the lesson**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7d540bf",
   "metadata": {},
   "source": [
    "## Core Concepts\n",
    "\n",
    "**Vectorization** is the practice of replacing explicit loops with built-in functions to perform computations more efficiently. In deep learning, vectorization is essential because it allows us to leverage hardware capabilities and built-in optimizations that significantly speed up code execution.\n",
    "\n",
    "Key concepts include:\n",
    "\n",
    "- **Explicit for loops**: Traditional Python loops that iterate through elements one at a time\n",
    "- **Vectorized implementation**: Using NumPy and other libraries to perform operations on entire arrays at once\n",
    "- **Non-vectorized implementation**: Implementation using explicit loops, which is slower\n",
    "- **Numpy dot product**: The `np.dot()` function performs efficient matrix/vector multiplication\n",
    "- **Deep learning performance**: Training neural networks on large datasets requires fast computation; vectorization is critical for reducing training time\n",
    "- **Hardware parallelization**: Modern CPUs and GPUs can execute multiple operations in parallel through SIMD instructions and other mechanisms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "663ed10f",
   "metadata": {},
   "source": [
    "**Explain intuitions and mental models for the lesson**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5d5c1bb",
   "metadata": {},
   "source": [
    "## Intuitions and Mental Models\n",
    "\n",
    "**Why Vectorization Matters:**\n",
    "\n",
    "Vectorization is the practice of replacing explicit loops with built-in functions, leading to significantly faster code execution. When you write a Python for loop, the interpreter processes each iteration sequentially. In contrast, vectorized operations allow libraries like NumPy to delegate computation to optimized C code and hardware accelerators.\n",
    "\n",
    "**Performance in Deep Learning:**\n",
    "\n",
    "Faster code is crucial for deep learning because training on large datasets can otherwise take a very long time, hindering the iterative experimentation cycle. When you're experimenting with different architectures or hyperparameters, slow training times make iteration impractical.\n",
    "\n",
    "**Hardware Acceleration:**\n",
    "\n",
    "Vectorized operations leverage hardware capabilities like SIMD instructions in CPUs and GPUs to perform computations in parallel, boosting efficiency. By avoiding explicit for loops, Python libraries like NumPy can take advantage of underlying parallelization for performance gains. This means the same computation can run orders of magnitude faster when vectorized.\n",
    "\n",
    "**The Key Insight:**\n",
    "\n",
    "The difference between vectorized and non-vectorized code is not just about convenience—it's about unlocking the full computational power of modern hardware."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aeba4c3",
   "metadata": {},
   "source": [
    "**Present and explain the key equations used in the lesson**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a387a581",
   "metadata": {},
   "source": [
    "## Key Equations\n",
    "\n",
    "In deep learning, one of the fundamental computations is the linear transformation used in logistic regression and neural networks:\n",
    "\n",
    "$$Z = W^T X + B$$\n",
    "\n",
    "Where:\n",
    "- $Z$ is the output (predictions or activations)\n",
    "- $W$ is the weight matrix\n",
    "- $X$ is the input data\n",
    "- $B$ is the bias term\n",
    "- $W^T$ denotes the transpose of $W$\n",
    "\n",
    "This equation involves a dot product ($W^T X$), which is a perfect candidate for vectorization. Computing this efficiently across millions of samples is where vectorization provides the most dramatic performance improvements."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46b0d46d",
   "metadata": {},
   "source": [
    "**Implement code primitive: Importing numpy and time libraries.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "382c6bf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d83116c5",
   "metadata": {},
   "source": [
    "**Implement code primitive: Generating large random NumPy arrays (e.g., million-dimensional).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb29e01e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate large random arrays\n",
    "n = 1000000\n",
    "a = np.random.rand(n)\n",
    "b = np.random.rand(n)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5041a881",
   "metadata": {},
   "source": [
    "**Implement code primitive: Measuring code execution time using `time.time()`.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccb34e43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: measuring execution time\n",
    "start_time = time.time()\n",
    "# ... code to measure ...\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "print(f\"Elapsed time: {elapsed_time:.6f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de5e0ff5",
   "metadata": {},
   "source": [
    "**Implement code primitive: Implementing a vectorized dot product using `np.dot(a, b)`.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ae314a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorized dot product\n",
    "start_time = time.time()\n",
    "z_vectorized = np.dot(a, b)\n",
    "end_time = time.time()\n",
    "time_vectorized = end_time - start_time\n",
    "print(f\"Vectorized dot product: {z_vectorized}\")\n",
    "print(f\"Vectorized time: {time_vectorized:.6f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1687d90",
   "metadata": {},
   "source": [
    "**Implement code primitive: Implementing a non-vectorized dot product using an explicit for loop.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fa43518",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Non-vectorized dot product using explicit for loop\n",
    "start_time = time.time()\n",
    "z_non_vectorized = 0\n",
    "for i in range(n):\n",
    "    z_non_vectorized += a[i] * b[i]\n",
    "end_time = time.time()\n",
    "time_non_vectorized = end_time - start_time\n",
    "print(f\"Non-vectorized dot product: {z_non_vectorized}\")\n",
    "print(f\"Non-vectorized time: {time_non_vectorized:.6f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3041dd55",
   "metadata": {},
   "source": [
    "**Implement code primitive: Comparing the execution times of vectorized versus non-vectorized implementations.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4281e6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare execution times\n",
    "speedup = time_non_vectorized / time_vectorized\n",
    "print(f\"\\nComparison:\")\n",
    "print(f\"Vectorized time: {time_vectorized:.6f} seconds\")\n",
    "print(f\"Non-vectorized time: {time_non_vectorized:.6f} seconds\")\n",
    "print(f\"Speedup factor: {speedup:.1f}x\")\n",
    "print(f\"\\nVectorization is {speedup:.1f} times faster!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20127620",
   "metadata": {},
   "source": [
    "## Lesson 12"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1253ff56",
   "metadata": {},
   "source": [
    "**Explain the core concepts of the lesson**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ba5abd9",
   "metadata": {},
   "source": [
    "## Core Concepts of Vectorization\n",
    "\n",
    "**Vectorization** is the practice of replacing explicit for-loops with built-in functions and NumPy operations that work on entire arrays at once. This approach leverages optimized, compiled code that executes much faster than Python loops.\n",
    "\n",
    "Key concepts include:\n",
    "\n",
    "- **Explicit for-loops**: Traditional Python loops that iterate over individual elements or training examples. These are slow because Python interprets each iteration.\n",
    "- **Built-in functions**: NumPy and other libraries provide optimized functions that perform operations on entire vectors or matrices without explicit loops.\n",
    "- **Matrix-vector multiplication**: A fundamental operation where each element of the output is computed as a dot product of a matrix row with a vector.\n",
    "- **Element-wise operations**: Operations applied independently to each element of a vector or matrix, such as exponential, logarithm, or squaring.\n",
    "- **Non-vectorized vs. Vectorized implementations**: Non-vectorized code uses explicit loops; vectorized code uses NumPy functions to process entire arrays simultaneously.\n",
    "\n",
    "In the context of machine learning, vectorization is critical for efficient gradient computation and parameter updates in algorithms like logistic regression."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "966b9b83",
   "metadata": {},
   "source": [
    "**Explain intuitions and mental models for the lesson**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86393b87",
   "metadata": {},
   "source": [
    "## Intuitions Behind Vectorization\n",
    "\n",
    "**Why vectorization matters:**\n",
    "\n",
    "1. **Speed through optimization**: Avoiding explicit for-loops by using built-in functions significantly speeds up code execution. NumPy functions are written in C and optimized for numerical computation, making them orders of magnitude faster than Python loops.\n",
    "\n",
    "2. **Leveraging library efficiency**: NumPy functions are optimized to perform operations on entire vectors or matrices much faster than manual looping. The library handles memory layout, CPU caching, and parallel operations automatically.\n",
    "\n",
    "3. **Incremental improvements**: Even partial vectorization, like eliminating one for-loop, can lead to substantial performance improvements. You don't need to vectorize everything at once—removing bottleneck loops has immediate benefits.\n",
    "\n",
    "4. **Scaling to datasets**: Full vectorization allows processing entire datasets simultaneously without any explicit loops over training examples. This is essential for handling large datasets efficiently.\n",
    "\n",
    "**Mental model**: Think of vectorization as delegating work to a highly optimized specialist (NumPy) instead of doing it yourself (Python loops). The specialist knows tricks and shortcuts that make the work much faster."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8108aa70",
   "metadata": {},
   "source": [
    "**Present and explain the key equations used in the lesson**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd7f1bc2",
   "metadata": {},
   "source": [
    "## Key Equations\n",
    "\n",
    "**Matrix-Vector Multiplication**\n",
    "\n",
    "The fundamental operation in vectorization is computing the product of a matrix $A$ and a vector $V$. The $i$-th element of the result $U$ is:\n",
    "\n",
    "$$U_i = \\sum_j A_{ij} V_j$$\n",
    "\n",
    "This equation represents the dot product of the $i$-th row of matrix $A$ with the vector $V$. In non-vectorized form, this requires nested loops: one over rows $i$ and one over columns $j$. In vectorized form, this entire computation is replaced by a single call to `np.dot(A, V)` or `A @ V`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2ef2dea",
   "metadata": {},
   "source": [
    "**Implement code primitive: Non-vectorized matrix-vector multiplication using nested Python for-loops.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4855f438",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Non-vectorized matrix-vector multiplication\n",
    "A = np.array([[1, 2, 3],\n",
    "              [4, 5, 6],\n",
    "              [7, 8, 9]])\n",
    "V = np.array([1, 2, 3])\n",
    "\n",
    "U = np.zeros(A.shape[0])\n",
    "for i in range(A.shape[0]):\n",
    "    for j in range(A.shape[1]):\n",
    "        U[i] += A[i, j] * V[j]\n",
    "\n",
    "print(\"Non-vectorized result:\")\n",
    "print(U)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3082ce23",
   "metadata": {},
   "source": [
    "**Implement code primitive: Vectorized matrix-vector multiplication using `np.dot(A, v)`.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e06fdb09",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Vectorized matrix-vector multiplication\n",
    "A = np.array([[1, 2, 3],\n",
    "              [4, 5, 6],\n",
    "              [7, 8, 9]])\n",
    "V = np.array([1, 2, 3])\n",
    "\n",
    "U = np.dot(A, V)\n",
    "\n",
    "print(\"Vectorized result:\")\n",
    "print(U)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eca6a9d",
   "metadata": {},
   "source": [
    "**Implement code primitive: Non-vectorized element-wise exponential operation using a Python for-loop.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f94d606f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Non-vectorized element-wise exponential\n",
    "V = np.array([1, 2, 3, 4])\n",
    "\n",
    "result = np.zeros(len(V))\n",
    "for i in range(len(V)):\n",
    "    result[i] = np.exp(V[i])\n",
    "\n",
    "print(\"Non-vectorized exponential:\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35be68fb",
   "metadata": {},
   "source": [
    "**Implement code primitive: Vectorized element-wise exponential operation using `np.exp(v)`.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb4c9e59",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Vectorized element-wise exponential\n",
    "V = np.array([1, 2, 3, 4])\n",
    "\n",
    "result = np.exp(V)\n",
    "\n",
    "print(\"Vectorized exponential:\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cab6cc7b",
   "metadata": {},
   "source": [
    "**Implement code primitive: Demonstration of other NumPy element-wise functions like `np.log(v)`, `np.abs(v)`, `np.maximum(v, 0)`, `v**2`, `1/v`.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef1d8e69",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "V = np.array([1, 2, 3, 4])\n",
    "\n",
    "print(\"Logarithm:\")\n",
    "print(np.log(V))\n",
    "\n",
    "print(\"\\nAbsolute value:\")\n",
    "print(np.abs(np.array([-1, -2, 3, -4])))\n",
    "\n",
    "print(\"\\nMaximum with 0 (ReLU):\")\n",
    "print(np.maximum(np.array([-1, 2, -3, 4]), 0))\n",
    "\n",
    "print(\"\\nSquaring:\")\n",
    "print(V**2)\n",
    "\n",
    "print(\"\\nReciprocal:\")\n",
    "print(1/V)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fbc34c7",
   "metadata": {},
   "source": [
    "**Implement code primitive: Transforming logistic regression derivative calculation `dw` from individual components to a vectorized `np.zeros` array.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24321281",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Example: logistic regression with m training examples and n features\n",
    "m = 5  # number of training examples\n",
    "n = 3  # number of features\n",
    "\n",
    "# Non-vectorized: initialize dw with individual components\n",
    "dw_non_vec = 0\n",
    "for i in range(n):\n",
    "    dw_non_vec = 0  # reset for each feature (inefficient)\n",
    "\n",
    "# Vectorized: initialize dw as a zero array\n",
    "dw = np.zeros(n)\n",
    "\n",
    "print(\"Vectorized dw shape:\")\n",
    "print(dw.shape)\n",
    "print(\"Vectorized dw:\")\n",
    "print(dw)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdc3263d",
   "metadata": {},
   "source": [
    "**Implement code primitive: Replacing a for-loop over features with a vectorized update `dw += x_i * dz_i`.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b34a780c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Example data\n",
    "m = 5  # training examples\n",
    "n = 3  # features\n",
    "X = np.random.randn(n, m)  # feature matrix (n x m)\n",
    "dz = np.random.randn(m)    # gradient signal (m,)\n",
    "\n",
    "# Non-vectorized: loop over features\n",
    "dw_non_vec = np.zeros(n)\n",
    "for i in range(n):\n",
    "    for j in range(m):\n",
    "        dw_non_vec[i] += X[i, j] * dz[j]\n",
    "\n",
    "# Vectorized: single matrix-vector multiplication\n",
    "dw = np.dot(X, dz)\n",
    "\n",
    "print(\"Non-vectorized dw:\")\n",
    "print(dw_non_vec)\n",
    "print(\"\\nVectorized dw:\")\n",
    "print(dw)\n",
    "print(\"\\nAre they equal?\", np.allclose(dw_non_vec, dw))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "964c6506",
   "metadata": {},
   "source": [
    "**Implement code primitive: Replacing a for-loop with a vectorized division `dw /= m`.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7c5514c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Example: averaging gradient over training examples\n",
    "m = 5  # number of training examples\n",
    "n = 3  # number of features\n",
    "dw = np.array([10, 20, 30])\n",
    "\n",
    "# Non-vectorized: loop over features\n",
    "dw_non_vec = dw.copy()\n",
    "for i in range(n):\n",
    "    dw_non_vec[i] = dw_non_vec[i] / m\n",
    "\n",
    "# Vectorized: single division operation\n",
    "dw_vec = dw / m\n",
    "\n",
    "print(\"Non-vectorized dw / m:\")\n",
    "print(dw_non_vec)\n",
    "print(\"\\nVectorized dw / m:\")\n",
    "print(dw_vec)\n",
    "print(\"\\nAre they equal?\", np.allclose(dw_non_vec, dw_vec))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d503dde",
   "metadata": {},
   "source": [
    "## Lesson 13"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf5dc312",
   "metadata": {},
   "source": [
    "**Explain the core concepts of the lesson**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b952965",
   "metadata": {},
   "source": [
    "## Core Concepts\n",
    "\n",
    "Vectorized forward propagation is the process of computing predictions for an entire training set simultaneously using matrix operations, rather than computing predictions one example at a time in a loop.\n",
    "\n",
    "**Key Concepts:**\n",
    "\n",
    "- **Vectorization**: Replacing explicit for loops with matrix operations to process multiple training examples at once\n",
    "- **Forward Propagation**: Computing predictions by passing inputs through the model (computing Z and then A)\n",
    "- **Matrix Stacking**: Arranging all training examples as columns in a matrix to enable batch processing\n",
    "- **Training Set Processing**: Computing predictions for all m training examples in a single operation\n",
    "- **Broadcasting**: Automatically expanding scalar values to match matrix dimensions during computation\n",
    "- **Sigmoid Activation**: The activation function that transforms linear predictions into probabilities\n",
    "- **Computational Efficiency**: Vectorized operations run significantly faster on modern hardware compared to loops\n",
    "- **Batch Computation**: Processing the entire batch of training examples simultaneously"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73aefee4",
   "metadata": {},
   "source": [
    "**Explain intuitions and mental models for the lesson**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c0740f1",
   "metadata": {},
   "source": [
    "## Intuitions and Mental Models\n",
    "\n",
    "**From Loops to Matrices:**\n",
    "Instead of computing predictions one training example at a time in a loop, you can stack all training examples into matrices and compute all predictions simultaneously with matrix operations. This transforms a sequential process into a single parallel computation.\n",
    "\n",
    "**Why Vectorization Matters:**\n",
    "Vectorization replaces explicit for loops with matrix multiplication, making code run much faster on modern hardware. Modern processors and GPUs are optimized for matrix operations, so vectorized code can be orders of magnitude faster than loop-based code.\n",
    "\n",
    "**Broadcasting Simplifies Code:**\n",
    "Broadcasting automatically expands scalar values to match matrix dimensions, eliminating the need to manually replicate values. When you add a scalar bias to a matrix, the scalar is automatically broadcast to every element, making the code cleaner and more efficient.\n",
    "\n",
    "**Horizontal Stacking:**\n",
    "Stacking training examples horizontally into a matrix allows you to process the entire batch in a single operation. Each column represents one training example, and matrix multiplication naturally processes all columns together."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e790cc5",
   "metadata": {},
   "source": [
    "**Present and explain the key equations used in the lesson**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5a95e5c",
   "metadata": {},
   "source": [
    "## Key Equations\n",
    "\n",
    "The vectorized forward propagation for logistic regression uses the following equations:\n",
    "\n",
    "**Linear Transformation:**\n",
    "$$Z = W^T X + B$$\n",
    "\n",
    "where:\n",
    "- $W \\in \\mathbb{R}^{n_x \\times 1}$ is the weight vector\n",
    "- $X \\in \\mathbb{R}^{n_x \\times m}$ is the input matrix (m training examples stacked as columns)\n",
    "- $B \\in \\mathbb{R}^{1 \\times m}$ is the bias vector (or scalar broadcasted to all examples)\n",
    "- $Z \\in \\mathbb{R}^{1 \\times m}$ is the output matrix (one prediction per training example)\n",
    "\n",
    "**Activation Function:**\n",
    "$$A = \\sigma(Z)$$\n",
    "\n",
    "where:\n",
    "- $\\sigma$ is the sigmoid activation function applied element-wise\n",
    "- $A \\in \\mathbb{R}^{1 \\times m}$ is the output matrix of predictions (probabilities between 0 and 1)\n",
    "\n",
    "**Dimensions:**\n",
    "- Input matrix: $X \\in \\mathbb{R}^{n_x \\times m}$\n",
    "- Linear output: $Z \\in \\mathbb{R}^{1 \\times m}$\n",
    "- Activation output: $A \\in \\mathbb{R}^{1 \\times m}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c65c3a69",
   "metadata": {},
   "source": [
    "**Implement code primitive: Implement vectorized computation of Z for all training examples using matrix multiplication: W transpose times X plus bias vector B**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4a396b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Example dimensions\n",
    "n_x = 3  # number of features\n",
    "m = 5    # number of training examples\n",
    "\n",
    "# Initialize parameters\n",
    "W = np.array([[0.5], [0.3], [0.2]])  # shape: (n_x, 1)\n",
    "B = 0.1  # scalar bias\n",
    "\n",
    "# Create sample input matrix (each column is one training example)\n",
    "X = np.random.randn(n_x, m)  # shape: (n_x, m)\n",
    "\n",
    "# Vectorized computation of Z\n",
    "Z = np.dot(W.T, X) + B\n",
    "\n",
    "print(f\"W shape: {W.shape}\")\n",
    "print(f\"X shape: {X.shape}\")\n",
    "print(f\"Z shape: {Z.shape}\")\n",
    "print(f\"Z:\\n{Z}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ee91541",
   "metadata": {},
   "source": [
    "**Implement code primitive: Implement vectorized sigmoid activation function that accepts matrix Z and outputs matrix A of same dimensions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af3ece23",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def sigmoid(Z):\n",
    "    \"\"\"\n",
    "    Vectorized sigmoid activation function.\n",
    "    \n",
    "    Args:\n",
    "        Z: numpy array of any shape\n",
    "    \n",
    "    Returns:\n",
    "        A: sigmoid(Z) with same shape as Z\n",
    "    \"\"\"\n",
    "    return 1 / (1 + np.exp(-Z))\n",
    "\n",
    "# Example usage\n",
    "Z = np.array([[-2.0, -1.0, 0.0, 1.0, 2.0]])  # shape: (1, 5)\n",
    "A = sigmoid(Z)\n",
    "\n",
    "print(f\"Z shape: {Z.shape}\")\n",
    "print(f\"A shape: {A.shape}\")\n",
    "print(f\"Z: {Z}\")\n",
    "print(f\"A: {A}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c29f1ad",
   "metadata": {},
   "source": [
    "**Implement code primitive: Demonstrate broadcasting behavior when adding scalar bias B to row vector during vectorized computation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f9cd6c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Create a row vector (1, m)\n",
    "Z_linear = np.array([[1.0, 2.0, 3.0, 4.0, 5.0]])  # shape: (1, 5)\n",
    "print(f\"Z_linear shape: {Z_linear.shape}\")\n",
    "print(f\"Z_linear:\\n{Z_linear}\")\n",
    "\n",
    "# Scalar bias\n",
    "B = 0.5\n",
    "print(f\"\\nB (scalar): {B}\")\n",
    "\n",
    "# Broadcasting: scalar is automatically expanded to match Z_linear dimensions\n",
    "Z_with_bias = Z_linear + B\n",
    "print(f\"\\nZ_with_bias shape: {Z_with_bias.shape}\")\n",
    "print(f\"Z_with_bias:\\n{Z_with_bias}\")\n",
    "\n",
    "# Verify broadcasting worked correctly\n",
    "print(f\"\\nBroadcasting expanded scalar {B} to all {Z_linear.shape[1]} columns\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05d93aec",
   "metadata": {},
   "source": [
    "**Create a Mermaid diagram: Flowchart showing the progression from computing single example predictions (Z1, A1) to vectorized batch computation (Z, A) using matrix operations**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a81f64a",
   "metadata": {},
   "source": [
    "## Progression from Single Example to Vectorized Batch Computation\n",
    "\n",
    "```mermaid\n",
    "graph TD\n",
    "    A[\"Single Training Example\"] --> B[\"Compute Z1 = w^T x1 + b\"]\n",
    "    B --> C[\"Compute A1 = sigmoid(Z1)\"]\n",
    "    C --> D[\"Loop over all m examples\"]\n",
    "    \n",
    "    E[\"Vectorized Approach\"] --> F[\"Stack all examples: X = [x1, x2, ..., xm]\"]\n",
    "    F --> G[\"Compute Z = W^T X + B\"]\n",
    "    G --> H[\"Compute A = sigmoid(Z)\"]\n",
    "    H --> I[\"All m predictions at once\"]\n",
    "    \n",
    "    D --> J[\"Result: Z1, A1, Z2, A2, ..., Zm, Am\"]\n",
    "    I --> K[\"Result: Z, A with shape 1 × m\"]\n",
    "    \n",
    "    J --> L[\"Slower: Sequential computation\"]\n",
    "    K --> M[\"Faster: Parallel matrix operations\"]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a081a5b",
   "metadata": {},
   "source": [
    "## Lesson 14"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88c828ab",
   "metadata": {},
   "source": [
    "**Explain the core concepts of the lesson**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b9f8437",
   "metadata": {},
   "source": [
    "## Core Concepts\n",
    "\n",
    "Vectorized gradient computations form the foundation of efficient machine learning implementations. Instead of computing gradients for each training example individually using explicit loops, we can leverage matrix operations to compute gradients for all training examples simultaneously.\n",
    "\n",
    "The key concepts in this lesson are:\n",
    "\n",
    "1. **Vectorized Gradient Computation**: Computing gradients for all training examples at once using matrix operations rather than element-by-element loops.\n",
    "\n",
    "2. **dZ Matrix Formation**: The difference between predictions and actual labels, stacked into a matrix: $dZ = A - Y$, where $A$ is the matrix of predictions and $Y$ is the matrix of true labels.\n",
    "\n",
    "3. **Vectorized db Calculation**: The gradient with respect to bias is computed by summing all elements of $dZ$ and dividing by the number of training examples: $db = \\frac{1}{m} \\sum dZ$.\n",
    "\n",
    "4. **Vectorized dW Calculation**: The gradient with respect to weights is computed using matrix multiplication: $dW = \\frac{1}{m} X \\cdot dZ^T$, where $X$ is the feature matrix and $dZ^T$ is the transpose of the gradient matrix.\n",
    "\n",
    "5. **Forward and Backward Propagation**: A complete iteration of logistic regression includes vectorized forward propagation ($Z = W^T X + B$, $A = \\text{sigmoid}(Z)$) and vectorized backpropagation (computing $dZ$, $dW$, and $db$).\n",
    "\n",
    "6. **Parameter Updates**: Weights and bias are updated using the computed gradients and a learning rate: $W := W - \\text{learning\\_rate} \\cdot dW$ and $B := B - \\text{learning\\_rate} \\cdot db$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "701f56a1",
   "metadata": {},
   "source": [
    "**Explain intuitions and mental models for the lesson**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec9ec291",
   "metadata": {},
   "source": [
    "## Intuitions and Mental Models\n",
    "\n",
    "**Vectorization as Parallelization**: Vectorization allows us to compute gradients for all training examples simultaneously, replacing explicit for-loops with efficient matrix operations. Instead of iterating through each training example one by one, we stack all examples into matrices and perform operations on the entire matrices at once. This is not only more concise but also significantly faster due to optimized linear algebra libraries.\n",
    "\n",
    "**Stacking Gradients into Matrices**: When we compute the gradient $dz_i$ for each individual training example $i$, we can stack all these scalar values into a single matrix $dZ$. This enables us to compute all $dz$ values through a single matrix subtraction ($dZ = A - Y$) rather than computing each one separately.\n",
    "\n",
    "**Matrix Multiplication for Aggregation**: The vectorized calculation of $dW$ uses matrix multiplication to efficiently sum up the contributions of each training example. Each term $x_i \\cdot dz_i$ (the product of features and gradient for example $i$) is automatically summed across all examples through the operation $X \\cdot dZ^T$. This is far more efficient than explicitly looping through examples.\n",
    "\n",
    "**Full Iteration Vectorization**: A complete iteration of logistic regression—encompassing forward propagation, backpropagation, and parameter updates—can be implemented without any explicit for-loops over training samples. All operations work on entire matrices, making the code both cleaner and faster.\n",
    "\n",
    "**Multiple Iterations Still Require a Loop**: While a single iteration can be fully vectorized, an outermost for-loop is still necessary to perform multiple iterations of gradient descent. This loop controls the number of training epochs and allows the algorithm to converge by repeatedly applying the vectorized update steps."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20acb2a4",
   "metadata": {},
   "source": [
    "**Present and explain the key equations used in the lesson**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "757051d5",
   "metadata": {},
   "source": [
    "## Key Equations\n",
    "\n",
    "The following equations form the mathematical foundation of vectorized gradient computations for logistic regression:\n",
    "\n",
    "**Gradient with respect to predictions:**\n",
    "$$dZ = A - Y$$\n",
    "\n",
    "where $A$ is the matrix of predictions (shape: $1 \\times m$) and $Y$ is the matrix of true labels (shape: $1 \\times m$).\n",
    "\n",
    "**Gradient with respect to bias:**\n",
    "$$db = \\frac{1}{m} \\sum dZ$$\n",
    "\n",
    "where $m$ is the number of training examples. This sums all elements of $dZ$ and averages them.\n",
    "\n",
    "**Gradient with respect to weights:**\n",
    "$$dW = \\frac{1}{m} X \\cdot dZ^T$$\n",
    "\n",
    "where $X$ is the feature matrix (shape: $n_x \\times m$) and $dZ^T$ is the transpose of $dZ$ (shape: $m \\times 1$). The result is a weight gradient matrix of shape $n_x \\times 1$.\n",
    "\n",
    "**Forward propagation:**\n",
    "$$Z = W^T X + B$$\n",
    "$$A = \\text{sigmoid}(Z)$$\n",
    "\n",
    "where $W$ is the weight vector, $X$ is the feature matrix, $B$ is the bias, and $\\text{sigmoid}$ is the logistic function.\n",
    "\n",
    "**Parameter updates:**\n",
    "$$W := W - \\text{learning\\_rate} \\cdot dW$$\n",
    "$$B := B - \\text{learning\\_rate} \\cdot db$$\n",
    "\n",
    "These updates move the parameters in the direction opposite to the gradient, scaled by the learning rate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09798fb1",
   "metadata": {},
   "source": [
    "**Implement code primitive: Compute `dZ` using element-wise subtraction of `A` and `Y` matrices.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c04c0175",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Example: Compute dZ = A - Y\n",
    "A = np.array([[0.9, 0.2, 0.8, 0.7]])  # Predictions (1, m)\n",
    "Y = np.array([[1, 0, 1, 0]])            # True labels (1, m)\n",
    "\n",
    "dZ = A - Y\n",
    "print(\"dZ:\", dZ)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5b68ef4",
   "metadata": {},
   "source": [
    "**Implement code primitive: Compute `db` by summing all elements of `dZ` and dividing by the number of training examples `m` (e.g., `np.sum(dZ)`).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77bad0a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Example: Compute db = (1/m) * sum(dZ)\n",
    "dZ = np.array([[0.9, 0.2, 0.8, 0.7]])  # Shape: (1, m)\n",
    "m = dZ.shape[1]  # Number of training examples\n",
    "\n",
    "db = np.sum(dZ) / m\n",
    "print(\"db:\", db)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cb5c007",
   "metadata": {},
   "source": [
    "**Implement code primitive: Compute `dW` using matrix multiplication of the input feature matrix `X` and the transpose of `dZ`, then dividing by `m` (e.g., `np.dot(X, dZ.T)`).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6d45abc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Example: Compute dW = (1/m) * X * dZ^T\n",
    "X = np.array([[1, 2, 3, 4],\n",
    "              [5, 6, 7, 8]])  # Shape: (n_x, m)\n",
    "dZ = np.array([[0.9, 0.2, 0.8, 0.7]])  # Shape: (1, m)\n",
    "m = X.shape[1]  # Number of training examples\n",
    "\n",
    "dW = np.dot(X, dZ.T) / m\n",
    "print(\"dW shape:\", dW.shape)\n",
    "print(\"dW:\\n\", dW)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea4e31d3",
   "metadata": {},
   "source": [
    "**Implement code primitive: Implement vectorized forward propagation: `Z = np.dot(w.T, X) + b` and `A = sigmoid(Z)`.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9986028",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "# Example: Vectorized forward propagation\n",
    "W = np.array([[0.5], [0.3]])  # Shape: (n_x, 1)\n",
    "b = 0.1\n",
    "X = np.array([[1, 2, 3, 4],\n",
    "              [5, 6, 7, 8]])  # Shape: (n_x, m)\n",
    "\n",
    "Z = np.dot(W.T, X) + b\n",
    "A = sigmoid(Z)\n",
    "print(\"Z shape:\", Z.shape)\n",
    "print(\"A shape:\", A.shape)\n",
    "print(\"A:\", A)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f6d9c6f",
   "metadata": {},
   "source": [
    "**Implement code primitive: Implement vectorized backpropagation steps to compute `dZ`, `dW`, and `db` without explicit loops over training examples.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20e84009",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "# Example: Vectorized backpropagation\n",
    "W = np.array([[0.5], [0.3]])  # Shape: (n_x, 1)\n",
    "b = 0.1\n",
    "X = np.array([[1, 2, 3, 4],\n",
    "              [5, 6, 7, 8]])  # Shape: (n_x, m)\n",
    "Y = np.array([[1, 0, 1, 0]])  # Shape: (1, m)\n",
    "m = X.shape[1]\n",
    "\n",
    "# Forward propagation\n",
    "Z = np.dot(W.T, X) + b\n",
    "A = sigmoid(Z)\n",
    "\n",
    "# Backpropagation\n",
    "dZ = A - Y\n",
    "dW = np.dot(X, dZ.T) / m\n",
    "db = np.sum(dZ) / m\n",
    "\n",
    "print(\"dZ shape:\", dZ.shape)\n",
    "print(\"dW shape:\", dW.shape)\n",
    "print(\"db:\", db)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae0b64d3",
   "metadata": {},
   "source": [
    "**Implement code primitive: Update `W` and `b` parameters using the computed gradients (`dW`, `db`) and a learning rate.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b43cf185",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Example: Parameter update\n",
    "W = np.array([[0.5], [0.3]])  # Shape: (n_x, 1)\n",
    "b = 0.1\n",
    "dW = np.array([[0.02], [0.01]])  # Computed gradient\n",
    "db = 0.005  # Computed gradient\n",
    "learning_rate = 0.01\n",
    "\n",
    "# Update parameters\n",
    "W = W - learning_rate * dW\n",
    "b = b - learning_rate * db\n",
    "\n",
    "print(\"Updated W:\", W.flatten())\n",
    "print(\"Updated b:\", b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c9668d0",
   "metadata": {},
   "source": [
    "**Implement code primitive: Utilize an outer `for` loop to execute multiple iterations of the vectorized gradient descent process.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02d29ddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "# Example: Multiple iterations of vectorized gradient descent\n",
    "W = np.array([[0.5], [0.3]])  # Shape: (n_x, 1)\n",
    "b = 0.1\n",
    "X = np.array([[1, 2, 3, 4],\n",
    "              [5, 6, 7, 8]])  # Shape: (n_x, m)\n",
    "Y = np.array([[1, 0, 1, 0]])  # Shape: (1, m)\n",
    "m = X.shape[1]\n",
    "learning_rate = 0.01\n",
    "num_iterations = 100\n",
    "\n",
    "for iteration in range(num_iterations):\n",
    "    # Forward propagation\n",
    "    Z = np.dot(W.T, X) + b\n",
    "    A = sigmoid(Z)\n",
    "    \n",
    "    # Backpropagation\n",
    "    dZ = A - Y\n",
    "    dW = np.dot(X, dZ.T) / m\n",
    "    db = np.sum(dZ) / m\n",
    "    \n",
    "    # Parameter update\n",
    "    W = W - learning_rate * dW\n",
    "    b = b - learning_rate * db\n",
    "\n",
    "print(\"Final W:\", W.flatten())\n",
    "print(\"Final b:\", b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ef8c44b",
   "metadata": {},
   "source": [
    "**Create a Mermaid diagram: graph TD\n",
    "    A[Start Iteration] --> B{Vectorized Forward Propagation};\n",
    "    B --> C[Compute Z = W^T X + B];\n",
    "    C --> D[Compute A = sigmoid(Z)];\n",
    "    D --> E{Vectorized Backpropagation};\n",
    "    E --> F[Compute dZ = A - Y];\n",
    "    F --> G[Compute dW = (1/m) X dZ^T];\n",
    "    G --> H[Compute db = (1/m) sum(dZ)];\n",
    "    H --> I{Parameter Update};\n",
    "    I --> J[W = W - learning_rate * dW];\n",
    "    J --> K[B = B - learning_rate * db];\n",
    "    K --> L[End Iteration];**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2726f964",
   "metadata": {},
   "source": [
    "```mermaid\n",
    "graph TD\n",
    "    A[Start Iteration] --> B{Vectorized Forward Propagation};\n",
    "    B --> C[Compute Z = W^T X + B];\n",
    "    C --> D[Compute A = sigmoid(Z)];\n",
    "    D --> E{Vectorized Backpropagation};\n",
    "    E --> F[Compute dZ = A - Y];\n",
    "    F --> G[Compute dW = 1/m X dZ^T];\n",
    "    G --> H[Compute db = 1/m sum dZ];\n",
    "    H --> I{Parameter Update};\n",
    "    I --> J[W = W - learning_rate * dW];\n",
    "    J --> K[B = B - learning_rate * db];\n",
    "    K --> L[End Iteration];\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfc4d36b",
   "metadata": {},
   "source": [
    "**Create a Mermaid diagram: graph TD\n",
    "    X_matrix[\"X (n_x, m)\"] --> MatMul;\n",
    "    dZ_T_matrix[\"dZ^T (m, 1)\"] --> MatMul;\n",
    "    MatMul[\"Matrix Multiplication (X @ dZ^T)\"] --> Result;\n",
    "    Result[\"(1/m) * Result of Multiplication\"] --> dW_vector[\"dW (n_x, 1)\"];**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3b6081f",
   "metadata": {},
   "source": [
    "```mermaid\n",
    "graph TD\n",
    "    X_matrix[\"X (n_x, m)\"] --> MatMul;\n",
    "    dZ_T_matrix[\"dZ^T (m, 1)\"] --> MatMul;\n",
    "    MatMul[\"Matrix Multiplication (X @ dZ^T)\"] --> Result;\n",
    "    Result[\"(1/m) * Result of Multiplication\"] --> dW_vector[\"dW (n_x, 1)\"];\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea211435",
   "metadata": {},
   "source": [
    "## Lesson 15"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a3db42d",
   "metadata": {},
   "source": [
    "**Explain the core concepts of the lesson**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd0ef11b",
   "metadata": {},
   "source": [
    "## Core Concepts\n",
    "\n",
    "Python broadcasting is a powerful mechanism in NumPy that allows operations between arrays of different shapes. The key concepts include:\n",
    "\n",
    "- **Broadcasting**: The process of conceptually 'stretching' smaller arrays to match the shape of larger arrays during element-wise operations.\n",
    "- **Matrix operations**: Operations performed on 2D arrays (matrices) where broadcasting enables efficient computation without explicit loops.\n",
    "- **Vector-scalar operations**: Operations between vectors and scalars, where the scalar is conceptually expanded to match the vector's shape.\n",
    "- **Matrix-vector operations**: Operations between matrices and vectors, where the vector is conceptually expanded to match the matrix's shape.\n",
    "- **NumPy sum function**: The `sum()` method with axis parameter to compute sums along specific dimensions.\n",
    "- **Axis for summation**: The `axis` parameter controls whether summation occurs along rows (axis=1) or columns (axis=0).\n",
    "- **NumPy reshape command**: A constant-time operation that changes array dimensions without copying data.\n",
    "- **Element-wise operations**: Operations applied independently to each element of arrays, such as addition, subtraction, multiplication, and division.\n",
    "- **Computational efficiency**: Broadcasting eliminates the need for explicit loops, resulting in faster execution and more concise code."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bc23bb4",
   "metadata": {},
   "source": [
    "**Explain intuitions and mental models for the lesson**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd566e92",
   "metadata": {},
   "source": [
    "## Intuitions and Mental Models\n",
    "\n",
    "**Broadcasting as Conceptual Stretching**: Imagine broadcasting as a way to mentally 'stretch' or 'expand' a smaller array to match the shape of a larger array. When you perform an operation between arrays of different shapes, NumPy doesn't actually copy the data; instead, it conceptually repeats the smaller array to align with the larger one. This mental model helps you understand why operations work without explicitly writing loops.\n",
    "\n",
    "**Efficiency Through Implicit Expansion**: Broadcasting allows you to write more concise and faster Python code by eliminating the need for explicit loops. Instead of manually iterating through rows or columns, you can write a single operation that NumPy applies efficiently across all elements. This leads to improved performance because the underlying operations are optimized at the C level.\n",
    "\n",
    "**Reshape as a Lightweight Operation**: The `reshape` command is an efficient, constant-time operation that doesn't copy data—it simply changes how the array is viewed. When you reshape an array to prepare it for broadcasting, you're not performing expensive data movement; you're just changing the shape metadata. This makes it safe and efficient to reshape arrays as needed for operations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4402474b",
   "metadata": {},
   "source": [
    "**Present and explain the key equations used in the lesson**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a303051",
   "metadata": {},
   "source": [
    "## Key Equations\n",
    "\n",
    "Broadcasting follows specific rules for different shape combinations:\n",
    "\n",
    "**Matrix-Row Vector Broadcasting**:\n",
    "$$A_{m,n} \\text{ op } B_{1,n} \\implies \\text{conceptually, } B \\text{ is copied } m \\text{ times to match } A_{m,n}$$\n",
    "\n",
    "When a matrix of shape $(m, n)$ is combined with a row vector of shape $(1, n)$, the row vector is conceptually repeated $m$ times vertically to match the matrix's shape.\n",
    "\n",
    "**Matrix-Column Vector Broadcasting**:\n",
    "$$A_{m,n} \\text{ op } B_{m,1} \\implies \\text{conceptually, } B \\text{ is copied } n \\text{ times to match } A_{m,n}$$\n",
    "\n",
    "When a matrix of shape $(m, n)$ is combined with a column vector of shape $(m, 1)$, the column vector is conceptually repeated $n$ times horizontally to match the matrix's shape.\n",
    "\n",
    "**Vector-Scalar Broadcasting**:\n",
    "$$A_{m,1} \\text{ op } B_{1,1} \\implies \\text{conceptually, } B \\text{ is copied } m \\text{ times to match } A_{m,1}$$\n",
    "\n",
    "When a column vector of shape $(m, 1)$ is combined with a scalar (shape $(1, 1)$), the scalar is conceptually repeated $m$ times to match the vector's shape."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca8ca621",
   "metadata": {},
   "source": [
    "**Implement code primitive: Initialize a NumPy 2D array (matrix) with specific values.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18bcdb6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Initialize a 2D array (matrix) with specific values\n",
    "A = np.array([[1, 2, 3, 4],\n",
    "              [5, 6, 7, 8],\n",
    "              [9, 10, 11, 12]])\n",
    "\n",
    "print(\"Matrix A:\")\n",
    "print(A)\n",
    "print(f\"Shape: {A.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "196e93bd",
   "metadata": {},
   "source": [
    "**Implement code primitive: Calculate column-wise sums of a matrix using `sum(axis=0)`.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce2ad4e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Initialize a matrix\n",
    "A = np.array([[1, 2, 3, 4],\n",
    "              [5, 6, 7, 8],\n",
    "              [9, 10, 11, 12]])\n",
    "\n",
    "# Calculate column-wise sums using axis=0\n",
    "column_sums = A.sum(axis=0)\n",
    "\n",
    "print(\"Column-wise sums:\")\n",
    "print(column_sums)\n",
    "print(f\"Shape: {column_sums.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cba8f8c",
   "metadata": {},
   "source": [
    "**Implement code primitive: Reshape a 1D array (vector) into a specific row vector dimension (e.g., 1x4).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eafc535a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Create a 1D array\n",
    "v = np.array([15, 18, 21, 24])\n",
    "\n",
    "print(\"Original vector shape:\", v.shape)\n",
    "\n",
    "# Reshape to a row vector (1, 4)\n",
    "v_row = v.reshape(1, -1)\n",
    "\n",
    "print(\"Reshaped to row vector:\")\n",
    "print(v_row)\n",
    "print(f\"Shape: {v_row.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9018174c",
   "metadata": {},
   "source": [
    "**Implement code primitive: Perform element-wise division of a matrix by a reshaped row vector using broadcasting.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26ab26e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Initialize a matrix\n",
    "A = np.array([[1, 2, 3, 4],\n",
    "              [5, 6, 7, 8],\n",
    "              [9, 10, 11, 12]])\n",
    "\n",
    "# Calculate column-wise sums and reshape to row vector\n",
    "column_sums = A.sum(axis=0).reshape(1, -1)\n",
    "\n",
    "print(\"Column sums (row vector):\")\n",
    "print(column_sums)\n",
    "print(f\"Shape: {column_sums.shape}\")\n",
    "\n",
    "# Perform element-wise division using broadcasting\n",
    "result = A / column_sums\n",
    "\n",
    "print(\"\\nResult of A / column_sums:\")\n",
    "print(result)\n",
    "print(f\"Shape: {result.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb1896ec",
   "metadata": {},
   "source": [
    "**Create a Mermaid diagram: graph TD\\nA[Matrix A (m,n)]\\nB[Vector B (1,n)]\\nB -- Copy m times --> B_exp[Expanded B (m,n)]\\nA & B_exp -- Element-wise Operation --> Result[(Result (m,n))]**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6b7a63f",
   "metadata": {},
   "source": [
    "```mermaid\n",
    "graph TD\n",
    "    A[\"Matrix A (m,n)\"]\n",
    "    B[\"Vector B (1,n)\"]\n",
    "    B_exp[\"Expanded B (m,n)\"]\n",
    "    Result[\"Result (m,n)\"]\n",
    "    \n",
    "    B -->|Copy m times| B_exp\n",
    "    A -->|Element-wise Operation| Result\n",
    "    B_exp -->|Element-wise Operation| Result\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88c0130b",
   "metadata": {},
   "source": [
    "**Create a Mermaid diagram: graph TD\\nA[Matrix A (m,n)]\\nB[Vector B (m,1)]\\nB -- Copy n times --> B_exp[Expanded B (m,n)]\\nA & B_exp -- Element-wise Operation --> Result[(Result (m,n))]**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76debd87",
   "metadata": {},
   "source": [
    "```mermaid\n",
    "graph TD\n",
    "    A[\"Matrix A (m,n)\"]\n",
    "    B[\"Vector B (m,1)\"]\n",
    "    B_exp[\"Expanded B (m,n)\"]\n",
    "    Result[\"Result (m,n)\"]\n",
    "    \n",
    "    B -->|Copy n times| B_exp\n",
    "    A -->|Element-wise Operation| Result\n",
    "    B_exp -->|Element-wise Operation| Result\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79d8d081",
   "metadata": {},
   "source": [
    "**Create a Mermaid diagram: graph TD\\nA[Vector A (m,1)]\\nB[Scalar B (1,1)]\\nB -- Copy m times --> B_exp[Expanded B (m,1)]\\nA & B_exp -- Element-wise Operation --> Result[(Result (m,1))]**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "807b9f2e",
   "metadata": {},
   "source": [
    "```mermaid\n",
    "graph TD\n",
    "    A[\"Vector A (m,1)\"]\n",
    "    B[\"Scalar B (1,1)\"]\n",
    "    B_exp[\"Expanded B (m,1)\"]\n",
    "    Result[\"Result (m,1)\"]\n",
    "    \n",
    "    B -->|Copy m times| B_exp\n",
    "    A -->|Element-wise Operation| Result\n",
    "    B_exp -->|Element-wise Operation| Result\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7adeddc8",
   "metadata": {},
   "source": [
    "## Lesson 16"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "714fb905",
   "metadata": {},
   "source": [
    "**Explain the core concepts of the lesson**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8cc610b",
   "metadata": {},
   "source": [
    "## Core Concepts: NumPy Vector Dimensions and Rank 1 Array Bugs\n",
    "\n",
    "NumPy's flexibility in handling arrays can introduce subtle, hard-to-find bugs if array dimensions are not fully understood. The key distinction lies in how NumPy treats different vector representations:\n",
    "\n",
    "**Rank 1 Arrays**: Created with `np.random.randn(N)`, these have shape `(N,)`. They behave inconsistently—transposition appears to have no effect, and operations like matrix multiplication can produce unexpected results.\n",
    "\n",
    "**Column Vectors**: Created with `np.random.randn(N, 1)`, these have shape `(N, 1)`. They are explicitly 2D and behave predictably in linear algebra operations.\n",
    "\n",
    "**Row Vectors**: Created with `np.random.randn(1, N)`, these have shape `(1, N)`. They are also explicitly 2D and provide consistent behavior.\n",
    "\n",
    "The core issue is that rank 1 arrays lack a clear orientation (neither row nor column), leading to broadcasting ambiguities. Explicitly using 2D vectors eliminates this ambiguity and makes code more maintainable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1b284d3",
   "metadata": {},
   "source": [
    "**Explain intuitions and mental models for the lesson**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e990d8d9",
   "metadata": {},
   "source": [
    "## Intuitions and Mental Models\n",
    "\n",
    "**Broadcasting Flexibility as a Double-Edged Sword**: NumPy's broadcasting rules are powerful but can mask errors. A rank 1 array can be broadcast in multiple ways, making it unclear what operation is actually being performed.\n",
    "\n",
    "**Transposition Confusion**: When you transpose a rank 1 array, it looks identical visually. This violates the mathematical expectation that transposing a vector should change its orientation. Explicit column and row vectors make this behavior transparent.\n",
    "\n",
    "**Inner vs. Outer Products**: With a rank 1 array, `a @ a.T` produces a scalar (inner product). With an explicit column vector, the same operation produces a matrix (outer product). This dramatic difference in behavior is a common source of bugs.\n",
    "\n",
    "**Assertions as Self-Documentation**: Using assertions like `assert(array.shape == (N, 1))` serves dual purposes: it catches dimension mismatches early and documents your intent to future readers.\n",
    "\n",
    "**Reshaping as a Solution**: Converting a rank 1 array to an explicit column or row vector using `reshape()` resolves ambiguity and ensures predictable behavior in downstream operations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f833355c",
   "metadata": {},
   "source": [
    "**Implement code primitive: Creating a rank 1 array using `np.random.randn(N)`.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da0dd087",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "N = 5\n",
    "a = np.random.randn(N)\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6874363f",
   "metadata": {},
   "source": [
    "**Implement code primitive: Inspecting an array's shape with `array.shape`.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db8bea12",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Shape of a: {a.shape}\")\n",
    "print(f\"Number of dimensions: {a.ndim}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4541b484",
   "metadata": {},
   "source": [
    "**Implement code primitive: Demonstrating transposition (`array.T`) of a rank 1 array and its non-intuitive visual effect.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c106cb48",
   "metadata": {},
   "outputs": [],
   "source": [
    "a_transposed = a.T\n",
    "print(f\"Original array: {a}\")\n",
    "print(f\"Transposed array: {a_transposed}\")\n",
    "print(f\"Shape of a.T: {a_transposed.shape}\")\n",
    "print(f\"Are they identical? {np.array_equal(a, a_transposed)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f24e1a80",
   "metadata": {},
   "source": [
    "**Implement code primitive: Performing matrix multiplication (`array @ array.T`) with a rank 1 array resulting in a scalar.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37ab1e8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = a @ a.T\n",
    "print(f\"a @ a.T = {result}\")\n",
    "print(f\"Type of result: {type(result)}\")\n",
    "print(f\"Is it a scalar? {np.isscalar(result)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9be10973",
   "metadata": {},
   "source": [
    "**Implement code primitive: Creating an explicit column vector using `np.random.randn(N, 1)`.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d65c4ed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "b = np.random.randn(N, 1)\n",
    "print(f\"Column vector:\\n{b}\")\n",
    "print(f\"Shape: {b.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb659744",
   "metadata": {},
   "source": [
    "**Implement code primitive: Creating an explicit row vector using `np.random.randn(1, N)`.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dda48618",
   "metadata": {},
   "outputs": [],
   "source": [
    "c = np.random.randn(1, N)\n",
    "print(f\"Row vector:\\n{c}\")\n",
    "print(f\"Shape: {c.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79dcdee4",
   "metadata": {},
   "source": [
    "**Implement code primitive: Demonstrating transposition (`array.T`) of a column vector to show it becomes a row vector.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beef589e",
   "metadata": {},
   "outputs": [],
   "source": [
    "b_transposed = b.T\n",
    "print(f\"Original column vector shape: {b.shape}\")\n",
    "print(f\"Transposed column vector shape: {b_transposed.shape}\")\n",
    "print(f\"Transposed is now a row vector: {b_transposed.shape == (1, N)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29771c4b",
   "metadata": {},
   "source": [
    "**Implement code primitive: Performing matrix multiplication (`array @ array.T`) with an explicit column vector resulting in a matrix (outer product).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "981a2699",
   "metadata": {},
   "outputs": [],
   "source": [
    "outer_product = b @ b.T\n",
    "print(f\"b @ b.T shape: {outer_product.shape}\")\n",
    "print(f\"b @ b.T (outer product):\\n{outer_product}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f6af6c3",
   "metadata": {},
   "source": [
    "**Implement code primitive: Adding an assertion statement to check for specific array dimensions, e.g., `assert(array.shape == (N, 1))`.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24bb1220",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert b.shape == (N, 1), f\"Expected shape (N, 1), got {b.shape}\"\n",
    "print(\"Assertion passed: b is a column vector\")\n",
    "\n",
    "assert c.shape == (1, N), f\"Expected shape (1, N), got {c.shape}\"\n",
    "print(\"Assertion passed: c is a row vector\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fd827b2",
   "metadata": {},
   "source": [
    "**Implement code primitive: Reshaping an array using `array.reshape((N, 1))` to explicitly define its dimensions.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac8f2bce",
   "metadata": {},
   "outputs": [],
   "source": [
    "a_reshaped = a.reshape((N, 1))\n",
    "print(f\"Original rank 1 array shape: {a.shape}\")\n",
    "print(f\"Reshaped to column vector: {a_reshaped.shape}\")\n",
    "print(f\"Reshaped array:\\n{a_reshaped}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0fa86ea",
   "metadata": {},
   "source": [
    "**Create a Mermaid diagram: graph TD\n",
    "    subgraph NumPy Vector Types and Behaviors\n",
    "        A[Rank 1 Array (e.g., np.random.randn(N))] --> A_SHAPE(Shape: (N,));\n",
    "        A_SHAPE --> A_TRANSPOSE{a.T looks identical to a};\n",
    "        A_TRANSPOSE --> A_PRODUCT{a @ a.T gives a scalar};\n",
    "\n",
    "        B[Column Vector (e.g., np.random.randn(N, 1))] --> B_SHAPE(Shape: (N,1));\n",
    "        B_SHAPE --> B_TRANSPOSE{a.T is a (1,N) row vector};\n",
    "        B_TRANSPOSE --> B_PRODUCT{a @ a.T gives a (N,N) matrix};\n",
    "\n",
    "        C[Row Vector (e.g., np.random.randn(1, N))] --> C_SHAPE(Shape: (1,N));\n",
    "        C_SHAPE --> C_TRANSPOSE{a.T is a (N,1) column vector};\n",
    "        C_TRANSPOSE --> C_PRODUCT{a @ a.T gives a (1,1) scalar};\n",
    "    end\n",
    "\n",
    "    A_PRODUCT -- Leads to bugs/confusion --> D(Recommendation: Avoid Rank 1 Arrays);\n",
    "    B_PRODUCT -- Promotes clarity --> E(Recommendation: Use (N,1) or (1,N) for vectors);\n",
    "    C_PRODUCT -- Promotes clarity --> E;**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "591d1ad4",
   "metadata": {},
   "source": [
    "```mermaid\n",
    "graph TD\n",
    "    subgraph NumPy Vector Types and Behaviors\n",
    "        A[Rank 1 Array - np.random.randn(N)] --> A_SHAPE(Shape: (N,))\n",
    "        A_SHAPE --> A_TRANSPOSE{a.T looks identical to a}\n",
    "        A_TRANSPOSE --> A_PRODUCT{a @ a.T gives a scalar}\n",
    "\n",
    "        B[Column Vector - np.random.randn(N, 1)] --> B_SHAPE(Shape: (N,1))\n",
    "        B_SHAPE --> B_TRANSPOSE{a.T is a (1,N) row vector}\n",
    "        B_TRANSPOSE --> B_PRODUCT{a @ a.T gives a (N,N) matrix}\n",
    "\n",
    "        C[Row Vector - np.random.randn(1, N)] --> C_SHAPE(Shape: (1,N))\n",
    "        C_SHAPE --> C_TRANSPOSE{a.T is a (N,1) column vector}\n",
    "        C_TRANSPOSE --> C_PRODUCT{a @ a.T gives a (1,1) scalar}\n",
    "    end\n",
    "\n",
    "    A_PRODUCT -- Leads to bugs/confusion --> D(Recommendation: Avoid Rank 1 Arrays)\n",
    "    B_PRODUCT -- Promotes clarity --> E(Recommendation: Use (N,1) or (1,N) for vectors)\n",
    "    C_PRODUCT -- Promotes clarity --> E\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cc1a2d3",
   "metadata": {},
   "source": [
    "## Lesson 17"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5f40d31",
   "metadata": {},
   "source": [
    "**Explain the core concepts of the lesson**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d8efff1",
   "metadata": {},
   "source": [
    "## Core Concepts\n",
    "\n",
    "Jupyter iPython Notebooks are interactive computing environments that combine text instructions and executable code in a single document. Each notebook consists of cells that can be either:\n",
    "\n",
    "- **Text blocks (Markdown cells)**: Contain formatted text, instructions, and explanations\n",
    "- **Code blocks (Code cells)**: Contain executable Python code\n",
    "\n",
    "When you execute a code cell, the code is sent to a **kernel** running on a server. The kernel processes your code and returns the output, which is displayed directly below the cell. This interactive workflow allows you to write, test, and refine code while learning.\n",
    "\n",
    "The **kernel** is a backend process that executes your code. If something goes wrong, you can restart the kernel to clear all variables and start fresh. Importantly, the order in which you execute cells matters—earlier cells may set up variables or import libraries that later cells depend on."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a84e5d9",
   "metadata": {},
   "source": [
    "**Explain intuitions and mental models for the lesson**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa710111",
   "metadata": {},
   "source": [
    "## Intuitions and Mental Models\n",
    "\n",
    "**Jupyter Notebooks as Interactive Learning Tools**: Think of a Jupyter notebook as a conversation between you and a computer. You write instructions and code in cells, execute them, see the results immediately, and then refine your approach. This rapid feedback loop makes learning and experimentation much faster than traditional programming workflows.\n",
    "\n",
    "**The Kernel as a Backend Worker**: Imagine the kernel as a worker on a remote server. You send it tasks (code cells), it executes them, and reports back with results. The kernel maintains state—variables you create persist until you restart it. This is why executing cells in the correct order is crucial; if an earlier cell defines a variable that a later cell uses, you must run the earlier cell first.\n",
    "\n",
    "**Cell Execution as a Sequential Process**: Each time you execute a cell, your code travels to the kernel, gets processed, and the output returns to your notebook. If the kernel crashes or becomes unresponsive, you can restart it, but this clears all stored variables and state. Understanding this flow helps you debug issues and organize your work effectively."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7c7dedb",
   "metadata": {},
   "source": [
    "**Implement code primitive: Writing code between 'START CODE HERE' and 'END CODE HERE' markers.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "696bdd0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# START CODE HERE\n",
    "x = 5\n",
    "y = 10\n",
    "z = x + y\n",
    "# END CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e2a1faf",
   "metadata": {},
   "source": [
    "**Implement code primitive: Executing a code block using Shift+Enter or 'Cell > Run Cell'.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f809f32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute this cell by pressing Shift+Enter or using Cell > Run Cell\n",
    "result = 42\n",
    "print(f\"The result is: {result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d52dbef2",
   "metadata": {},
   "source": [
    "**Implement code primitive: Printing a 'Hello world' message.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dd1fb66",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Hello world\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28678ec1",
   "metadata": {},
   "source": [
    "**Implement code primitive: Importing the NumPy library as 'np'.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dcabf8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67af97e3",
   "metadata": {},
   "source": [
    "**Create a Mermaid diagram: graph TD;A[Instructions in Text Block] --> B[User writes code in Code Block];B --> C{Execute Code Block (Shift+Enter/Run Cell)};C --> D[Code sent to Kernel on Server];D --> E[Kernel executes code];E --> F[Output displayed in Code Block];F --> B;D -- \"Kernel dies\" --> G[Restart Kernel];**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69facc56",
   "metadata": {},
   "source": [
    "## Notebook Execution Flow\n",
    "\n",
    "```mermaid\n",
    "graph TD\n",
    "    A[Instructions in Text Block] --> B[User writes code in Code Block]\n",
    "    B --> C{Execute Code Block<br/>Shift+Enter/Run Cell}\n",
    "    C --> D[Code sent to Kernel on Server]\n",
    "    D --> E[Kernel executes code]\n",
    "    E --> F[Output displayed in Code Block]\n",
    "    F --> B\n",
    "    D -- \"Kernel dies\" --> G[Restart Kernel]\n",
    "```\n",
    "\n",
    "This diagram illustrates the interactive cycle of working in a Jupyter notebook. You read instructions, write code, execute it, see results, and iterate. If the kernel crashes, you can restart it and continue your work."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a08b7f80",
   "metadata": {},
   "source": [
    "## Lesson 18"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90af89fc",
   "metadata": {},
   "source": [
    "**Explain the core concepts of the lesson**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b92561f",
   "metadata": {},
   "source": [
    "## Core Concepts of Logistic Regression Cost Function\n",
    "\n",
    "Logistic regression is a fundamental algorithm for binary classification. The cost function used in logistic regression is derived from principles of probability and maximum likelihood estimation.\n",
    "\n",
    "**Key Concepts:**\n",
    "\n",
    "1. **Logistic Regression Prediction**: The model outputs a probability using the sigmoid function:\n",
    "   $$\\hat{y} = \\sigma(w^T x + b)$$\n",
    "   where $\\sigma$ is the sigmoid function that maps any input to a value between 0 and 1.\n",
    "\n",
    "2. **Conditional Probability**: The prediction $\\hat{y}$ represents the probability that the label is 1 given the input:\n",
    "   - $P(y=1|x) = \\hat{y}$\n",
    "   - $P(y=0|x) = 1 - \\hat{y}$\n",
    "\n",
    "3. **Combined Probability Equation**: A single expression elegantly captures both outcomes:\n",
    "   $$P(y|x) = \\hat{y}^y (1 - \\hat{y})^{(1-y)}$$\n",
    "   When $y=1$, this equals $\\hat{y}$; when $y=0$, this equals $1-\\hat{y}$.\n",
    "\n",
    "4. **Loss Function**: For a single training example, the loss measures how well the model's prediction matches the actual label:\n",
    "   $$L(\\hat{y}, y) = -[y \\log \\hat{y} + (1-y) \\log (1-\\hat{y})]$$\n",
    "\n",
    "5. **Overall Cost Function**: The average loss across all training examples:\n",
    "   $$J(w,b) = - \\frac{1}{m} \\sum_{i=1}^m [y^{(i)} \\log \\hat{y}^{(i)} + (1-y^{(i)}) \\log (1-\\hat{y}^{(i)})]$$\n",
    "\n",
    "6. **Maximum Likelihood Estimation**: The cost function is derived to maximize the probability of observing the actual training labels given the model parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80d38683",
   "metadata": {},
   "source": [
    "**Explain intuitions and mental models for the lesson**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca558ced",
   "metadata": {},
   "source": [
    "## Intuitions and Mental Models\n",
    "\n",
    "**Why This Cost Function?**\n",
    "\n",
    "The logistic regression cost function is not arbitrary—it emerges naturally from a fundamental principle: **maximize the probability of correctly observing the actual labels in the training data**.\n",
    "\n",
    "**Key Intuitions:**\n",
    "\n",
    "1. **Unified Expression for Both Outcomes**: Rather than writing separate equations for $y=0$ and $y=1$, the expression $\\hat{y}^y (1 - \\hat{y})^{(1-y)}$ elegantly combines both cases into one. This mathematical elegance simplifies both understanding and computation.\n",
    "\n",
    "2. **Logarithm Simplification**: Taking the logarithm of probabilities converts products into sums:\n",
    "   $$\\log P(\\text{labels}) = \\sum_{i=1}^m \\log P(y^{(i)}|x^{(i)})$$\n",
    "   This transformation makes optimization much easier and more numerically stable.\n",
    "\n",
    "3. **Likelihood and Loss Are Inverses**: Minimizing the loss function is mathematically equivalent to maximizing the likelihood of the training data. When we minimize the cost function, we're finding the parameters that make the observed data most probable.\n",
    "\n",
    "4. **Averaging Over Examples**: Dividing by $m$ (the number of training examples) scales the cost function appropriately. This makes the cost comparable across datasets of different sizes and prevents the cost from growing simply because we have more data.\n",
    "\n",
    "5. **Independent and Identically Distributed Assumption**: The derivation assumes each training example is independent and drawn from the same distribution. This allows us to multiply individual probabilities to get the joint probability of all labels."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "571b1612",
   "metadata": {},
   "source": [
    "**Present and explain the key equations used in the lesson**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90465ea1",
   "metadata": {},
   "source": [
    "## Key Equations\n",
    "\n",
    "**Prediction:**\n",
    "$$\\hat{y} = \\sigma(w^T x + b)$$\n",
    "\n",
    "**Conditional Probabilities:**\n",
    "$$P(y=1|x) = \\hat{y}$$\n",
    "$$P(y=0|x) = 1 - \\hat{y}$$\n",
    "\n",
    "**Combined Probability for a Single Example:**\n",
    "$$P(y|x) = \\hat{y}^y (1 - \\hat{y})^{(1-y)}$$\n",
    "\n",
    "**Logarithm of Conditional Probability:**\n",
    "$$\\log P(y|x) = y \\log \\hat{y} + (1-y) \\log (1-\\hat{y})$$\n",
    "\n",
    "**Loss Function for a Single Training Example:**\n",
    "$$L(\\hat{y}, y) = -[y \\log \\hat{y} + (1-y) \\log (1-\\hat{y})]$$\n",
    "\n",
    "**Joint Probability of All Training Labels:**\n",
    "$$P(\\text{labels in training set}) = \\prod_{i=1}^m P(y^{(i)}|x^{(i)})$$\n",
    "\n",
    "**Log of Joint Probability:**\n",
    "$$\\log P(\\text{labels in training set}) = \\sum_{i=1}^m \\log P(y^{(i)}|x^{(i)})$$\n",
    "\n",
    "**Overall Cost Function (Average Loss):**\n",
    "$$J(w,b) = - \\frac{1}{m} \\sum_{i=1}^m [y^{(i)} \\log \\hat{y}^{(i)} + (1-y^{(i)}) \\log (1-\\hat{y}^{(i)})]$$\n",
    "\n",
    "The goal is to find the weights $w$ and bias $b$ that minimize $J(w,b)$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2460f1c",
   "metadata": {},
   "source": [
    "**Implement code primitive: Compute the logistic regression prediction (y_hat) for a given input x, weights w, and bias b.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3ad3292",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def sigmoid(z):\n",
    "    \"\"\"Sigmoid activation function.\"\"\"\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "def logistic_regression_prediction(x, w, b):\n",
    "    \"\"\"\n",
    "    Compute logistic regression prediction.\n",
    "    \n",
    "    Args:\n",
    "        x: Input features (1D array or scalar)\n",
    "        w: Weights (1D array, same shape as x)\n",
    "        b: Bias (scalar)\n",
    "    \n",
    "    Returns:\n",
    "        y_hat: Predicted probability (scalar between 0 and 1)\n",
    "    \"\"\"\n",
    "    z = np.dot(w, x) + b\n",
    "    y_hat = sigmoid(z)\n",
    "    return y_hat\n",
    "\n",
    "# Example usage\n",
    "w = np.array([0.5, -0.3])\n",
    "b = 0.1\n",
    "x = np.array([2.0, 1.0])\n",
    "y_hat = logistic_regression_prediction(x, w, b)\n",
    "print(f\"Prediction: {y_hat}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16622884",
   "metadata": {},
   "source": [
    "**Implement code primitive: Calculate the loss function for a single training example (y_hat, y).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14b7c7fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function(y_hat, y):\n",
    "    \"\"\"\n",
    "    Calculate the loss function for a single training example.\n",
    "    \n",
    "    Args:\n",
    "        y_hat: Predicted probability (scalar between 0 and 1)\n",
    "        y: Actual label (0 or 1)\n",
    "    \n",
    "    Returns:\n",
    "        loss: Loss value (scalar)\n",
    "    \"\"\"\n",
    "    epsilon = 1e-15  # Small value to avoid log(0)\n",
    "    y_hat = np.clip(y_hat, epsilon, 1 - epsilon)\n",
    "    loss = -(y * np.log(y_hat) + (1 - y) * np.log(1 - y_hat))\n",
    "    return loss\n",
    "\n",
    "# Example usage\n",
    "y_hat = 0.8\n",
    "y = 1\n",
    "loss = loss_function(y_hat, y)\n",
    "print(f\"Loss: {loss}\")\n",
    "\n",
    "y_hat = 0.3\n",
    "y = 0\n",
    "loss = loss_function(y_hat, y)\n",
    "print(f\"Loss: {loss}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "065903a7",
   "metadata": {},
   "source": [
    "**Implement code primitive: Calculate the overall cost function for the entire training set by averaging individual losses.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bc89bd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost_function(X, y, w, b):\n",
    "    \"\"\"\n",
    "    Calculate the overall cost function for the entire training set.\n",
    "    \n",
    "    Args:\n",
    "        X: Training features (2D array, shape: m x n)\n",
    "        y: Training labels (1D array, shape: m)\n",
    "        w: Weights (1D array, shape: n)\n",
    "        b: Bias (scalar)\n",
    "    \n",
    "    Returns:\n",
    "        J: Cost function value (scalar)\n",
    "    \"\"\"\n",
    "    m = X.shape[0]\n",
    "    total_loss = 0\n",
    "    \n",
    "    for i in range(m):\n",
    "        y_hat = logistic_regression_prediction(X[i], w, b)\n",
    "        total_loss += loss_function(y_hat, y[i])\n",
    "    \n",
    "    J = total_loss / m\n",
    "    return J\n",
    "\n",
    "# Example usage\n",
    "X = np.array([[2.0, 1.0], [1.0, 3.0], [3.0, 2.0]])\n",
    "y = np.array([1, 0, 1])\n",
    "w = np.array([0.5, -0.3])\n",
    "b = 0.1\n",
    "\n",
    "J = cost_function(X, y, w, b)\n",
    "print(f\"Cost function: {J}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c2a44c3",
   "metadata": {},
   "source": [
    "**Implement code primitive: Implement an optimization algorithm to minimize the calculated cost function J(w,b).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1525f68a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(X, y, w, b, learning_rate=0.01, iterations=100):\n",
    "    \"\"\"\n",
    "    Optimize weights and bias using gradient descent to minimize cost function.\n",
    "    \n",
    "    Args:\n",
    "        X: Training features (2D array, shape: m x n)\n",
    "        y: Training labels (1D array, shape: m)\n",
    "        w: Initial weights (1D array, shape: n)\n",
    "        b: Initial bias (scalar)\n",
    "        learning_rate: Learning rate for gradient descent\n",
    "        iterations: Number of iterations\n",
    "    \n",
    "    Returns:\n",
    "        w: Optimized weights\n",
    "        b: Optimized bias\n",
    "        costs: List of cost values at each iteration\n",
    "    \"\"\"\n",
    "    m, n = X.shape\n",
    "    costs = []\n",
    "    \n",
    "    for iteration in range(iterations):\n",
    "        dw = np.zeros(n)\n",
    "        db = 0\n",
    "        \n",
    "        for i in range(m):\n",
    "            y_hat = logistic_regression_prediction(X[i], w, b)\n",
    "            error = y_hat - y[i]\n",
    "            dw += error * X[i]\n",
    "            db += error\n",
    "        \n",
    "        dw /= m\n",
    "        db /= m\n",
    "        \n",
    "        w = w - learning_rate * dw\n",
    "        b = b - learning_rate * db\n",
    "        \n",
    "        J = cost_function(X, y, w, b)\n",
    "        costs.append(J)\n",
    "    \n",
    "    return w, b, costs\n",
    "\n",
    "# Example usage\n",
    "X = np.array([[2.0, 1.0], [1.0, 3.0], [3.0, 2.0], [2.5, 0.5]])\n",
    "y = np.array([1, 0, 1, 1])\n",
    "w = np.array([0.0, 0.0])\n",
    "b = 0.0\n",
    "\n",
    "w_opt, b_opt, costs = gradient_descent(X, y, w, b, learning_rate=0.1, iterations=50)\n",
    "print(f\"Optimized weights: {w_opt}\")\n",
    "print(f\"Optimized bias: {b_opt}\")\n",
    "print(f\"Final cost: {costs[-1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85ec85a8",
   "metadata": {},
   "source": [
    "## Lesson 19"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "390d60fa",
   "metadata": {},
   "source": [
    "**Explain the core concepts of the lesson**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79cd845e",
   "metadata": {},
   "source": [
    "## Core Concepts of Deep Reinforcement Learning\n",
    "\n",
    "Deep reinforcement learning combines two powerful paradigms:\n",
    "\n",
    "**Deep Learning** provides representation learning—the ability to automatically discover patterns and features in raw sensory data (images, audio, text) without hand-engineering features.\n",
    "\n",
    "**Reinforcement Learning** provides decision-making under uncertainty—agents learn to select actions that maximize cumulative rewards over time, learning from their own experience rather than labeled data.\n",
    "\n",
    "When combined, deep RL enables agents to learn directly from raw sensory inputs (like pixels) to action outputs, end-to-end. However, this combination introduces unique challenges that supervised learning does not face:\n",
    "\n",
    "1. **The Exploration Problem**: Unlike supervised learning where training data is provided, RL agents must actively explore to gather experience. Where does the training data come from?\n",
    "\n",
    "2. **Credit Assignment**: Understanding which actions taken early in a sequence led to rewards received much later—a fundamentally different problem from supervised learning's immediate input-output mapping.\n",
    "\n",
    "3. **Safety in Autonomous Systems**: When agents collect their own data through exploration, random exploration can cause real-world damage before learning anything useful.\n",
    "\n",
    "4. **Long-Horizon Reasoning**: Effective planning and action over extended time periods (days or lifetimes), not just seconds.\n",
    "\n",
    "These challenges define the frontier of deep reinforcement learning research and applications."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d769e51",
   "metadata": {},
   "source": [
    "**Explain intuitions and mental models for the lesson**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fc2e72c",
   "metadata": {},
   "source": [
    "## Intuitions and Mental Models\n",
    "\n",
    "**The Representation Problem is Solved; New Problems Emerge**\n",
    "\n",
    "Deep networks excel at solving the representation problem—they can capture complex patterns in raw sensory data. However, solving representation learning doesn't automatically solve reinforcement learning. RL still faces exploration, credit assignment, and safety challenges that are orthogonal to representation quality.\n",
    "\n",
    "**Exploration as Active Data Collection**\n",
    "\n",
    "In supervised learning, data is given. In reinforcement learning, the agent must actively explore to gather experience. This raises a fundamental question: where does training data come from? The agent must balance exploring new actions (to discover better strategies) with exploiting known good actions (to maximize immediate reward).\n",
    "\n",
    "**Credit Assignment Across Time**\n",
    "\n",
    "When an agent receives a reward after many steps, it must determine which earlier actions were responsible. This is harder than supervised learning because there's no immediate feedback signal for each action. The agent must propagate credit backward through time, accounting for the delayed consequences of its decisions.\n",
    "\n",
    "**Safety Through Constrained Learning**\n",
    "\n",
    "Behavioral cloning (learning from human demonstrations) can bootstrap RL by providing initial good behavior. Then reinforcement learning refines this behavior with explicit objectives. This hybrid approach reduces the risk of dangerous exploration early in training.\n",
    "\n",
    "**Learning the Algorithm Itself**\n",
    "\n",
    "Just as deep learning replaced hand-engineered features, meta-learning could replace hand-designed RL algorithms. One RL algorithm could learn to modify another RL algorithm's parameters based on task performance, enabling the system to discover better learning strategies automatically."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30a603c3",
   "metadata": {},
   "source": [
    "**Present and explain the key equations used in the lesson**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c50ae30",
   "metadata": {},
   "source": [
    "## Key Equations and Formulas\n",
    "\n",
    "**Exploration vs. Exploitation**\n",
    "\n",
    "$$\\text{Where does training data come from?}$$\n",
    "\n",
    "This is the fundamental question of the exploration problem. In reinforcement learning, the agent must decide whether to explore new actions (gathering information about the environment) or exploit known good actions (maximizing immediate reward). The balance between these two drives the data collection process.\n",
    "\n",
    "**Credit Assignment**\n",
    "\n",
    "$$\\text{Which early actions led to later rewards?}$$\n",
    "\n",
    "When an agent receives a reward signal at time $t$, it must determine which actions taken at earlier times $t-1, t-2, \\ldots$ were responsible. This requires propagating credit backward through the sequence of decisions, accounting for the causal relationships between actions and outcomes.\n",
    "\n",
    "**Safety Constraint**\n",
    "\n",
    "$$\\text{Minimize accidents during autonomous data collection}$$\n",
    "\n",
    "When an autonomous system learns by exploring, it must do so safely. The safety constraint requires that the cost of exploration (accidents, damage, resource waste) remains acceptable while the agent learns to perform its task effectively."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01c0a27d",
   "metadata": {},
   "source": [
    "**Implement code primitive: Implement a deep Q-network (DQN) agent that learns to play Atari games from raw pixel inputs, demonstrating end-to-end learning from sensory input to action output.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e3ce7d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from collections import deque\n",
    "\n",
    "class DQNNetwork(nn.Module):\n",
    "    def __init__(self, input_channels, num_actions):\n",
    "        super(DQNNetwork, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(input_channels, 32, kernel_size=8, stride=4)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=4, stride=2)\n",
    "        self.conv3 = nn.Conv2d(64, 64, kernel_size=3, stride=1)\n",
    "        self.fc1 = nn.Linear(64 * 7 * 7, 512)\n",
    "        self.fc2 = nn.Linear(512, num_actions)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.conv1(x))\n",
    "        x = torch.relu(self.conv2(x))\n",
    "        x = torch.relu(self.conv3(x))\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        return self.fc2(x)\n",
    "\n",
    "class DQNAgent:\n",
    "    def __init__(self, input_channels, num_actions, learning_rate=0.0001, gamma=0.99):\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        self.q_network = DQNNetwork(input_channels, num_actions).to(self.device)\n",
    "        self.target_network = DQNNetwork(input_channels, num_actions).to(self.device)\n",
    "        self.target_network.load_state_dict(self.q_network.state_dict())\n",
    "        self.optimizer = optim.Adam(self.q_network.parameters(), lr=learning_rate)\n",
    "        self.gamma = gamma\n",
    "        self.num_actions = num_actions\n",
    "        self.memory = deque(maxlen=100000)\n",
    "        self.epsilon = 1.0\n",
    "    \n",
    "    def select_action(self, state):\n",
    "        if np.random.random() < self.epsilon:\n",
    "            return np.random.randint(self.num_actions)\n",
    "        with torch.no_grad():\n",
    "            state_tensor = torch.FloatTensor(state).unsqueeze(0).to(self.device)\n",
    "            q_values = self.q_network(state_tensor)\n",
    "            return q_values.argmax(dim=1).item()\n",
    "    \n",
    "    def store_transition(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "    \n",
    "    def train(self, batch_size):\n",
    "        if len(self.memory) < batch_size:\n",
    "            return\n",
    "        \n",
    "        batch = np.random.choice(len(self.memory), batch_size, replace=False)\n",
    "        states, actions, rewards, next_states, dones = zip(*[self.memory[i] for i in batch])\n",
    "        \n",
    "        states = torch.FloatTensor(np.array(states)).to(self.device)\n",
    "        actions = torch.LongTensor(actions).to(self.device)\n",
    "        rewards = torch.FloatTensor(rewards).to(self.device)\n",
    "        next_states = torch.FloatTensor(np.array(next_states)).to(self.device)\n",
    "        dones = torch.FloatTensor(dones).to(self.device)\n",
    "        \n",
    "        q_values = self.q_network(states).gather(1, actions.unsqueeze(1)).squeeze(1)\n",
    "        next_q_values = self.target_network(next_states).max(dim=1)[0]\n",
    "        target_q_values = rewards + self.gamma * next_q_values * (1 - dones)\n",
    "        \n",
    "        loss = nn.functional.mse_loss(q_values, target_q_values)\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "    \n",
    "    def update_target_network(self):\n",
    "        self.target_network.load_state_dict(self.q_network.state_dict())\n",
    "    \n",
    "    def decay_epsilon(self, decay_rate=0.995):\n",
    "        self.epsilon *= decay_rate\n",
    "\n",
    "agent = DQNAgent(input_channels=4, num_actions=18)\n",
    "print(f\"DQN Agent initialized with Q-network: {agent.q_network}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe938c65",
   "metadata": {},
   "source": [
    "**Implement code primitive: Demonstrate transfer of a trained RL policy across different robot morphologies (two-legged to four-legged) using the same algorithm, showing generalization without retraining.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d49ff6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class RobotPolicyNetwork(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, hidden_dim=128):\n",
    "        super(RobotPolicyNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.fc3 = nn.Linear(hidden_dim, action_dim)\n",
    "    \n",
    "    def forward(self, state):\n",
    "        x = torch.relu(self.fc1(state))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        return torch.tanh(self.fc3(x))\n",
    "\n",
    "class RobotController:\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        self.policy = RobotPolicyNetwork(state_dim, action_dim)\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        self.policy.to(self.device)\n",
    "    \n",
    "    def select_action(self, state):\n",
    "        state_tensor = torch.FloatTensor(state).unsqueeze(0).to(self.device)\n",
    "        with torch.no_grad():\n",
    "            action = self.policy(state_tensor).cpu().numpy()[0]\n",
    "        return action\n",
    "    \n",
    "    def save_policy(self, filepath):\n",
    "        torch.save(self.policy.state_dict(), filepath)\n",
    "    \n",
    "    def load_policy(self, filepath):\n",
    "        self.policy.load_state_dict(torch.load(filepath))\n",
    "\n",
    "class TwoLeggedRobot:\n",
    "    def __init__(self):\n",
    "        self.state_dim = 8\n",
    "        self.action_dim = 2\n",
    "    \n",
    "    def get_state(self):\n",
    "        return np.random.randn(self.state_dim)\n",
    "    \n",
    "    def execute_action(self, action):\n",
    "        reward = np.sum(action ** 2)\n",
    "        return reward\n",
    "\n",
    "class FourLeggedRobot:\n",
    "    def __init__(self):\n",
    "        self.state_dim = 12\n",
    "        self.action_dim = 4\n",
    "    \n",
    "    def get_state(self):\n",
    "        return np.random.randn(self.state_dim)\n",
    "    \n",
    "    def execute_action(self, action):\n",
    "        reward = np.sum(action ** 2)\n",
    "        return reward\n",
    "\n",
    "controller_2leg = RobotController(state_dim=8, action_dim=2)\n",
    "controller_2leg.save_policy('policy_2leg.pt')\n",
    "\n",
    "controller_4leg = RobotController(state_dim=12, action_dim=4)\n",
    "controller_4leg.load_policy('policy_2leg.pt')\n",
    "\n",
    "robot_2leg = TwoLeggedRobot()\n",
    "robot_4leg = FourLeggedRobot()\n",
    "\n",
    "state_2leg = robot_2leg.get_state()\n",
    "action_2leg = controller_2leg.select_action(state_2leg)\n",
    "reward_2leg = robot_2leg.execute_action(action_2leg)\n",
    "\n",
    "state_4leg = robot_4leg.get_state()\n",
    "action_4leg = controller_4leg.select_action(state_4leg[:8])\n",
    "reward_4leg = robot_4leg.execute_action(np.concatenate([action_4leg, action_4leg[:2]]))\n",
    "\n",
    "print(f\"2-legged robot reward: {reward_2leg:.4f}\")\n",
    "print(f\"4-legged robot reward (transferred policy): {reward_4leg:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d489e70d",
   "metadata": {},
   "source": [
    "**Implement code primitive: Build a behavioral cloning system that learns to mimic human actions from demonstrations, then augment it with reinforcement learning objectives to optimize for specific metrics.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf181d6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "class BehavioralCloningNetwork(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, hidden_dim=128):\n",
    "        super(BehavioralCloningNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.fc3 = nn.Linear(hidden_dim, action_dim)\n",
    "    \n",
    "    def forward(self, state):\n",
    "        x = torch.relu(self.fc1(state))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        return self.fc3(x)\n",
    "\n",
    "class BehavioralCloningAgent:\n",
    "    def __init__(self, state_dim, action_dim, learning_rate=0.001):\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        self.policy = BehavioralCloningNetwork(state_dim, action_dim).to(self.device)\n",
    "        self.optimizer = optim.Adam(self.policy.parameters(), lr=learning_rate)\n",
    "        self.bc_loss_fn = nn.MSELoss()\n",
    "        self.rl_loss_fn = nn.MSELoss()\n",
    "    \n",
    "    def behavioral_cloning_loss(self, states, expert_actions):\n",
    "        predicted_actions = self.policy(states)\n",
    "        return self.bc_loss_fn(predicted_actions, expert_actions)\n",
    "    \n",
    "    def reinforcement_learning_loss(self, states, predicted_actions, rewards):\n",
    "        action_quality = torch.sum(predicted_actions * predicted_actions, dim=1)\n",
    "        reward_tensor = torch.FloatTensor(rewards).to(self.device)\n",
    "        return self.rl_loss_fn(action_quality, reward_tensor)\n",
    "    \n",
    "    def train_bc(self, states, expert_actions, epochs=10):\n",
    "        states = torch.FloatTensor(states).to(self.device)\n",
    "        expert_actions = torch.FloatTensor(expert_actions).to(self.device)\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            loss = self.behavioral_cloning_loss(states, expert_actions)\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "    \n",
    "    def train_rl(self, states, rewards, epochs=10, bc_weight=0.5):\n",
    "        states = torch.FloatTensor(states).to(self.device)\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            predicted_actions = self.policy(states)\n",
    "            expert_actions = predicted_actions.detach()\n",
    "            \n",
    "            bc_loss = self.behavioral_cloning_loss(states, expert_actions)\n",
    "            rl_loss = self.reinforcement_learning_loss(states, predicted_actions, rewards)\n",
    "            \n",
    "            total_loss = bc_weight * bc_loss + (1 - bc_weight) * rl_loss\n",
    "            self.optimizer.zero_grad()\n",
    "            total_loss.backward()\n",
    "            self.optimizer.step()\n",
    "    \n",
    "    def select_action(self, state):\n",
    "        state_tensor = torch.FloatTensor(state).unsqueeze(0).to(self.device)\n",
    "        with torch.no_grad():\n",
    "            action = self.policy(state_tensor).cpu().numpy()[0]\n",
    "        return action\n",
    "\n",
    "state_dim, action_dim = 10, 3\n",
    "agent = BehavioralCloningAgent(state_dim, action_dim)\n",
    "\n",
    "expert_states = np.random.randn(100, state_dim)\n",
    "expert_actions = np.random.randn(100, action_dim)\n",
    "agent.train_bc(expert_states, expert_actions, epochs=5)\n",
    "\n",
    "rewards = np.random.rand(100)\n",
    "agent.train_rl(expert_states, rewards, epochs=5, bc_weight=0.7)\n",
    "\n",
    "test_state = np.random.randn(state_dim)\n",
    "action = agent.select_action(test_state)\n",
    "print(f\"Behavioral cloning + RL agent action: {action}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f6c5332",
   "metadata": {},
   "source": [
    "**Implement code primitive: Implement a meta-learning framework where one RL algorithm learns to modify another RL algorithm's parameters based on task performance, enabling algorithm learning.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4a47f0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "class BaseRLAlgorithm(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, hidden_dim=64):\n",
    "        super(BaseRLAlgorithm, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.fc3 = nn.Linear(hidden_dim, action_dim)\n",
    "        self.learning_rate = 0.01\n",
    "    \n",
    "    def forward(self, state):\n",
    "        x = torch.relu(self.fc1(state))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        return self.fc3(x)\n",
    "    \n",
    "    def get_parameters(self):\n",
    "        return torch.cat([p.view(-1) for p in self.parameters()])\n",
    "    \n",
    "    def set_parameters(self, params):\n",
    "        offset = 0\n",
    "        for p in self.parameters():\n",
    "            p.data = params[offset:offset + p.numel()].view(p.shape)\n",
    "            offset += p.numel()\n",
    "\n",
    "class MetaLearner(nn.Module):\n",
    "    def __init__(self, param_dim, hidden_dim=128):\n",
    "        super(MetaLearner, self).__init__()\n",
    "        self.fc1 = nn.Linear(param_dim + 1, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.fc3 = nn.Linear(hidden_dim, param_dim)\n",
    "    \n",
    "    def forward(self, params, task_performance):\n",
    "        x = torch.cat([params, task_performance.unsqueeze(0)], dim=0)\n",
    "        x = torch.relu(self.fc1(x.unsqueeze(0)))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        param_update = self.fc3(x)\n",
    "        return param_update.squeeze(0)\n",
    "\n",
    "class MetaRLFramework:\n",
    "    def __init__(self, state_dim, action_dim, param_dim=None):\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        self.base_algorithm = BaseRLAlgorithm(state_dim, action_dim).to(self.device)\n",
    "        \n",
    "        if param_dim is None:\n",
    "            param_dim = sum(p.numel() for p in self.base_algorithm.parameters())\n",
    "        \n",
    "        self.meta_learner = MetaLearner(param_dim).to(self.device)\n",
    "        self.meta_optimizer = optim.Adam(self.meta_learner.parameters(), lr=0.001)\n",
    "    \n",
    "    def adapt_algorithm(self, task_performance):\n",
    "        params = self.base_algorithm.get_parameters()\n",
    "        param_update = self.meta_learner(params, torch.FloatTensor([task_performance]).to(self.device))\n",
    "        new_params = params + 0.1 * param_update\n",
    "        self.base_algorithm.set_parameters(new_params)\n",
    "    \n",
    "    def select_action(self, state):\n",
    "        state_tensor = torch.FloatTensor(state).unsqueeze(0).to(self.device)\n",
    "        with torch.no_grad():\n",
    "            action = self.base_algorithm(state_tensor).cpu().numpy()[0]\n",
    "        return action\n",
    "    \n",
    "    def train_meta_learner(self, task_performances, num_iterations=10):\n",
    "        for iteration in range(num_iterations):\n",
    "            total_loss = 0\n",
    "            for perf in task_performances:\n",
    "                params = self.base_algorithm.get_parameters()\n",
    "                param_update = self.meta_learner(params, torch.FloatTensor([perf]).to(self.device))\n",
    "                new_params = params + 0.1 * param_update\n",
    "                \n",
    "                loss = torch.norm(param_update) * (1 - perf)\n",
    "                total_loss += loss\n",
    "            \n",
    "            self.meta_optimizer.zero_grad()\n",
    "            (total_loss / len(task_performances)).backward()\n",
    "            self.meta_optimizer.step()\n",
    "\n",
    "state_dim, action_dim = 8, 4\n",
    "meta_framework = MetaRLFramework(state_dim, action_dim)\n",
    "\n",
    "task_performances = [0.3, 0.5, 0.7, 0.6, 0.8]\n",
    "meta_framework.train_meta_learner(task_performances, num_iterations=5)\n",
    "\n",
    "test_state = np.random.randn(state_dim)\n",
    "action = meta_framework.select_action(test_state)\n",
    "print(f\"Meta-learned RL algorithm action: {action}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d053c5c4",
   "metadata": {},
   "source": [
    "**Create a Mermaid diagram: flowchart TD\n",
    "    A[Raw Sensory Input] -->|Deep Network| B[Learned Representation]\n",
    "    B -->|RL Algorithm| C[Action Selection]\n",
    "    C -->|Environment| D[Reward Signal]\n",
    "    D -->|Credit Assignment| E[Policy Update]\n",
    "    E -->|Exploration| A\n",
    "    style A fill:#e1f5ff\n",
    "    style B fill:#fff3e0\n",
    "    style C fill:#f3e5f5\n",
    "    style D fill:#e8f5e9\n",
    "    style E fill:#fce4ec**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c98de86",
   "metadata": {},
   "source": [
    "```mermaid\n",
    "flowchart TD\n",
    "    A[Raw Sensory Input] -->|Deep Network| B[Learned Representation]\n",
    "    B -->|RL Algorithm| C[Action Selection]\n",
    "    C -->|Environment| D[Reward Signal]\n",
    "    D -->|Credit Assignment| E[Policy Update]\n",
    "    E -->|Exploration| A\n",
    "    style A fill:#e1f5ff\n",
    "    style B fill:#fff3e0\n",
    "    style C fill:#f3e5f5\n",
    "    style D fill:#e8f5e9\n",
    "    style E fill:#fce4ec\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efbef251",
   "metadata": {},
   "source": [
    "**Create a Mermaid diagram: flowchart TD\n",
    "    A[Behavioral Cloning] -->|Supervised Learning| B[Mimic Human Behavior]\n",
    "    B -->|Initial Policy| C[Reinforcement Learning]\n",
    "    C -->|Explicit Objectives| D[Optimized Policy]\n",
    "    D -->|Deployment| E[Real-World System]\n",
    "    style A fill:#fff3e0\n",
    "    style B fill:#f3e5f5\n",
    "    style C fill:#e8f5e9\n",
    "    style D fill:#fce4ec\n",
    "    style E fill:#e1f5ff**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "794eb193",
   "metadata": {},
   "source": [
    "```mermaid\n",
    "flowchart TD\n",
    "    A[Behavioral Cloning] -->|Supervised Learning| B[Mimic Human Behavior]\n",
    "    B -->|Initial Policy| C[Reinforcement Learning]\n",
    "    C -->|Explicit Objectives| D[Optimized Policy]\n",
    "    D -->|Deployment| E[Real-World System]\n",
    "    style A fill:#fff3e0\n",
    "    style B fill:#f3e5f5\n",
    "    style C fill:#e8f5e9\n",
    "    style D fill:#fce4ec\n",
    "    style E fill:#e1f5ff\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5e18fa2",
   "metadata": {},
   "source": [
    "**Create a Mermaid diagram: flowchart TD\n",
    "    A[Exploration] -->|Data Collection| B[Credit Assignment Problem]\n",
    "    B -->|Identify Causal Actions| C[Policy Improvement]\n",
    "    C -->|New Behavior| D[Safety Risk]\n",
    "    D -->|Autonomous Learning| A\n",
    "    A -->|Challenge| E[Where does data come from?]\n",
    "    B -->|Challenge| F[Which actions caused rewards?]\n",
    "    D -->|Challenge| G[How to learn safely?]\n",
    "    style E fill:#ffebee\n",
    "    style F fill:#ffebee\n",
    "    style G fill:#ffebee**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f373f2f",
   "metadata": {},
   "source": [
    "```mermaid\n",
    "flowchart TD\n",
    "    A[Exploration] -->|Data Collection| B[Credit Assignment Problem]\n",
    "    B -->|Identify Causal Actions| C[Policy Improvement]\n",
    "    C -->|New Behavior| D[Safety Risk]\n",
    "    D -->|Autonomous Learning| A\n",
    "    A -->|Challenge| E[Where does data come from?]\n",
    "    B -->|Challenge| F[Which actions caused rewards?]\n",
    "    D -->|Challenge| G[How to learn safely?]\n",
    "    style E fill:#ffebee\n",
    "    style F fill:#ffebee\n",
    "    style G fill:#ffebee\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6524aff6",
   "metadata": {},
   "source": [
    "# Week 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f997adc2",
   "metadata": {},
   "source": [
    "## Lesson 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db48316d",
   "metadata": {},
   "source": [
    "**Explain the core concepts of the lesson**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c6fce33",
   "metadata": {},
   "source": [
    "## Core Concepts\n",
    "\n",
    "A neural network is fundamentally a repetition of logistic regression across multiple layers. Each layer performs the same computational pattern:\n",
    "\n",
    "1. **Neural Network Layer**: A layer consists of stacked sigmoid units that transform inputs through linear and activation steps.\n",
    "\n",
    "2. **Layer Notation (Square Brackets)**: We use square brackets to denote layer indices. For example, $w^{[1]}$ refers to weights in layer 1, while $w^{[2]}$ refers to weights in layer 2. Round brackets distinguish between different training examples.\n",
    "\n",
    "3. **Z and A Calculations**: Each layer computes two key quantities:\n",
    "   - **z**: The linear combination of inputs and weights plus bias\n",
    "   - **a**: The activated output after applying the sigmoid function\n",
    "\n",
    "4. **Forward Propagation Layers**: Information flows left to right through the network, with each layer's output becoming the next layer's input.\n",
    "\n",
    "5. **Backward Propagation Layers**: During training, derivatives flow right to left through the network, computing gradients for all parameters.\n",
    "\n",
    "6. **Hidden Layer Computation**: Intermediate layers (between input and output) compute their z and a values using the previous layer's activation as input.\n",
    "\n",
    "7. **Output Layer Computation**: The final layer produces the network's prediction, which is compared against the target to compute loss.\n",
    "\n",
    "8. **Multi-layer Computation Graph**: The entire network forms a directed acyclic graph where each layer's computation depends on the previous layer's output."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29b6eff3",
   "metadata": {},
   "source": [
    "**Explain intuitions and mental models for the lesson**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b23c699b",
   "metadata": {},
   "source": [
    "## Intuitions and Mental Models\n",
    "\n",
    "**Intuition 1: Repeated Logistic Regression**\n",
    "\n",
    "A neural network is logistic regression repeated multiple times. In logistic regression, we compute $z = wx + b$ and then $a = \\sigma(z)$. In a neural network, we repeat this exact pattern at each layer, but instead of using the raw input $x$, each layer uses the previous layer's activation $a^{[l-1]}$ as its input.\n",
    "\n",
    "**Intuition 2: Stacked Sigmoid Units**\n",
    "\n",
    "Each layer consists of stacked sigmoid units. Think of each unit as a small logistic regression classifier. When you stack multiple units in a layer, you're creating multiple parallel logistic regressors that all operate on the same input. These units learn different features and patterns.\n",
    "\n",
    "**Intuition 3: Layer Notation as Organizational Tool**\n",
    "\n",
    "The square bracket notation is purely organizational—it helps us keep track of which layer we're talking about. Layer 1 is the first hidden layer, layer 2 is the second hidden layer, and so on. This notation cleanly separates the concept of \"which layer\" (square brackets) from \"which training example\" (round brackets).\n",
    "\n",
    "**Intuition 4: Forward and Backward Flow**\n",
    "\n",
    "During forward propagation, information flows left to right: input → layer 1 → layer 2 → output. During backward propagation, error signals flow right to left: output loss → layer 2 gradients → layer 1 gradients → input gradients. This bidirectional flow is the essence of how neural networks learn."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4f73edc",
   "metadata": {},
   "source": [
    "**Present and explain the key equations used in the lesson**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5af578f",
   "metadata": {},
   "source": [
    "## Key Equations\n",
    "\n",
    "**Layer 1 Computation:**\n",
    "\n",
    "The first hidden layer takes the input $x$ and computes:\n",
    "\n",
    "$$z^{[1]} = w^{[1]}x + b^{[1]}$$\n",
    "\n",
    "$$a^{[1]} = \\sigma(z^{[1]})$$\n",
    "\n",
    "where $w^{[1]}$ are the weights, $b^{[1]}$ is the bias, and $\\sigma$ is the sigmoid activation function.\n",
    "\n",
    "**Layer 2 Computation:**\n",
    "\n",
    "The second layer takes the output of layer 1 as input:\n",
    "\n",
    "$$z^{[2]} = w^{[2]}a^{[1]} + b^{[2]}$$\n",
    "\n",
    "$$a^{[2]} = \\sigma(z^{[2]})$$\n",
    "\n",
    "Notice that the input to layer 2 is $a^{[1]}$ (the activation from layer 1), not the original input $x$.\n",
    "\n",
    "**General Pattern:**\n",
    "\n",
    "For any layer $l$:\n",
    "\n",
    "$$z^{[l]} = w^{[l]}a^{[l-1]} + b^{[l]}$$\n",
    "\n",
    "$$a^{[l]} = \\sigma(z^{[l]})$$\n",
    "\n",
    "This pattern repeats for each layer in the network, creating a chain of transformations from input to output."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92915a83",
   "metadata": {},
   "source": [
    "**Implement code primitive: Implement forward propagation through multiple layers by sequentially computing z and a values for each layer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc34f1a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "def forward_propagation(X, parameters):\n",
    "    \"\"\"\n",
    "    Implement forward propagation through multiple layers.\n",
    "    \n",
    "    Args:\n",
    "        X: Input data of shape (n_x, m) where n_x is number of features, m is number of examples\n",
    "        parameters: Dictionary containing weights and biases for each layer\n",
    "                   keys: 'W1', 'b1', 'W2', 'b2', etc.\n",
    "    \n",
    "    Returns:\n",
    "        cache: Dictionary containing all z and a values for each layer\n",
    "    \"\"\"\n",
    "    cache = {}\n",
    "    A = X\n",
    "    layer = 1\n",
    "    \n",
    "    while f'W{layer}' in parameters:\n",
    "        W = parameters[f'W{layer}']\n",
    "        b = parameters[f'b{layer}']\n",
    "        \n",
    "        Z = np.dot(W, A) + b\n",
    "        A = sigmoid(Z)\n",
    "        \n",
    "        cache[f'Z{layer}'] = Z\n",
    "        cache[f'A{layer}'] = A\n",
    "        \n",
    "        layer += 1\n",
    "    \n",
    "    return cache, A\n",
    "\n",
    "# Example usage\n",
    "X = np.random.randn(2, 3)  # 2 features, 3 examples\n",
    "parameters = {\n",
    "    'W1': np.random.randn(3, 2),\n",
    "    'b1': np.random.randn(3, 1),\n",
    "    'W2': np.random.randn(1, 3),\n",
    "    'b2': np.random.randn(1, 1)\n",
    "}\n",
    "\n",
    "cache, output = forward_propagation(X, parameters)\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "print(f\"Cache keys: {list(cache.keys())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f232bf25",
   "metadata": {},
   "source": [
    "**Implement code primitive: Implement backward propagation by computing derivatives (da, dz, dw, db) flowing from output layer back to input layer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "077a22df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid_derivative(Z):\n",
    "    A = sigmoid(Z)\n",
    "    return A * (1 - A)\n",
    "\n",
    "def backward_propagation(dA, cache, parameters, m):\n",
    "    \"\"\"\n",
    "    Implement backward propagation through multiple layers.\n",
    "    \n",
    "    Args:\n",
    "        dA: Gradient of loss with respect to output activation\n",
    "        cache: Dictionary containing Z and A values from forward propagation\n",
    "        parameters: Dictionary containing weights and biases\n",
    "        m: Number of training examples\n",
    "    \n",
    "    Returns:\n",
    "        gradients: Dictionary containing dW and db for each layer\n",
    "    \"\"\"\n",
    "    gradients = {}\n",
    "    layer = 2\n",
    "    \n",
    "    while f'Z{layer}' in cache:\n",
    "        Z = cache[f'Z{layer}']\n",
    "        A_prev = cache[f'A{layer-1}']\n",
    "        W = parameters[f'W{layer}']\n",
    "        \n",
    "        dZ = dA * sigmoid_derivative(Z)\n",
    "        dW = np.dot(dZ, A_prev.T) / m\n",
    "        db = np.sum(dZ, axis=1, keepdims=True) / m\n",
    "        dA = np.dot(W.T, dZ)\n",
    "        \n",
    "        gradients[f'dW{layer}'] = dW\n",
    "        gradients[f'db{layer}'] = db\n",
    "        \n",
    "        layer -= 1\n",
    "    \n",
    "    # Handle first layer\n",
    "    Z = cache['Z1']\n",
    "    A_prev = cache.get('A0', None)  # Would be input X\n",
    "    dZ = dA * sigmoid_derivative(Z)\n",
    "    \n",
    "    gradients['dW1'] = dZ\n",
    "    gradients['db1'] = np.sum(dZ, axis=1, keepdims=True) / m\n",
    "    \n",
    "    return gradients\n",
    "\n",
    "# Example usage\n",
    "m = 3  # number of examples\n",
    "dA_output = np.random.randn(1, m)\n",
    "gradients = backward_propagation(dA_output, cache, parameters, m)\n",
    "print(f\"Gradient keys: {list(gradients.keys())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8869e61",
   "metadata": {},
   "source": [
    "**Create a Mermaid diagram: flowchart LR\n",
    "    X[\"Input x\"] --> Z1[\"Compute z^[1]\"] --> A1[\"Compute a^[1]\"] --> Z2[\"Compute z^[2]\"] --> A2[\"Compute a^[2]\"] --> L[\"Loss L\"]\n",
    "    style Z1 fill:#e1f5ff\n",
    "    style A1 fill:#e1f5ff\n",
    "    style Z2 fill:#fff3e0\n",
    "    style A2 fill:#fff3e0**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be920794",
   "metadata": {},
   "source": [
    "## Computation Flow Diagram\n",
    "\n",
    "```mermaid\n",
    "flowchart LR\n",
    "    X[\"Input x\"] --> Z1[\"Compute z^[1]\"] --> A1[\"Compute a^[1]\"] --> Z2[\"Compute z^[2]\"] --> A2[\"Compute a^[2]\"] --> L[\"Loss L\"]\n",
    "    style Z1 fill:#e1f5ff\n",
    "    style A1 fill:#e1f5ff\n",
    "    style Z2 fill:#fff3e0\n",
    "    style A2 fill:#fff3e0\n",
    "```\n",
    "\n",
    "This diagram shows the forward propagation flow through a two-layer neural network. The input $x$ flows through layer 1 (light blue) where it is transformed into $z^{[1]}$ and then activated to $a^{[1]}$. The activation $a^{[1]}$ then flows through layer 2 (light orange) where it is transformed into $z^{[2]}$ and activated to $a^{[2]}$. Finally, the output $a^{[2]}$ is used to compute the loss $L$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c786bb9",
   "metadata": {},
   "source": [
    "## Lesson 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa783444",
   "metadata": {},
   "source": [
    "**Explain the core concepts of the lesson**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00be2f5e",
   "metadata": {},
   "source": [
    "## Core Concepts\n",
    "\n",
    "A neural network is organized into distinct layers that process information sequentially:\n",
    "\n",
    "- **Input Layer**: Contains the raw features of your data, stacked as a vector. This is denoted as $A^{[0]} = X$.\n",
    "\n",
    "- **Hidden Layer**: An intermediate layer that learns representations of the input. The activations (outputs) of this layer are denoted $A^{[1]}$. It is called \"hidden\" because we do not observe the true values of its nodes in the training set—we only see inputs and desired outputs.\n",
    "\n",
    "- **Output Layer**: Produces the final prediction of the network. For a single output, this is denoted $\\hat{y} = A^{[2]}$.\n",
    "\n",
    "- **Activation Values**: The outputs computed by each layer as data flows forward through the network. These values represent what each layer computes and passes to the next layer.\n",
    "\n",
    "- **Layer Parameters**: Each layer has associated weights ($W$) and biases ($b$) that determine how it transforms its inputs into outputs.\n",
    "\n",
    "- **Two-Layer Network**: A network with one hidden layer is called a two-layer network because we count layers starting from the hidden layer (we skip the input layer in the count)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e23337b3",
   "metadata": {},
   "source": [
    "**Explain intuitions and mental models for the lesson**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "657b0df2",
   "metadata": {},
   "source": [
    "## Intuitions and Mental Models\n",
    "\n",
    "**Layered Processing**: Think of a neural network as a pipeline. Data enters through the input layer, gets transformed by the hidden layer, and emerges as a prediction from the output layer. Each layer builds on the representations created by the previous layer.\n",
    "\n",
    "**Why \"Hidden\"?**: The hidden layer is called hidden not because it is mysterious, but because we never see its true values during training. We only observe the inputs we provide and the outputs we want to match. The hidden layer's job is to learn useful intermediate representations that help the network make better predictions.\n",
    "\n",
    "**Activations as Information Flow**: Activations are the actual numerical values flowing through the network at each stage. Think of them as the \"state\" of the network at each layer—they carry the processed information forward.\n",
    "\n",
    "**Counting Layers**: When we say \"two-layer network,\" we are counting the hidden layer and the output layer. The input layer is not counted because it simply holds the raw data without performing any computation.\n",
    "\n",
    "**Parameters Shape the Transformation**: The weights and biases in each layer determine exactly how that layer transforms its input. The dimensions of these parameters are carefully chosen to match the number of units in the current layer and the previous layer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea89422b",
   "metadata": {},
   "source": [
    "**Present and explain the key equations used in the lesson**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "085f33ed",
   "metadata": {},
   "source": [
    "## Key Equations\n",
    "\n",
    "**Input Layer Activation**:\n",
    "$$A^{[0]} = X$$\n",
    "The input layer activation is simply the input data itself.\n",
    "\n",
    "**Hidden Layer Activation**:\n",
    "$$A^{[1]} \\in \\mathbb{R}^{4 \\times 1}$$\n",
    "The hidden layer has 4 units, so its activation is a column vector with 4 rows and 1 column.\n",
    "\n",
    "**Output Layer Activation (Prediction)**:\n",
    "$$\\hat{y} = A^{[2]}$$\n",
    "The output layer produces the final prediction.\n",
    "\n",
    "**Hidden Layer Weights**:\n",
    "$$W^{[1]} \\in \\mathbb{R}^{4 \\times 3}$$\n",
    "The weight matrix for the hidden layer has 4 rows (one for each hidden unit) and 3 columns (one for each input feature).\n",
    "\n",
    "**Hidden Layer Bias**:\n",
    "$$b^{[1]} \\in \\mathbb{R}^{4 \\times 1}$$\n",
    "The bias vector for the hidden layer has 4 rows, matching the number of hidden units.\n",
    "\n",
    "**Output Layer Weights**:\n",
    "$$W^{[2]} \\in \\mathbb{R}^{1 \\times 4}$$\n",
    "The weight matrix for the output layer has 1 row (one output unit) and 4 columns (one for each hidden unit).\n",
    "\n",
    "**Output Layer Bias**:\n",
    "$$b^{[2]} \\in \\mathbb{R}^{1 \\times 1}$$\n",
    "The bias for the output layer is a scalar (1 row, 1 column)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c064f2c0",
   "metadata": {},
   "source": [
    "**Implement code primitive: Represent the input layer as a vector of features stacked vertically**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc43d7fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Input layer: 3 features stacked vertically\n",
    "A_0 = np.array([\n",
    "    [2.0],\n",
    "    [1.5],\n",
    "    [0.8]\n",
    "])\n",
    "\n",
    "print(\"Input layer (A^[0]):\")\n",
    "print(A_0)\n",
    "print(f\"Shape: {A_0.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2539b564",
   "metadata": {},
   "source": [
    "**Implement code primitive: Store hidden layer activations as a column vector with dimensions matching the number of hidden units**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69f9a0f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hidden layer: 4 units, represented as a column vector\n",
    "A_1 = np.array([\n",
    "    [0.5],\n",
    "    [0.3],\n",
    "    [0.9],\n",
    "    [0.1]\n",
    "])\n",
    "\n",
    "print(\"Hidden layer activations (A^[1]):\")\n",
    "print(A_1)\n",
    "print(f\"Shape: {A_1.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc8de379",
   "metadata": {},
   "source": [
    "**Implement code primitive: Represent output layer activation as a scalar value**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54c3b13c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output layer: single scalar output\n",
    "A_2 = np.array([[0.7]])\n",
    "\n",
    "print(\"Output layer activation (A^[2]):\")\n",
    "print(A_2)\n",
    "print(f\"Shape: {A_2.shape}\")\n",
    "print(f\"Prediction (ŷ): {A_2[0, 0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dd26131",
   "metadata": {},
   "source": [
    "**Implement code primitive: Organize layer parameters (W and b) with dimensions determined by the number of units in the current and previous layers**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2c41c18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hidden layer parameters\n",
    "# W^[1]: 4 hidden units × 3 input features\n",
    "W_1 = np.array([\n",
    "    [0.2, 0.5, 0.1],\n",
    "    [0.3, 0.1, 0.4],\n",
    "    [0.6, 0.2, 0.3],\n",
    "    [0.1, 0.4, 0.2]\n",
    "])\n",
    "\n",
    "# b^[1]: 4 hidden units × 1\n",
    "b_1 = np.array([\n",
    "    [0.1],\n",
    "    [0.2],\n",
    "    [0.05],\n",
    "    [0.15]\n",
    "])\n",
    "\n",
    "# Output layer parameters\n",
    "# W^[2]: 1 output unit × 4 hidden units\n",
    "W_2 = np.array([\n",
    "    [0.5, 0.3, 0.8, 0.2]\n",
    "])\n",
    "\n",
    "# b^[2]: 1 output unit × 1\n",
    "b_2 = np.array([[0.1]])\n",
    "\n",
    "print(\"Hidden layer weights W^[1]:\")\n",
    "print(W_1)\n",
    "print(f\"Shape: {W_1.shape}\")\n",
    "print(\"\\nHidden layer bias b^[1]:\")\n",
    "print(b_1)\n",
    "print(f\"Shape: {b_1.shape}\")\n",
    "print(\"\\nOutput layer weights W^[2]:\")\n",
    "print(W_2)\n",
    "print(f\"Shape: {W_2.shape}\")\n",
    "print(\"\\nOutput layer bias b^[2]:\")\n",
    "print(b_2)\n",
    "print(f\"Shape: {b_2.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efee31d8",
   "metadata": {},
   "source": [
    "**Create a Mermaid diagram: A flowchart showing data flow through a two-layer neural network: input layer → hidden layer → output layer, with activations and parameters labeled at each stage**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e637640b",
   "metadata": {},
   "source": [
    "## Neural Network Data Flow\n",
    "\n",
    "```mermaid\n",
    "graph TD\n",
    "    A[\"Input Layer<br/>A^[0] ∈ ℝ^3×1\"] -->|\"W^[1] ∈ ℝ^4×3<br/>b^[1] ∈ ℝ^4×1\"| B[\"Hidden Layer<br/>A^[1] ∈ ℝ^4×1\"]\n",
    "    B -->|\"W^[2] ∈ ℝ^1×4<br/>b^[2] ∈ ℝ^1×1\"| C[\"Output Layer<br/>ŷ = A^[2] ∈ ℝ^1×1\"]\n",
    "```\n",
    "\n",
    "The diagram shows how data flows through a two-layer neural network:\n",
    "\n",
    "1. **Input Layer**: Receives 3 features as a column vector $A^{[0]}$.\n",
    "2. **Hidden Layer**: Transforms the input using weights $W^{[1]}$ and bias $b^{[1]}$ to produce 4 hidden unit activations $A^{[1]}$.\n",
    "3. **Output Layer**: Transforms the hidden activations using weights $W^{[2]}$ and bias $b^{[2]}$ to produce the final prediction $\\hat{y} = A^{[2]}$.\n",
    "\n",
    "Each arrow is labeled with the parameters that govern that transformation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8c192e4",
   "metadata": {},
   "source": [
    "## Lesson 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b0d6a3f",
   "metadata": {},
   "source": [
    "**Explain the core concepts of the lesson**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e088b0eb",
   "metadata": {},
   "source": [
    "## Core Concepts\n",
    "\n",
    "Forward propagation in a two-layer neural network involves computing predictions by passing input data through two sequential layers of computation.\n",
    "\n",
    "**Hidden Layer Computation**: The first layer takes the input vector $x$ and computes a hidden representation. Each hidden node performs a linear transformation followed by an activation function, similar to logistic regression but with different parameters for each node.\n",
    "\n",
    "**Layer Notation Convention**: We use square brackets to denote the layer number. For example, $W^{[1]}$ refers to weights in the first layer, and $a^{[1]}$ refers to activations from the first layer. Subscripts indicate specific nodes within a layer.\n",
    "\n",
    "**Weight Matrix Stacking**: Instead of computing each hidden node individually with loops, we stack all weight vectors into a matrix. This allows us to compute all hidden nodes simultaneously through a single matrix-vector multiplication.\n",
    "\n",
    "**Output Layer Computation**: The hidden layer activations become inputs to the output layer, which performs another linear transformation and activation to produce the final prediction.\n",
    "\n",
    "**Vectorized Forward Pass**: By organizing computations as matrix operations, we can efficiently compute the entire forward pass without explicit loops, making the implementation fast and scalable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6e36a11",
   "metadata": {},
   "source": [
    "**Explain intuitions and mental models for the lesson**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2385635",
   "metadata": {},
   "source": [
    "## Intuitions and Mental Models\n",
    "\n",
    "**Neural Networks as Repeated Logistic Regression**: A neural network is logistic regression applied multiple times. Each hidden node performs the same z and activation computation as logistic regression, but with different parameters. The output layer then applies logistic regression to the hidden layer outputs.\n",
    "\n",
    "**Stacking for Efficiency**: Stacking parameter vectors into matrices allows you to compute all hidden nodes simultaneously instead of using loops. This vectorization is not just a convenience—it's essential for efficient computation on modern hardware.\n",
    "\n",
    "**Layer Notation as Bookkeeping**: The notation with square brackets for layer number and subscripts for node index keeps track of which layer and which node within that layer you're referring to. This systematic notation prevents confusion when working with multiple layers.\n",
    "\n",
    "**Chaining Transformations**: Hidden layer outputs become inputs to the output layer, creating a chain of transformations from raw features to final prediction. Each layer learns to transform its input into a more useful representation for the next layer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dc20bb8",
   "metadata": {},
   "source": [
    "**Present and explain the key equations used in the lesson**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10dc7152",
   "metadata": {},
   "source": [
    "## Key Equations\n",
    "\n",
    "The forward propagation process follows a sequence of four key computations:\n",
    "\n",
    "**Hidden Layer Linear Transformation**:\n",
    "$$z^{[1]} = W^{[1]} x + b^{[1]}$$\n",
    "\n",
    "Here, $W^{[1]}$ is the weight matrix for the first layer, $x$ is the input vector, and $b^{[1]}$ is the bias vector. The result $z^{[1]}$ is the pre-activation value for the hidden layer.\n",
    "\n",
    "**Hidden Layer Activation**:\n",
    "$$a^{[1]} = \\sigma(z^{[1]})$$\n",
    "\n",
    "The sigmoid function $\\sigma$ is applied element-wise to each component of $z^{[1]}$ to produce the hidden layer activations $a^{[1]}$.\n",
    "\n",
    "**Output Layer Linear Transformation**:\n",
    "$$z^{[2]} = W^{[2]} a^{[1]} + b^{[2]}$$\n",
    "\n",
    "The hidden layer activations $a^{[1]}$ are transformed by the second layer weights $W^{[2]}$ and bias $b^{[2]}$ to produce $z^{[2]}$.\n",
    "\n",
    "**Output Layer Activation (Final Prediction)**:\n",
    "$$\\hat{y} = a^{[2]} = \\sigma(z^{[2]})$$\n",
    "\n",
    "The sigmoid function is applied to $z^{[2]}$ to produce the final prediction $\\hat{y}$, which is also denoted as $a^{[2]}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fe9fe59",
   "metadata": {},
   "source": [
    "**Implement code primitive: Compute z for hidden layer by matrix-vector multiplication: W[1] times x plus b[1]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0d0c07c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Example dimensions\n",
    "n_x = 2  # input features\n",
    "n_h = 3  # hidden nodes\n",
    "\n",
    "# Initialize parameters\n",
    "W1 = np.random.randn(n_h, n_x)\n",
    "b1 = np.zeros((n_h, 1))\n",
    "x = np.array([[1.0], [2.0]])\n",
    "\n",
    "# Compute z[1]\n",
    "z1 = np.dot(W1, x) + b1\n",
    "print(\"z[1] shape:\", z1.shape)\n",
    "print(\"z[1]:\\n\", z1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "507043ff",
   "metadata": {},
   "source": [
    "**Implement code primitive: Apply sigmoid element-wise to z[1] to get activation vector a[1]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2e6fe98",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "# Apply sigmoid to z[1]\n",
    "a1 = sigmoid(z1)\n",
    "print(\"a[1] shape:\", a1.shape)\n",
    "print(\"a[1]:\\n\", a1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbc7cfc8",
   "metadata": {},
   "source": [
    "**Implement code primitive: Compute z for output layer: W[2] times a[1] plus b[2]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ec66373",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize output layer parameters\n",
    "n_y = 1  # output nodes\n",
    "W2 = np.random.randn(n_y, n_h)\n",
    "b2 = np.zeros((n_y, 1))\n",
    "\n",
    "# Compute z[2]\n",
    "z2 = np.dot(W2, a1) + b2\n",
    "print(\"z[2] shape:\", z2.shape)\n",
    "print(\"z[2]:\\n\", z2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "541b6f92",
   "metadata": {},
   "source": [
    "**Implement code primitive: Apply sigmoid to z[2] to get final prediction**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78d23fc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply sigmoid to z[2] for final prediction\n",
    "a2 = sigmoid(z2)\n",
    "print(\"a[2] (prediction) shape:\", a2.shape)\n",
    "print(\"a[2] (prediction):\\n\", a2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "482cb01f",
   "metadata": {},
   "source": [
    "**Implement code primitive: Stack individual node computations into vectorized matrix operations**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a84bfc00",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_propagation(x, W1, b1, W2, b2):\n",
    "    \"\"\"\n",
    "    Vectorized forward propagation for a two-layer neural network.\n",
    "    \n",
    "    Parameters:\n",
    "    x: input vector (n_x, 1)\n",
    "    W1: weights for hidden layer (n_h, n_x)\n",
    "    b1: bias for hidden layer (n_h, 1)\n",
    "    W2: weights for output layer (n_y, n_h)\n",
    "    b2: bias for output layer (n_y, 1)\n",
    "    \n",
    "    Returns:\n",
    "    a2: final prediction (n_y, 1)\n",
    "    \"\"\"\n",
    "    z1 = np.dot(W1, x) + b1\n",
    "    a1 = sigmoid(z1)\n",
    "    z2 = np.dot(W2, a1) + b2\n",
    "    a2 = sigmoid(z2)\n",
    "    return a2\n",
    "\n",
    "# Test the vectorized forward propagation\n",
    "prediction = forward_propagation(x, W1, b1, W2, b2)\n",
    "print(\"Final prediction:\", prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d471847",
   "metadata": {},
   "source": [
    "**Create a Mermaid diagram: Flowchart showing the sequence of forward propagation: input x → compute z[1] → apply sigmoid → get a[1] → compute z[2] → apply sigmoid → get output prediction**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7c09a28",
   "metadata": {},
   "source": [
    "## Forward Propagation Flow\n",
    "\n",
    "```mermaid\n",
    "graph TD\n",
    "    A[\"Input: x\"] --> B[\"Compute z[1] = W[1]x + b[1]\"]\n",
    "    B --> C[\"Apply sigmoid: a[1] = σ(z[1])\"]\n",
    "    C --> D[\"Compute z[2] = W[2]a[1] + b[2]\"]\n",
    "    D --> E[\"Apply sigmoid: ŷ = σ(z[2])\"]\n",
    "    E --> F[\"Output: ŷ\"]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b62bc21b",
   "metadata": {},
   "source": [
    "## Lesson 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0be32d1",
   "metadata": {},
   "source": [
    "**Explain the core concepts of the lesson**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96e9b915",
   "metadata": {},
   "source": [
    "## Core Concepts\n",
    "\n",
    "Vectorization across multiple training examples is a fundamental technique in neural network computation that dramatically improves computational efficiency. Instead of processing training examples one at a time using loops, we can stack all examples into matrices and compute predictions for the entire batch simultaneously.\n",
    "\n",
    "The key concepts include:\n",
    "\n",
    "- **Vectorization across examples**: Processing all m training examples at once using matrix operations rather than iterating through them individually\n",
    "- **Matrix stacking convention**: Organizing data so that each column represents a single training example and each row represents a feature or node\n",
    "- **Training example indexing**: Understanding how examples are indexed horizontally across the matrix\n",
    "- **Hidden unit indexing**: Understanding how network nodes are indexed vertically within the matrix\n",
    "- **Vectorized forward propagation**: Computing activations for all examples in a single matrix operation\n",
    "- **Batch computation**: Processing multiple examples together as a batch\n",
    "- **Matrix dimensions**: Understanding how dimensions change through the network layers\n",
    "- **Activation matrix structure**: How activation matrices are organized with examples as columns and units as rows"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a34edb1f",
   "metadata": {},
   "source": [
    "**Explain intuitions and mental models for the lesson**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce478332",
   "metadata": {},
   "source": [
    "## Intuitions and Mental Models\n",
    "\n",
    "**The Column-Based Perspective**: Instead of looping through each training example one at a time, you can stack all examples as columns in a matrix and compute predictions for all of them simultaneously. This is like processing an entire batch of data in parallel rather than sequentially.\n",
    "\n",
    "**Horizontal vs. Vertical Organization**: The horizontal direction of a matrix represents different training examples, while the vertical direction represents different nodes or features in the network. This convention makes it easy to think about what each dimension represents.\n",
    "\n",
    "**Equation Similarity**: The vectorized equations are nearly identical to the single-example equations, just replacing lowercase vectors with uppercase matrices. For example, $z^{[l]} = W^{[l]} x + b^{[l]}$ becomes $Z^{[l]} = W^{[l]} X + b^{[l]}$. This consistency makes the transition intuitive.\n",
    "\n",
    "**Connection to Logistic Regression**: This approach mirrors the vectorization technique used in logistic regression, making the transition from single to batch computation straightforward. If you've already learned vectorization in the context of logistic regression, the same principles apply here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "099fd91a",
   "metadata": {},
   "source": [
    "**Present and explain the key equations used in the lesson**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e1f3e3e",
   "metadata": {},
   "source": [
    "## Key Equations\n",
    "\n",
    "The vectorized forward propagation equations for a neural network processing all m training examples simultaneously are:\n",
    "\n",
    "**Layer 1 (Hidden Layer)**:\n",
    "$$Z^{[1]} = W^{[1]} X + b^{[1]}$$\n",
    "$$A^{[1]} = \\sigma(Z^{[1]})$$\n",
    "\n",
    "**Layer 2 (Output Layer)**:\n",
    "$$Z^{[2]} = W^{[2]} A^{[1]} + b^{[2]}$$\n",
    "$$A^{[2]} = \\sigma(Z^{[2]})$$\n",
    "\n",
    "Where:\n",
    "- $X$ is the input matrix with shape (n_x, m), where n_x is the number of input features and m is the number of training examples\n",
    "- $W^{[l]}$ is the weight matrix for layer l\n",
    "- $b^{[l]}$ is the bias vector for layer l (broadcasted across all examples)\n",
    "- $Z^{[l]}$ is the pre-activation matrix for layer l\n",
    "- $A^{[l]}$ is the activation matrix for layer l\n",
    "- $\\sigma$ is the activation function (applied element-wise)\n",
    "\n",
    "Each column in these matrices corresponds to one training example, and each row corresponds to a node or feature."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "715982b6",
   "metadata": {},
   "source": [
    "**Implement code primitive: Implement vectorized forward propagation by replacing single-example equations with matrix operations that process all m training examples simultaneously**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ac198b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def vectorized_forward_propagation(X, W1, b1, W2, b2, activation_fn):\n",
    "    \"\"\"\n",
    "    Vectorized forward propagation for all m training examples.\n",
    "    \n",
    "    X: input matrix of shape (n_x, m)\n",
    "    W1, b1: weights and bias for layer 1\n",
    "    W2, b2: weights and bias for layer 2\n",
    "    activation_fn: activation function\n",
    "    \"\"\"\n",
    "    Z1 = np.dot(W1, X) + b1\n",
    "    A1 = activation_fn(Z1)\n",
    "    \n",
    "    Z2 = np.dot(W2, A1) + b2\n",
    "    A2 = activation_fn(Z2)\n",
    "    \n",
    "    return Z1, A1, Z2, A2\n",
    "\n",
    "# Example usage\n",
    "np.random.seed(42)\n",
    "m = 5  # number of training examples\n",
    "n_x = 3  # number of input features\n",
    "n_h = 4  # number of hidden units\n",
    "n_y = 2  # number of output units\n",
    "\n",
    "X = np.random.randn(n_x, m)\n",
    "W1 = np.random.randn(n_h, n_x)\n",
    "b1 = np.zeros((n_h, 1))\n",
    "W2 = np.random.randn(n_y, n_h)\n",
    "b2 = np.zeros((n_y, 1))\n",
    "\n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "Z1, A1, Z2, A2 = vectorized_forward_propagation(X, W1, b1, W2, b2, sigmoid)\n",
    "print(f\"X shape: {X.shape}\")\n",
    "print(f\"Z1 shape: {Z1.shape}\")\n",
    "print(f\"A1 shape: {A1.shape}\")\n",
    "print(f\"Z2 shape: {Z2.shape}\")\n",
    "print(f\"A2 shape: {A2.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bbd2b5c",
   "metadata": {},
   "source": [
    "**Implement code primitive: Convert unvectorized for-loop implementation (iterating over each training example) into vectorized matrix operations**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "639b2b73",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def unvectorized_forward_propagation(X, W1, b1, W2, b2, activation_fn):\n",
    "    \"\"\"\n",
    "    Unvectorized forward propagation using a for-loop over training examples.\n",
    "    \"\"\"\n",
    "    m = X.shape[1]\n",
    "    A1_list = []\n",
    "    A2_list = []\n",
    "    \n",
    "    for i in range(m):\n",
    "        x_i = X[:, i:i+1]\n",
    "        z1_i = np.dot(W1, x_i) + b1\n",
    "        a1_i = activation_fn(z1_i)\n",
    "        z2_i = np.dot(W2, a1_i) + b2\n",
    "        a2_i = activation_fn(z2_i)\n",
    "        A1_list.append(a1_i)\n",
    "        A2_list.append(a2_i)\n",
    "    \n",
    "    A1 = np.hstack(A1_list)\n",
    "    A2 = np.hstack(A2_list)\n",
    "    return A1, A2\n",
    "\n",
    "def vectorized_forward_propagation(X, W1, b1, W2, b2, activation_fn):\n",
    "    \"\"\"\n",
    "    Vectorized forward propagation processing all m examples simultaneously.\n",
    "    \"\"\"\n",
    "    Z1 = np.dot(W1, X) + b1\n",
    "    A1 = activation_fn(Z1)\n",
    "    Z2 = np.dot(W2, A1) + b2\n",
    "    A2 = activation_fn(Z2)\n",
    "    return A1, A2\n",
    "\n",
    "# Test both implementations\n",
    "np.random.seed(42)\n",
    "m = 5\n",
    "n_x = 3\n",
    "n_h = 4\n",
    "n_y = 2\n",
    "\n",
    "X = np.random.randn(n_x, m)\n",
    "W1 = np.random.randn(n_h, n_x)\n",
    "b1 = np.zeros((n_h, 1))\n",
    "W2 = np.random.randn(n_y, n_h)\n",
    "b2 = np.zeros((n_y, 1))\n",
    "\n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "A1_unvec, A2_unvec = unvectorized_forward_propagation(X, W1, b1, W2, b2, sigmoid)\n",
    "A1_vec, A2_vec = vectorized_forward_propagation(X, W1, b1, W2, b2, sigmoid)\n",
    "\n",
    "print(f\"Unvectorized A2 shape: {A2_unvec.shape}\")\n",
    "print(f\"Vectorized A2 shape: {A2_vec.shape}\")\n",
    "print(f\"Results match: {np.allclose(A2_unvec, A2_vec)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e805c09",
   "metadata": {},
   "source": [
    "**Implement code primitive: Stack individual training example vectors as columns to form input matrix X and activation matrices A**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39975733",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Create individual training examples\n",
    "example_1 = np.array([[1.0], [2.0], [3.0]])\n",
    "example_2 = np.array([[4.0], [5.0], [6.0]])\n",
    "example_3 = np.array([[7.0], [8.0], [9.0]])\n",
    "\n",
    "# Stack examples as columns to form the input matrix X\n",
    "X = np.hstack([example_1, example_2, example_3])\n",
    "print(\"Input matrix X (examples as columns):\")\n",
    "print(X)\n",
    "print(f\"Shape: {X.shape} (3 features, 3 examples)\\n\")\n",
    "\n",
    "# Simulate activation values for hidden layer\n",
    "a1_example_1 = np.array([[0.5], [0.6], [0.7], [0.8]])\n",
    "a1_example_2 = np.array([[0.4], [0.5], [0.6], [0.7]])\n",
    "a1_example_3 = np.array([[0.3], [0.4], [0.5], [0.6]])\n",
    "\n",
    "# Stack activations as columns to form activation matrix A1\n",
    "A1 = np.hstack([a1_example_1, a1_example_2, a1_example_3])\n",
    "print(\"Activation matrix A1 (examples as columns):\")\n",
    "print(A1)\n",
    "print(f\"Shape: {A1.shape} (4 hidden units, 3 examples)\")\n",
    "print(f\"\\nEach column represents one training example\")\n",
    "print(f\"Each row represents one hidden unit\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99782d7b",
   "metadata": {},
   "source": [
    "**Create a Mermaid diagram: Flowchart showing the transformation from unvectorized loop-based computation to vectorized matrix computation**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0288f447",
   "metadata": {},
   "source": [
    "## Transformation from Unvectorized to Vectorized Computation\n",
    "\n",
    "```mermaid\n",
    "graph TD\n",
    "    A[\"Start: m training examples\"] --> B{\"Unvectorized or Vectorized?\"}\n",
    "    \n",
    "    B -->|Unvectorized| C[\"Loop: for i = 1 to m\"]\n",
    "    C --> D[\"Extract example i: x^(i)\"]\n",
    "    D --> E[\"Compute: z^[1](i) = W^[1] x^(i) + b^[1]\"]\n",
    "    E --> F[\"Compute: a^[1](i) = σ(z^[1](i))\"]\n",
    "    F --> G[\"Compute: z^[2](i) = W^[2] a^[1](i) + b^[2]\"]\n",
    "    G --> H[\"Compute: a^[2](i) = σ(z^[2](i))\"]\n",
    "    H --> I[\"Store result for example i\"]\n",
    "    I --> J{\"More examples?\"}\n",
    "    J -->|Yes| D\n",
    "    J -->|No| K[\"Combine all results\"]\n",
    "    \n",
    "    B -->|Vectorized| L[\"Stack all examples as columns: X\"]\n",
    "    L --> M[\"Compute: Z^[1] = W^[1] X + b^[1]\"]\n",
    "    M --> N[\"Compute: A^[1] = σ(Z^[1])\"]\n",
    "    N --> O[\"Compute: Z^[2] = W^[2] A^[1] + b^[2]\"]\n",
    "    O --> P[\"Compute: A^[2] = σ(Z^[2])\"]\n",
    "    P --> Q[\"All results computed in one step\"]\n",
    "    \n",
    "    K --> R[\"End: Forward propagation complete\"]\n",
    "    Q --> R\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d84d3d3",
   "metadata": {},
   "source": [
    "**Create a Mermaid diagram: Diagram illustrating matrix structure where horizontal axis represents training examples and vertical axis represents network nodes/features**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "053e8101",
   "metadata": {},
   "source": [
    "## Matrix Structure: Examples and Nodes\n",
    "\n",
    "```mermaid\n",
    "graph LR\n",
    "    subgraph Input[\"Input Matrix X\"]\n",
    "        direction TB\n",
    "        X1[\"Feature 1\"]\n",
    "        X2[\"Feature 2\"]\n",
    "        X3[\"Feature 3\"]\n",
    "        X1 --> XE1[\"Example 1\"]\n",
    "        X1 --> XE2[\"Example 2\"]\n",
    "        X1 --> XE3[\"Example 3\"]\n",
    "        X2 --> XE1\n",
    "        X2 --> XE2\n",
    "        X2 --> XE3\n",
    "        X3 --> XE1\n",
    "        X3 --> XE2\n",
    "        X3 --> XE3\n",
    "    end\n",
    "    \n",
    "    subgraph Hidden[\"Hidden Layer Matrix A^[1]\"]\n",
    "        direction TB\n",
    "        H1[\"Hidden Unit 1\"]\n",
    "        H2[\"Hidden Unit 2\"]\n",
    "        H3[\"Hidden Unit 3\"]\n",
    "        H4[\"Hidden Unit 4\"]\n",
    "        H1 --> HE1[\"Example 1\"]\n",
    "        H1 --> HE2[\"Example 2\"]\n",
    "        H1 --> HE3[\"Example 3\"]\n",
    "        H2 --> HE1\n",
    "        H2 --> HE2\n",
    "        H2 --> HE3\n",
    "        H3 --> HE1\n",
    "        H3 --> HE2\n",
    "        H3 --> HE3\n",
    "        H4 --> HE1\n",
    "        H4 --> HE2\n",
    "        H4 --> HE3\n",
    "    end\n",
    "    \n",
    "    Input -->|W^[1], b^[1]| Hidden\n",
    "    \n",
    "    style Input fill:#e1f5ff\n",
    "    style Hidden fill:#f3e5f5\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd33cfa8",
   "metadata": {},
   "source": [
    "## Lesson 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40641155",
   "metadata": {},
   "source": [
    "**Explain the core concepts of the lesson**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9761475",
   "metadata": {},
   "source": [
    "## Core Concepts\n",
    "\n",
    "Vectorized forward propagation extends the forward propagation computation from processing a single training example to processing all $m$ training examples simultaneously using matrix operations.\n",
    "\n",
    "**Key Ideas:**\n",
    "\n",
    "1. **Stacking Training Examples**: Instead of processing each training example $x^{(i)}$ individually, we stack all examples horizontally into a matrix $X$ of shape $(n_x, m)$, where $n_x$ is the input dimension and $m$ is the number of training examples.\n",
    "\n",
    "2. **Matrix Multiplication Vectorization**: When we multiply the weight matrix $W$ by the stacked input matrix $X$, we compute all outputs simultaneously: $Z = WX$. Each column of $Z$ corresponds to the output for one training example.\n",
    "\n",
    "3. **Broadcasting Bias Addition**: Python broadcasting automatically adds the bias vector $b$ to each column of $Z$ without explicit replication, making the computation efficient.\n",
    "\n",
    "4. **Layer-wise Computation Symmetry**: Each layer performs the same computation pattern—multiply by weights, add bias, apply activation—regardless of how many examples are processed.\n",
    "\n",
    "5. **Column Correspondence**: The column structure is preserved through the computation: the $i$-th column of $X$ produces the $i$-th column of $Z$, which becomes the $i$-th column of $A$ after applying the activation function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ecb8633",
   "metadata": {},
   "source": [
    "**Explain intuitions and mental models for the lesson**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57187a65",
   "metadata": {},
   "source": [
    "## Intuitions and Mental Models\n",
    "\n",
    "**Intuition 1: Column Stacking Preserves Structure**\n",
    "\n",
    "When you stack training examples as columns in a matrix, the corresponding outputs also stack as columns in the result matrix. This is because matrix multiplication naturally produces outputs in the same column positions as the inputs. If you think of each column as a \"slot\" for one example, matrix multiplication fills all slots simultaneously while preserving their positions.\n",
    "\n",
    "**Intuition 2: Repeated Layer Computation**\n",
    "\n",
    "Each layer of a neural network performs the same type of computation repeatedly: multiply by weights, add bias, apply activation. Deeper networks just repeat this pattern more times. Whether you're processing one example or a thousand, each layer does the same operation—the only difference is the shape of the matrices involved.\n",
    "\n",
    "**Intuition 3: Broadcasting as Implicit Replication**\n",
    "\n",
    "Python broadcasting automatically handles adding the bias vector to each column of the result matrix, so you don't need to explicitly replicate it. Instead of manually copying $b$ to match the shape of $Z$, NumPy automatically \"stretches\" $b$ across all columns during the addition operation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf1a9d60",
   "metadata": {},
   "source": [
    "**Present and explain the key equations used in the lesson**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fa3a14f",
   "metadata": {},
   "source": [
    "## Key Equations\n",
    "\n",
    "**Single Example Forward Propagation:**\n",
    "\n",
    "For a single training example $x^{(i)}$:\n",
    "\n",
    "$$z^{[l](i)} = W^{[l]} x^{(i)} + b^{[l]}$$\n",
    "\n",
    "$$a^{[l](i)} = \\sigma(z^{[l](i)})$$\n",
    "\n",
    "where $x^{(i)} = a^{[0](i)}$ is the input for example $i$.\n",
    "\n",
    "**Vectorized Forward Propagation:**\n",
    "\n",
    "For all $m$ training examples stacked as columns in matrix $X$:\n",
    "\n",
    "$$Z^{[l]} = W^{[l]} X + b^{[l]}$$\n",
    "\n",
    "$$A^{[l]} = \\sigma(Z^{[l]})$$\n",
    "\n",
    "where:\n",
    "- $X$ has shape $(n_x, m)$ — each column is one training example\n",
    "- $W^{[l]}$ has shape $(n^{[l]}, n^{[l-1]})$ — the weight matrix for layer $l$\n",
    "- $Z^{[l]}$ has shape $(n^{[l]}, m)$ — each column is the pre-activation output for one example\n",
    "- $b^{[l]}$ has shape $(n^{[l]}, 1)$ — broadcasted to all $m$ columns\n",
    "- $A^{[l]}$ has shape $(n^{[l]}, m)$ — each column is the activation output for one example\n",
    "\n",
    "The key insight is that $Z^{[l]} = W^{[l]} A^{[l-1]} + b^{[l]}$ applies the same transformation to all examples in parallel."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "069b5ddd",
   "metadata": {},
   "source": [
    "**Implement code primitive: Demonstrate matrix multiplication of weight matrix W with stacked input matrix X to produce stacked output matrix Z**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3fdc2b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Example: 3 training examples, 2 input features, 4 output units\n",
    "m = 3  # number of training examples\n",
    "n_x = 2  # input dimension\n",
    "n_l = 4  # output dimension (number of units in layer l)\n",
    "\n",
    "# Weight matrix W^[l] with shape (n_l, n_x)\n",
    "W = np.array([\n",
    "    [0.1, 0.2],\n",
    "    [0.3, 0.4],\n",
    "    [0.5, 0.6],\n",
    "    [0.7, 0.8]\n",
    "])\n",
    "\n",
    "# Stacked input matrix X with shape (n_x, m)\n",
    "# Each column is one training example\n",
    "X = np.array([\n",
    "    [1.0, 2.0, 3.0],\n",
    "    [4.0, 5.0, 6.0]\n",
    "])\n",
    "\n",
    "# Matrix multiplication: Z = W @ X\n",
    "Z = W @ X\n",
    "\n",
    "print(\"Weight matrix W shape:\", W.shape)\n",
    "print(\"Input matrix X shape:\", X.shape)\n",
    "print(\"Output matrix Z shape:\", Z.shape)\n",
    "print(\"\\nOutput matrix Z:\")\n",
    "print(Z)\n",
    "print(\"\\nEach column of Z is the output for one training example\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89d961f0",
   "metadata": {},
   "source": [
    "**Implement code primitive: Show how Python broadcasting adds bias vector b to each column of the Z matrix simultaneously**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8a56563",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Continue from previous example\n",
    "m = 3\n",
    "n_l = 4\n",
    "\n",
    "# Z from previous computation\n",
    "Z = np.array([\n",
    "    [0.9, 1.2, 1.5],\n",
    "    [1.5, 2.1, 2.7],\n",
    "    [2.1, 3.0, 3.9],\n",
    "    [2.7, 3.9, 5.1]\n",
    "])\n",
    "\n",
    "# Bias vector b^[l] with shape (n_l, 1)\n",
    "b = np.array([\n",
    "    [0.1],\n",
    "    [0.2],\n",
    "    [0.3],\n",
    "    [0.4]\n",
    "])\n",
    "\n",
    "# Broadcasting: b is automatically added to each column of Z\n",
    "Z_with_bias = Z + b\n",
    "\n",
    "print(\"Z shape:\", Z.shape)\n",
    "print(\"b shape:\", b.shape)\n",
    "print(\"Z + b shape:\", Z_with_bias.shape)\n",
    "print(\"\\nZ + b (bias added to each column):\")\n",
    "print(Z_with_bias)\n",
    "print(\"\\nNotice: b is added to all\", m, \"columns without explicit replication\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46b0469a",
   "metadata": {},
   "source": [
    "**Implement code primitive: Implement vectorized forward propagation loop that processes all m training examples in one matrix operation instead of iterating through individual examples**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fd752e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "# Network parameters\n",
    "m = 3  # number of training examples\n",
    "n_x = 2  # input dimension\n",
    "n_1 = 4  # hidden layer dimension\n",
    "n_2 = 1  # output layer dimension\n",
    "\n",
    "# Initialize weights and biases\n",
    "W1 = np.random.randn(n_1, n_x) * 0.01\n",
    "b1 = np.zeros((n_1, 1))\n",
    "W2 = np.random.randn(n_2, n_1) * 0.01\n",
    "b2 = np.zeros((n_2, 1))\n",
    "\n",
    "# Training data: X has shape (n_x, m)\n",
    "X = np.array([\n",
    "    [1.0, 2.0, 3.0],\n",
    "    [4.0, 5.0, 6.0]\n",
    "])\n",
    "\n",
    "# Vectorized forward propagation\n",
    "A0 = X\n",
    "Z1 = W1 @ A0 + b1\n",
    "A1 = sigmoid(Z1)\n",
    "Z2 = W2 @ A1 + b2\n",
    "A2 = sigmoid(Z2)\n",
    "\n",
    "print(\"Input A0 shape:\", A0.shape)\n",
    "print(\"Hidden layer Z1 shape:\", Z1.shape)\n",
    "print(\"Hidden layer A1 shape:\", A1.shape)\n",
    "print(\"Output Z2 shape:\", Z2.shape)\n",
    "print(\"Output A2 shape:\", A2.shape)\n",
    "print(\"\\nAll\", m, \"training examples processed in one forward pass\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06d942a2",
   "metadata": {},
   "source": [
    "**Create a Mermaid diagram: Flowchart showing the progression from single-example forward propagation (z = Wx + b for each i) to vectorized forward propagation (Z = WX + b for all examples simultaneously)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5fc6330",
   "metadata": {},
   "source": [
    "```mermaid\n",
    "graph TD\n",
    "    A[\"Single Example Approach\"] --> B[\"For each training example i\"]\n",
    "    B --> C[\"Compute z^[l](i) = W^[l] x^(i) + b^[l]\"]\n",
    "    C --> D[\"Compute a^[l](i) = σ(z^[l](i))\"]\n",
    "    D --> E[\"Repeat for all m examples\"]\n",
    "    E --> F[\"Result: m separate computations\"]\n",
    "    \n",
    "    G[\"Vectorized Approach\"] --> H[\"Stack all examples as columns in X\"]\n",
    "    H --> I[\"Compute Z^[l] = W^[l] X + b^[l]\"]\n",
    "    I --> J[\"Compute A^[l] = σ(Z^[l])\"]\n",
    "    J --> K[\"Result: All m examples in one operation\"]\n",
    "    \n",
    "    F --> L{\"Efficiency Comparison\"}\n",
    "    K --> L\n",
    "    L --> M[\"Vectorized is much faster\"]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43c59008",
   "metadata": {},
   "source": [
    "**Create a Mermaid diagram: Diagram illustrating how stacking training examples horizontally in matrix X results in corresponding outputs stacked horizontally in matrix Z, with column correspondence highlighted**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15c63115",
   "metadata": {},
   "source": [
    "```mermaid\n",
    "graph LR\n",
    "    subgraph Input[\"Input Matrix X (n_x × m)\"]\n",
    "        X1[\"x^(1)\"] \n",
    "        X2[\"x^(2)\"]\n",
    "        X3[\"x^(3)\"]\n",
    "    end\n",
    "    \n",
    "    subgraph Weights[\"Weight Matrix W (n_l × n_x)\"]\n",
    "        W[\"W^[l]\"]\n",
    "    end\n",
    "    \n",
    "    subgraph Output[\"Output Matrix Z (n_l × m)\"]\n",
    "        Z1[\"z^[l](1)\"]\n",
    "        Z2[\"z^[l](2)\"]\n",
    "        Z3[\"z^[l](3)\"]\n",
    "    end\n",
    "    \n",
    "    Input -->|Column 1| Z1\n",
    "    Input -->|Column 2| Z2\n",
    "    Input -->|Column 3| Z3\n",
    "    Weights -->|Applied to all| Z1\n",
    "    Weights -->|Applied to all| Z2\n",
    "    Weights -->|Applied to all| Z3\n",
    "    \n",
    "    style X1 fill:#e1f5ff\n",
    "    style X2 fill:#e1f5ff\n",
    "    style X3 fill:#e1f5ff\n",
    "    style Z1 fill:#fff3e0\n",
    "    style Z2 fill:#fff3e0\n",
    "    style Z3 fill:#fff3e0\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80ef465f",
   "metadata": {},
   "source": [
    "## Lesson 6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7b5a2d4",
   "metadata": {},
   "source": [
    "**Explain the core concepts of the lesson**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30d0e2ec",
   "metadata": {},
   "source": [
    "## Core Concepts\n",
    "\n",
    "This lesson explores why **non-linear activation functions** are essential in neural networks.\n",
    "\n",
    "**Linear Activation Functions** (also called identity activation) simply pass the input through unchanged: $a = z$. When used in hidden layers, they provide no additional computational power.\n",
    "\n",
    "**Non-linear Activation Functions** (such as ReLU, Tanh, or Sigmoid) apply non-linear transformations to the output of each neuron. These are crucial for enabling neural networks to learn complex, non-linear relationships in data.\n",
    "\n",
    "**Hidden Layer Expressiveness** refers to the ability of hidden layers to compute and represent increasingly complex functions. Without non-linearity, adding more layers does not increase this expressiveness.\n",
    "\n",
    "**Composition of Linear Functions** is the mathematical principle that explains why linear activation functions are limited: when you compose (chain) multiple linear functions together, the result is always another linear function. This means a deep network with only linear activations is mathematically equivalent to a single linear model.\n",
    "\n",
    "**Regression Output Layer** is a special case where linear activation may be appropriate—when predicting continuous real-valued targets (like housing prices), the output layer often uses linear activation to allow any real-valued output."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a4cb504",
   "metadata": {},
   "source": [
    "**Explain intuitions and mental models for the lesson**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "713d7ab3",
   "metadata": {},
   "source": [
    "## Intuitions and Mental Models\n",
    "\n",
    "**The Composition Problem**: Imagine stacking multiple transparent sheets, each with a linear transformation drawn on it. No matter how many sheets you stack, the combined effect is still just a linear transformation. You cannot create a curve or bend in the data by composing straight lines. This is exactly what happens in a neural network with only linear activations—no matter how deep it is, it can only learn linear relationships.\n",
    "\n",
    "**Why Non-linearity Matters**: Non-linear activation functions are like adding a \"bend\" or \"twist\" at each layer. When you compose a non-linear function with another non-linear function, you can create increasingly complex, curved decision boundaries and function approximations. This is what gives deep networks their power.\n",
    "\n",
    "**Depth Without Non-linearity is Useless**: A 10-layer neural network with linear activations is no more powerful than a single-layer linear model. The depth provides no benefit. But a 10-layer network with non-linear activations can represent vastly more complex functions than a shallow network. This is why non-linearity is the key ingredient that makes deep learning work.\n",
    "\n",
    "**Linear Output Layers**: In regression tasks, we often want the network to output any real number (positive, negative, or zero). A linear activation in the output layer allows this. However, hidden layers still need non-linearity to learn the complex patterns that feed into this final linear transformation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bfb2837",
   "metadata": {},
   "source": [
    "**Present and explain the key equations used in the lesson**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37d9d859",
   "metadata": {},
   "source": [
    "## Key Equations\n",
    "\n",
    "**Layer 1 with Linear Activation**:\n",
    "$$a^{[1]} = Z^{[1]} = W^{[1]}x + b^{[1]}$$\n",
    "\n",
    "Here, $W^{[1]}$ is the weight matrix, $x$ is the input, and $b^{[1]}$ is the bias. The activation $a^{[1]}$ is identical to the pre-activation $Z^{[1]}$ because we use linear activation.\n",
    "\n",
    "**Layer 2 with Linear Activation**:\n",
    "$$a^{[2]} = Z^{[2]} = W^{[2]}a^{[1]} + b^{[2]}$$\n",
    "\n",
    "The output of layer 2 depends on the activation from layer 1.\n",
    "\n",
    "**Composition Simplification**:\n",
    "$$a^{[2]} = W^{[2]}(W^{[1]}x + b^{[1]}) + b^{[2]} = (W^{[2]}W^{[1]})x + (W^{[2]}b^{[1]} + b^{[2]}) = W'x + b'$$\n",
    "\n",
    "This is the crucial insight: when we substitute the expression for $a^{[1]}$ into the equation for $a^{[2]}$, we can algebraically simplify it to a single linear transformation with combined weights $W' = W^{[2]}W^{[1]}$ and combined bias $b' = W^{[2]}b^{[1]} + b^{[2]}$. No matter how many linear layers we stack, the result is always equivalent to a single linear transformation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f7e2e2e",
   "metadata": {},
   "source": [
    "**Implement code primitive: Demonstrate the mathematical composition of two linear transformations to show that the result is a single linear function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63f02540",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Define two linear transformations\n",
    "W1 = np.array([[2, 1], [1, 3]])\n",
    "b1 = np.array([0.5, -0.5])\n",
    "\n",
    "W2 = np.array([[1, 2], [3, 1]])\n",
    "b2 = np.array([1.0, 0.5])\n",
    "\n",
    "# Input\n",
    "x = np.array([1, 2])\n",
    "\n",
    "# Method 1: Apply transformations sequentially\n",
    "a1 = W1 @ x + b1\n",
    "a2 = W2 @ a1 + b2\n",
    "\n",
    "print(\"Sequential application:\")\n",
    "print(f\"a1 = {a1}\")\n",
    "print(f\"a2 = {a2}\")\n",
    "\n",
    "# Method 2: Compose into a single transformation\n",
    "W_combined = W2 @ W1\n",
    "b_combined = W2 @ b1 + b2\n",
    "\n",
    "a2_direct = W_combined @ x + b_combined\n",
    "\n",
    "print(\"\\nDirect composition:\")\n",
    "print(f\"W' = W2 @ W1 = \\n{W_combined}\")\n",
    "print(f\"b' = W2 @ b1 + b2 = {b_combined}\")\n",
    "print(f\"a2 (direct) = {a2_direct}\")\n",
    "\n",
    "print(\"\\nAre they equal?\", np.allclose(a2, a2_direct))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3d68de5",
   "metadata": {},
   "source": [
    "**Implement code primitive: Implement a simple neural network with linear activation functions and show that it produces identical output to logistic regression**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ee581d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Generate simple binary classification data\n",
    "np.random.seed(42)\n",
    "X = np.random.randn(100, 2)\n",
    "y = (X[:, 0] + X[:, 1] > 0).astype(int)\n",
    "\n",
    "# Logistic regression (single linear model)\n",
    "lr = LogisticRegression()\n",
    "lr.fit(X, y)\n",
    "lr_predictions = lr.predict_proba(X)[:, 1]\n",
    "\n",
    "# Neural network with linear hidden layer and sigmoid output\n",
    "class LinearNetworkClassifier:\n",
    "    def __init__(self, input_dim, hidden_dim):\n",
    "        self.W1 = np.random.randn(input_dim, hidden_dim) * 0.01\n",
    "        self.b1 = np.zeros(hidden_dim)\n",
    "        self.W2 = np.random.randn(hidden_dim, 1) * 0.01\n",
    "        self.b2 = np.zeros(1)\n",
    "    \n",
    "    def forward(self, X):\n",
    "        # Linear hidden layer\n",
    "        Z1 = X @ self.W1 + self.b1\n",
    "        A1 = Z1  # Linear activation\n",
    "        # Output layer with sigmoid\n",
    "        Z2 = A1 @ self.W2 + self.b2\n",
    "        A2 = 1 / (1 + np.exp(-Z2))\n",
    "        return A2.flatten()\n",
    "\n",
    "# Create network with same weights as logistic regression\n",
    "net = LinearNetworkClassifier(2, 1)\n",
    "net.W2 = lr.coef_.T\n",
    "net.b2 = lr.intercept_\n",
    "net.W1 = np.eye(2)  # Identity mapping\n",
    "net.b1 = np.zeros(2)\n",
    "\n",
    "net_predictions = net.forward(X)\n",
    "\n",
    "print(\"Logistic Regression predictions (first 5):\", lr_predictions[:5])\n",
    "print(\"Linear Network predictions (first 5):\", net_predictions[:5])\n",
    "print(\"\\nAre they equivalent?\", np.allclose(lr_predictions, net_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a77b2d7",
   "metadata": {},
   "source": [
    "**Implement code primitive: Compare the output of a network with non-linear hidden activations versus linear activations on a non-linear dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38438f81",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Generate non-linear dataset (XOR-like)\n",
    "np.random.seed(42)\n",
    "X = np.random.randn(200, 2)\n",
    "y = ((X[:, 0]**2 + X[:, 1]**2) > 1).astype(int)\n",
    "\n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-np.clip(z, -500, 500)))\n",
    "\n",
    "def relu(z):\n",
    "    return np.maximum(0, z)\n",
    "\n",
    "# Network with linear hidden activation\n",
    "W1_linear = np.random.randn(2, 5) * 0.5\n",
    "b1_linear = np.zeros(5)\n",
    "W2_linear = np.random.randn(5, 1) * 0.5\n",
    "b2_linear = np.zeros(1)\n",
    "\n",
    "Z1_linear = X @ W1_linear + b1_linear\n",
    "A1_linear = Z1_linear  # Linear activation\n",
    "Z2_linear = A1_linear @ W2_linear + b2_linear\n",
    "A2_linear = sigmoid(Z2_linear).flatten()\n",
    "\n",
    "# Network with ReLU hidden activation\n",
    "W1_relu = np.random.randn(2, 5) * 0.5\n",
    "b1_relu = np.zeros(5)\n",
    "W2_relu = np.random.randn(5, 1) * 0.5\n",
    "b2_relu = np.zeros(1)\n",
    "\n",
    "Z1_relu = X @ W1_relu + b1_relu\n",
    "A1_relu = relu(Z1_relu)  # ReLU activation\n",
    "Z2_relu = A1_relu @ W2_relu + b2_relu\n",
    "A2_relu = sigmoid(Z2_relu).flatten()\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy_linear = np.mean((A2_linear > 0.5) == y)\n",
    "accuracy_relu = np.mean((A2_relu > 0.5) == y)\n",
    "\n",
    "print(f\"Linear activation accuracy: {accuracy_linear:.3f}\")\n",
    "print(f\"ReLU activation accuracy: {accuracy_relu:.3f}\")\n",
    "print(f\"\\nDifference: {accuracy_relu - accuracy_linear:.3f}\")\n",
    "print(\"\\nNon-linear activations enable better learning on non-linear data.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8157ce1",
   "metadata": {},
   "source": [
    "**Create a Mermaid diagram: flowchart showing the progression from linear activation (no hidden layer benefit) to non-linear activation (enables deep learning expressiveness)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0f16fee",
   "metadata": {},
   "source": [
    "```mermaid\n",
    "flowchart TD\n",
    "    A[\"Neural Network Architecture\"] --> B{\"Activation Function Type\"}\n",
    "    B -->|\"Linear Activation\" | C[\"Hidden Layer Output: Linear Combination\"]\n",
    "    C --> D[\"Composition of Linear Functions\"]\n",
    "    D --> E[\"Result: Single Linear Model\"]\n",
    "    E --> F[\"No Additional Expressiveness<br/>Depth Provides No Benefit\"]\n",
    "    \n",
    "    B -->|\"Non-linear Activation<br/>ReLU, Tanh, Sigmoid\" | G[\"Hidden Layer Output: Non-linear Transform\"]\n",
    "    G --> H[\"Composition of Non-linear Functions\"]\n",
    "    H --> I[\"Result: Complex Non-linear Function\"]\n",
    "    I --> J[\"Increased Expressiveness<br/>Depth Enables Learning Complex Patterns\"]\n",
    "    \n",
    "    F --> K[\"Conclusion: Non-linearity is Essential\"]\n",
    "    J --> K\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "def57809",
   "metadata": {},
   "source": [
    "**Create a Mermaid diagram: diagram illustrating how composing two linear functions W2(W1*x + b1) + b2 simplifies to a single linear function W'*x + b'**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7854876b",
   "metadata": {},
   "source": [
    "```mermaid\n",
    "graph TD\n",
    "    A[\"Input: x\"] --> B[\"Layer 1: W¹x + b¹\"]\n",
    "    B --> C[\"a¹ = W¹x + b¹<br/>Linear Activation\"]\n",
    "    C --> D[\"Layer 2: W²a¹ + b²\"]\n",
    "    D --> E[\"a² = W²a¹ + b²\"]\n",
    "    \n",
    "    E --> F[\"Substitute a¹\"]\n",
    "    F --> G[\"a² = W²(W¹x + b¹) + b²\"]\n",
    "    G --> H[\"Expand\"]\n",
    "    H --> I[\"a² = W²W¹x + W²b¹ + b²\"]\n",
    "    I --> J[\"Simplify\"]\n",
    "    J --> K[\"a² = W'x + b'<br/>where W' = W²W¹<br/>and b' = W²b¹ + b²\"]\n",
    "    \n",
    "    K --> L[\"Result: Single Linear Transformation\"]\n",
    "    L --> M[\"Equivalent to a 1-Layer Model\"]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8261e48e",
   "metadata": {},
   "source": [
    "## Lesson 7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2310e666",
   "metadata": {},
   "source": [
    "**Explain the core concepts of the lesson**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad7ceded",
   "metadata": {},
   "source": [
    "## Core Concepts\n",
    "\n",
    "A neural network with a single hidden layer consists of four learnable parameters that must be optimized through gradient descent:\n",
    "\n",
    "- **W¹** and **b¹**: Weight matrix and bias vector for the first layer (input to hidden)\n",
    "- **W²** and **b²**: Weight matrix and bias vector for the second layer (hidden to output)\n",
    "\n",
    "The dimensions of these parameters are determined by the network architecture:\n",
    "- W¹ has shape (hidden_size, input_size)\n",
    "- b¹ has shape (hidden_size, 1)\n",
    "- W² has shape (output_size, hidden_size)\n",
    "- b² has shape (output_size, 1)\n",
    "\n",
    "The learning process involves:\n",
    "1. **Forward propagation**: Computing predictions by passing data through the network\n",
    "2. **Cost computation**: Measuring prediction error across all training examples\n",
    "3. **Backpropagation**: Computing gradients of the cost with respect to each parameter\n",
    "4. **Gradient descent**: Updating parameters in the direction that reduces cost\n",
    "\n",
    "This cycle repeats until the parameters converge to values that minimize the cost function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16d1077f",
   "metadata": {},
   "source": [
    "**Explain intuitions and mental models for the lesson**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2350f18f",
   "metadata": {},
   "source": [
    "## Intuitions and Mental Models\n",
    "\n",
    "**Parameter Initialization**: Initializing parameters randomly (not to zero) is crucial. If all parameters start at zero, neurons in the hidden layer will compute identical functions, preventing the network from learning diverse features. Random initialization breaks this symmetry.\n",
    "\n",
    "**Forward vs. Backward Flow**: Forward propagation moves left-to-right through the network, transforming input data into predictions. Backpropagation moves right-to-left, using the chain rule to compute how changes in each parameter affect the overall cost. This is why we compute gradients in reverse order: dZ² → dW², dB² → dA¹ → dZ¹ → dW¹, dB¹.\n",
    "\n",
    "**Vectorization**: Instead of computing gradients for one training example at a time, we process all m examples simultaneously using matrix operations. This makes the code efficient and the notation cleaner—each variable represents a batch of values, not a single value.\n",
    "\n",
    "**Gradient Descent Update**: The update rule is uniform across all parameters: subtract the learning rate times the gradient from the current parameter. This simple rule, applied iteratively, gradually reduces the cost.\n",
    "\n",
    "**Matrix Dimensions**: Maintaining consistent matrix dimensions throughout forward and backward passes is essential. Each operation must respect the shapes of the matrices involved, and bias gradients require special handling with `np.sum(axis=1, keepdims=True)` to preserve dimensions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8a57357",
   "metadata": {},
   "source": [
    "**Present and explain the key equations used in the lesson**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3d2dfd5",
   "metadata": {},
   "source": [
    "## Key Equations\n",
    "\n",
    "**Forward Propagation:**\n",
    "\n",
    "$$Z^{[1]} = W^{[1]}X + b^{[1]}$$\n",
    "\n",
    "$$A^{[1]} = g^{[1]}(Z^{[1]})$$\n",
    "\n",
    "$$Z^{[2]} = W^{[2]}A^{[1]} + b^{[2]}$$\n",
    "\n",
    "$$A^{[2]} = g^{[2]}(Z^{[2]})$$\n",
    "\n",
    "where $g^{[1]}$ and $g^{[2]}$ are activation functions (e.g., ReLU for hidden layer, sigmoid for output layer in binary classification).\n",
    "\n",
    "**Cost Function:**\n",
    "\n",
    "$$J = \\frac{1}{m}\\sum_{i=1}^{m}L(\\hat{y}^{(i)}, y^{(i)})$$\n",
    "\n",
    "where $m$ is the number of training examples and $L$ is the loss function.\n",
    "\n",
    "**Backpropagation:**\n",
    "\n",
    "$$dZ^{[2]} = A^{[2]} - Y$$\n",
    "\n",
    "$$dW^{[2]} = \\frac{1}{m}dZ^{[2]}A^{[1]T}$$\n",
    "\n",
    "$$db^{[2]} = \\frac{1}{m}\\text{np.sum}(dZ^{[2]}, \\text{axis}=1, \\text{keepdims}=\\text{True})$$\n",
    "\n",
    "$$dA^{[1]} = W^{[2]T}dZ^{[2]}$$\n",
    "\n",
    "$$dZ^{[1]} = dA^{[1]} * g^{[1]\\prime}(Z^{[1]})$$\n",
    "\n",
    "$$dW^{[1]} = \\frac{1}{m}dZ^{[1]}X^{T}$$\n",
    "\n",
    "$$db^{[1]} = \\frac{1}{m}\\text{np.sum}(dZ^{[1]}, \\text{axis}=1, \\text{keepdims}=\\text{True})$$\n",
    "\n",
    "**Gradient Descent Update:**\n",
    "\n",
    "$$W := W - \\alpha dW$$\n",
    "\n",
    "$$b := b - \\alpha db$$\n",
    "\n",
    "where $\\alpha$ is the learning rate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc0809a3",
   "metadata": {},
   "source": [
    "**Implement code primitive: Initialize parameters W1, B1, W2, B2 randomly (not to zeros) with appropriate dimensions based on layer sizes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34f78a1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def initialize_parameters(input_size, hidden_size, output_size):\n",
    "    W1 = np.random.randn(hidden_size, input_size) * 0.01\n",
    "    b1 = np.zeros((hidden_size, 1))\n",
    "    W2 = np.random.randn(output_size, hidden_size) * 0.01\n",
    "    b2 = np.zeros((output_size, 1))\n",
    "    return W1, b1, W2, b2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c532b12c",
   "metadata": {},
   "source": [
    "**Implement code primitive: Implement forward propagation: compute Z1, A1, Z2, A2 using matrix operations and activation functions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f36b3076",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "def relu(z):\n",
    "    return np.maximum(0, z)\n",
    "\n",
    "def forward_propagation(X, W1, b1, W2, b2):\n",
    "    Z1 = np.dot(W1, X) + b1\n",
    "    A1 = relu(Z1)\n",
    "    Z2 = np.dot(W2, A1) + b2\n",
    "    A2 = sigmoid(Z2)\n",
    "    return Z1, A1, Z2, A2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0718bd74",
   "metadata": {},
   "source": [
    "**Implement code primitive: Compute cost function as average loss across all training examples for binary classification**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80dd7c9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(A2, Y):\n",
    "    m = Y.shape[1]\n",
    "    loss = -np.mean(Y * np.log(A2 + 1e-8) + (1 - Y) * np.log(1 - A2 + 1e-8))\n",
    "    cost = loss\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "022d9e0f",
   "metadata": {},
   "source": [
    "**Implement code primitive: Implement backpropagation: compute dZ2, dW2, dB2 using vectorized operations**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a680a320",
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_propagation_layer2(A2, Y, A1, W2):\n",
    "    m = Y.shape[1]\n",
    "    dZ2 = A2 - Y\n",
    "    dW2 = np.dot(dZ2, A1.T) / m\n",
    "    dB2 = np.sum(dZ2, axis=1, keepdims=True) / m\n",
    "    return dZ2, dW2, dB2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fdf7bd6",
   "metadata": {},
   "source": [
    "**Implement code primitive: Compute dA1 as transpose of W2 times dZ2 (chain rule through second layer)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61ef74c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_dA1(dZ2, W2):\n",
    "    dA1 = np.dot(W2.T, dZ2)\n",
    "    return dA1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b5899fe",
   "metadata": {},
   "source": [
    "**Implement code primitive: Compute dZ1 as element-wise product of dA1 and derivative of activation function applied to Z1**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "975d2a3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu_derivative(Z1):\n",
    "    return (Z1 > 0).astype(float)\n",
    "\n",
    "def compute_dZ1(dA1, Z1):\n",
    "    dZ1 = dA1 * relu_derivative(Z1)\n",
    "    return dZ1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f3a856a",
   "metadata": {},
   "source": [
    "**Implement code primitive: Compute dW1 and dB1 using vectorized matrix operations with proper dimension handling**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d6da342",
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_propagation_layer1(dZ1, X):\n",
    "    m = X.shape[1]\n",
    "    dW1 = np.dot(dZ1, X.T) / m\n",
    "    dB1 = np.sum(dZ1, axis=1, keepdims=True) / m\n",
    "    return dW1, dB1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20b92ffb",
   "metadata": {},
   "source": [
    "**Implement code primitive: Use np.sum with axis=1 and keepdims=True to maintain correct matrix dimensions for bias gradients**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f018ffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_bias_gradients(dZ, m):\n",
    "    dB = np.sum(dZ, axis=1, keepdims=True) / m\n",
    "    return dB"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38472c7c",
   "metadata": {},
   "source": [
    "**Implement code primitive: Implement gradient descent update: subtract learning rate times gradient from each parameter**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f8e9af3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters(W1, b1, W2, b2, dW1, dB1, dW2, dB2, learning_rate):\n",
    "    W1 = W1 - learning_rate * dW1\n",
    "    b1 = b1 - learning_rate * dB1\n",
    "    W2 = W2 - learning_rate * dW2\n",
    "    b2 = b2 - learning_rate * dB2\n",
    "    return W1, b1, W2, b2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92a9508e",
   "metadata": {},
   "source": [
    "**Implement code primitive: Repeat forward-backward-update cycle until convergence**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5e26759",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_neural_network(X, Y, input_size, hidden_size, output_size, learning_rate=0.01, iterations=1000):\n",
    "    W1, b1, W2, b2 = initialize_parameters(input_size, hidden_size, output_size)\n",
    "    \n",
    "    for i in range(iterations):\n",
    "        Z1, A1, Z2, A2 = forward_propagation(X, W1, b1, W2, b2)\n",
    "        cost = compute_cost(A2, Y)\n",
    "        \n",
    "        dZ2, dW2, dB2 = backward_propagation_layer2(A2, Y, A1, W2)\n",
    "        dA1 = compute_dA1(dZ2, W2)\n",
    "        dZ1 = compute_dZ1(dA1, Z1)\n",
    "        dW1, dB1 = backward_propagation_layer1(dZ1, X)\n",
    "        \n",
    "        W1, b1, W2, b2 = update_parameters(W1, b1, W2, b2, dW1, dB1, dW2, dB2, learning_rate)\n",
    "    \n",
    "    return W1, b1, W2, b2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f510a94",
   "metadata": {},
   "source": [
    "**Create a Mermaid diagram: Flowchart showing the iterative gradient descent loop: initialize parameters, forward propagation, compute cost, backpropagation, compute gradients, update parameters, repeat until convergence**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3690facb",
   "metadata": {},
   "source": [
    "```mermaid\n",
    "graph TD\n",
    "    A[Initialize Parameters] --> B[Forward Propagation]\n",
    "    B --> C[Compute Cost]\n",
    "    C --> D[Backpropagation]\n",
    "    D --> E[Compute Gradients]\n",
    "    E --> F[Update Parameters]\n",
    "    F --> G{Converged?}\n",
    "    G -->|No| B\n",
    "    G -->|Yes| H[Training Complete]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "188b3a8d",
   "metadata": {},
   "source": [
    "**Create a Mermaid diagram: Diagram showing forward propagation flow through layers: input X → Z1 → A1 → Z2 → A2 with parameter matrices W1, B1, W2, B2 labeled at each step**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e64134d",
   "metadata": {},
   "source": [
    "```mermaid\n",
    "graph LR\n",
    "    X[\"Input X\"] -->|W¹, b¹| Z1[\"Z¹\"]\n",
    "    Z1 -->|ReLU| A1[\"A¹\"]\n",
    "    A1 -->|W², b²| Z2[\"Z²\"]\n",
    "    Z2 -->|Sigmoid| A2[\"A² (Output)\"]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "253bba43",
   "metadata": {},
   "source": [
    "**Create a Mermaid diagram: Diagram showing backpropagation flow: cost J → dZ2 → dW2, dB2 and dA1 → dZ1 → dW1, dB1 with chain rule connections illustrated**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f1fe83e",
   "metadata": {},
   "source": [
    "```mermaid\n",
    "graph TD\n",
    "    J[\"Cost J\"] --> dZ2[\"dZ²\"]\n",
    "    dZ2 --> dW2[\"dW²\"]\n",
    "    dZ2 --> dB2[\"dB²\"]\n",
    "    dZ2 --> dA1[\"dA¹\"]\n",
    "    dA1 --> dZ1[\"dZ¹\"]\n",
    "    dZ1 --> dW1[\"dW¹\"]\n",
    "    dZ1 --> dB1[\"dB¹\"]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f759be5e",
   "metadata": {},
   "source": [
    "## Lesson 8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "905f4c8b",
   "metadata": {},
   "source": [
    "**Explain the core concepts of the lesson**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d0836f6",
   "metadata": {},
   "source": [
    "## Core Concepts of Backpropagation\n",
    "\n",
    "Backpropagation is the fundamental algorithm for training neural networks. It computes gradients of the loss function with respect to all parameters by working backward through the computation graph.\n",
    "\n",
    "**Key concepts:**\n",
    "\n",
    "1. **Computation Graph**: A directed acyclic graph representing how inputs flow through operations to produce the loss. Each node represents a variable, and edges represent operations.\n",
    "\n",
    "2. **Chain Rule Application**: Backpropagation applies the chain rule of calculus to compute derivatives. For a composite function, the derivative with respect to an input is the product of derivatives along the path.\n",
    "\n",
    "3. **Gradient Computation**: At each step backward, we compute the gradient of the loss with respect to intermediate variables (activations, pre-activations, weights, biases).\n",
    "\n",
    "4. **Vectorized Backpropagation**: Instead of computing gradients for one training example at a time, we stack examples into matrices and apply the same derivative equations to all examples simultaneously.\n",
    "\n",
    "5. **Matrix Dimensions Matching**: In vectorized implementations, gradient matrices must have the same dimensions as their corresponding parameter matrices. This constraint helps verify correctness.\n",
    "\n",
    "6. **Weight Initialization**: Neural network weights must be initialized randomly (not to zero) to break symmetry and enable the network to learn diverse features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c70a07cb",
   "metadata": {},
   "source": [
    "**Explain intuitions and mental models for the lesson**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90a0c11f",
   "metadata": {},
   "source": [
    "## Intuitions and Mental Models\n",
    "\n",
    "**Backward Flow of Information**: Imagine the loss as a scalar value at the end of the computation graph. Backpropagation traces backward from this loss, computing how much each parameter contributed to it. This is like asking: \"If I change this weight slightly, how much does the loss change?\"\n",
    "\n",
    "**Chain Rule as Path Multiplication**: When computing the gradient of the loss with respect to a weight deep in the network, you multiply the gradients along the path from the loss back to that weight. Each multiplication represents applying the chain rule at one operation.\n",
    "\n",
    "**Stacking Examples for Efficiency**: When you have multiple training examples, instead of running backpropagation separately for each one, you can stack them as columns in a matrix. The same derivative equations apply to the entire matrix at once, computing gradients for all examples in parallel.\n",
    "\n",
    "**Dimension Checking as Error Detection**: By tracking matrix dimensions through backpropagation, you can catch bugs early. If a gradient matrix doesn't match the shape of its corresponding parameter, something is wrong.\n",
    "\n",
    "**Symmetry Breaking**: If all weights start at zero, all neurons in a layer compute identical functions. Random initialization ensures different neurons learn different features, allowing the network to represent complex patterns."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0b33241",
   "metadata": {},
   "source": [
    "**Present and explain the key equations used in the lesson**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28a80903",
   "metadata": {},
   "source": [
    "## Key Equations\n",
    "\n",
    "**Loss Function** (binary cross-entropy):\n",
    "$$L(a, y) = -y \\log(a) - (1-y) \\log(1-a)$$\n",
    "\n",
    "**Gradient of Loss with Respect to Activation**:\n",
    "$$\\frac{dL}{da} = -\\frac{y}{a} + \\frac{1-y}{1-a}$$\n",
    "\n",
    "**Simplified Gradient for Logistic Regression**:\n",
    "$$dz = a - y$$\n",
    "\n",
    "This elegant result comes from combining the loss gradient with the sigmoid derivative.\n",
    "\n",
    "**General Backpropagation Through Activation**:\n",
    "$$dz = da \\cdot g'(z)$$\n",
    "\n",
    "where $g'(z)$ is the derivative of the activation function.\n",
    "\n",
    "**Vectorized Gradient for Layer 2 Weights**:\n",
    "$$dW^{[2]} = \\frac{1}{m} dZ^{[2]} A^{[1]T}$$\n",
    "\n",
    "**Vectorized Gradient for Layer 2 Bias**:\n",
    "$$db^{[2]} = \\frac{1}{m} \\sum dZ^{[2]}$$\n",
    "\n",
    "**Backpropagation to Previous Layer**:\n",
    "$$dZ^{[1]} = W^{[2]T} dZ^{[2]} * g'^{[1]}(Z^{[1]})$$\n",
    "\n",
    "where $*$ denotes element-wise multiplication.\n",
    "\n",
    "**Vectorized Gradient for Layer 1 Weights**:\n",
    "$$dW^{[1]} = \\frac{1}{m} dZ^{[1]} X^T$$\n",
    "\n",
    "In these equations, $m$ is the number of training examples, and the superscript denotes the layer number."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99feaacf",
   "metadata": {},
   "source": [
    "**Implement code primitive: Implement single-example backpropagation by computing da, dz, dw, db in reverse order through the computation graph**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64ba5d2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def single_example_backprop(x, y, z, a, w, b):\n",
    "    \"\"\"\n",
    "    Compute gradients for a single training example.\n",
    "    \n",
    "    Args:\n",
    "        x: input feature\n",
    "        y: true label\n",
    "        z: pre-activation (w*x + b)\n",
    "        a: activation (sigmoid(z))\n",
    "        w: weight\n",
    "        b: bias\n",
    "    \n",
    "    Returns:\n",
    "        da, dz, dw, db: gradients in reverse order\n",
    "    \"\"\"\n",
    "    # Gradient of loss with respect to activation\n",
    "    da = -y/a + (1-y)/(1-a)\n",
    "    \n",
    "    # Gradient of activation with respect to pre-activation (sigmoid derivative)\n",
    "    dz = da * a * (1 - a)\n",
    "    \n",
    "    # Gradient of pre-activation with respect to weight\n",
    "    dw = dz * x\n",
    "    \n",
    "    # Gradient of pre-activation with respect to bias\n",
    "    db = dz\n",
    "    \n",
    "    return da, dz, dw, db\n",
    "\n",
    "# Example usage\n",
    "x_example = 0.5\n",
    "y_example = 1\n",
    "z_example = 0.2\n",
    "a_example = 1 / (1 + np.exp(-z_example))  # sigmoid\n",
    "w_example = 0.3\n",
    "b_example = 0.1\n",
    "\n",
    "da, dz, dw, db = single_example_backprop(x_example, y_example, z_example, a_example, w_example, b_example)\n",
    "print(f\"da: {da:.4f}, dz: {dz:.4f}, dw: {dw:.4f}, db: {db:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6482147",
   "metadata": {},
   "source": [
    "**Implement code primitive: Implement vectorized backpropagation by stacking gradients across training examples as matrix columns and applying the same equations**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "831f2e8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def vectorized_backprop(X, Y, Z, A, W, b):\n",
    "    \"\"\"\n",
    "    Compute gradients for all training examples simultaneously.\n",
    "    \n",
    "    Args:\n",
    "        X: input features, shape (n_features, m_examples)\n",
    "        Y: true labels, shape (1, m_examples)\n",
    "        Z: pre-activations, shape (1, m_examples)\n",
    "        A: activations, shape (1, m_examples)\n",
    "        W: weights, shape (1, n_features)\n",
    "        b: bias, scalar\n",
    "    \n",
    "    Returns:\n",
    "        dW, db: gradient matrices\n",
    "    \"\"\"\n",
    "    m = X.shape[1]  # number of training examples\n",
    "    \n",
    "    # Gradient of loss with respect to activation\n",
    "    dA = -Y/A + (1-Y)/(1-A)\n",
    "    \n",
    "    # Gradient of activation with respect to pre-activation\n",
    "    dZ = dA * A * (1 - A)\n",
    "    \n",
    "    # Gradient with respect to weights\n",
    "    dW = (1/m) * np.dot(dZ, X.T)\n",
    "    \n",
    "    # Gradient with respect to bias\n",
    "    db = (1/m) * np.sum(dZ, axis=1, keepdims=True)\n",
    "    \n",
    "    return dW, db\n",
    "\n",
    "# Example usage\n",
    "X = np.array([[0.5, 0.3, 0.8]])\n",
    "Y = np.array([[1, 0, 1]])\n",
    "Z = np.array([[0.2, -0.1, 0.5]])\n",
    "A = 1 / (1 + np.exp(-Z))\n",
    "W = np.array([[0.3]])\n",
    "b = 0.1\n",
    "\n",
    "dW, db = vectorized_backprop(X, Y, Z, A, W, b)\n",
    "print(f\"dW shape: {dW.shape}, db shape: {db.shape}\")\n",
    "print(f\"dW: {dW}, db: {db}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bcb781f",
   "metadata": {},
   "source": [
    "**Implement code primitive: Verify backpropagation implementation by checking that gradient matrices have the same dimensions as their corresponding parameter matrices**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02ace3a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def verify_gradient_dimensions(W1, b1, W2, b2, dW1, db1, dW2, db2):\n",
    "    \"\"\"\n",
    "    Verify that gradient matrices match parameter matrix dimensions.\n",
    "    \n",
    "    Args:\n",
    "        W1, b1: Layer 1 parameters\n",
    "        W2, b2: Layer 2 parameters\n",
    "        dW1, db1: Layer 1 gradients\n",
    "        dW2, db2: Layer 2 gradients\n",
    "    \n",
    "    Returns:\n",
    "        Boolean indicating if all dimensions match\n",
    "    \"\"\"\n",
    "    checks = [\n",
    "        (W1.shape == dW1.shape, f\"W1 {W1.shape} vs dW1 {dW1.shape}\"),\n",
    "        (b1.shape == db1.shape, f\"b1 {b1.shape} vs db1 {db1.shape}\"),\n",
    "        (W2.shape == dW2.shape, f\"W2 {W2.shape} vs dW2 {dW2.shape}\"),\n",
    "        (b2.shape == db2.shape, f\"b2 {b2.shape} vs db2 {db2.shape}\")\n",
    "    ]\n",
    "    \n",
    "    all_match = True\n",
    "    for match, message in checks:\n",
    "        status = \"✓\" if match else \"✗\"\n",
    "        print(f\"{status} {message}\")\n",
    "        all_match = all_match and match\n",
    "    \n",
    "    return all_match\n",
    "\n",
    "# Example usage\n",
    "W1 = np.random.randn(3, 2)\n",
    "b1 = np.random.randn(3, 1)\n",
    "W2 = np.random.randn(1, 3)\n",
    "b2 = np.random.randn(1, 1)\n",
    "\n",
    "dW1 = np.random.randn(3, 2)\n",
    "db1 = np.random.randn(3, 1)\n",
    "dW2 = np.random.randn(1, 3)\n",
    "db2 = np.random.randn(1, 1)\n",
    "\n",
    "result = verify_gradient_dimensions(W1, b1, W2, b2, dW1, db1, dW2, db2)\n",
    "print(f\"\\nAll dimensions match: {result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4de26d8",
   "metadata": {},
   "source": [
    "**Implement code primitive: Initialize neural network weights randomly rather than to zero to enable proper learning**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6323767",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def initialize_parameters(layer_dims, seed=None):\n",
    "    \"\"\"\n",
    "    Initialize neural network parameters randomly.\n",
    "    \n",
    "    Args:\n",
    "        layer_dims: List of layer dimensions, e.g., [2, 3, 1] for 2 inputs, 3 hidden, 1 output\n",
    "        seed: Random seed for reproducibility\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary containing initialized weights and biases\n",
    "    \"\"\"\n",
    "    if seed is not None:\n",
    "        np.random.seed(seed)\n",
    "    \n",
    "    parameters = {}\n",
    "    \n",
    "    for l in range(1, len(layer_dims)):\n",
    "        # Random initialization for weights (small values to avoid saturation)\n",
    "        parameters[f'W{l}'] = np.random.randn(layer_dims[l], layer_dims[l-1]) * 0.01\n",
    "        \n",
    "        # Initialize biases to zero\n",
    "        parameters[f'b{l}'] = np.zeros((layer_dims[l], 1))\n",
    "    \n",
    "    return parameters\n",
    "\n",
    "# Example usage\n",
    "layer_dims = [2, 3, 1]  # 2 inputs, 3 hidden units, 1 output\n",
    "params = initialize_parameters(layer_dims, seed=42)\n",
    "\n",
    "for key, value in params.items():\n",
    "    print(f\"{key}: shape {value.shape}\")\n",
    "    print(f\"  Sample values: {value.flatten()[:3]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "740aa6c7",
   "metadata": {},
   "source": [
    "**Create a Mermaid diagram: Computation graph showing forward pass (z → a → loss) and backward pass (dL/da → dz → dw, db) for logistic regression**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd7c31cc",
   "metadata": {},
   "source": [
    "## Logistic Regression Computation Graph\n",
    "\n",
    "```mermaid\n",
    "graph TD\n",
    "    X[\"x<br/>(input)\"] --> Z[\"z = w·x + b<br/>(pre-activation)\"]\n",
    "    Z --> A[\"a = σ(z)<br/>(activation)\"]\n",
    "    A --> L[\"L(a,y)<br/>(loss)\"]\n",
    "    Y[\"y<br/>(label)\"] --> L\n",
    "    \n",
    "    L --> dA[\"dL/da<br/>(gradient w.r.t. a)\"]\n",
    "    dA --> dZ[\"dz = da·σ'(z)<br/>(gradient w.r.t. z)\"]\n",
    "    dZ --> dW[\"dw = dz·x<br/>(gradient w.r.t. w)\"]\n",
    "    dZ --> dB[\"db = dz<br/>(gradient w.r.t. b)\"]\n",
    "    \n",
    "    style X fill:#e1f5ff\n",
    "    style Y fill:#e1f5ff\n",
    "    style Z fill:#fff3e0\n",
    "    style A fill:#fff3e0\n",
    "    style L fill:#ffebee\n",
    "    style dA fill:#f3e5f5\n",
    "    style dZ fill:#f3e5f5\n",
    "    style dW fill:#e8f5e9\n",
    "    style dB fill:#e8f5e9\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6543c415",
   "metadata": {},
   "source": [
    "**Create a Mermaid diagram: Two-layer neural network computation graph with forward pass (x → z¹ → a¹ → z² → a² → loss) and corresponding backward pass derivatives**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "490a2244",
   "metadata": {},
   "source": [
    "## Two-Layer Neural Network Computation Graph\n",
    "\n",
    "```mermaid\n",
    "graph TD\n",
    "    X[\"X<br/>(input)\"] --> Z1[\"Z¹ = W¹·X + b¹\"]\n",
    "    Z1 --> A1[\"A¹ = g(Z¹)\"]\n",
    "    A1 --> Z2[\"Z² = W²·A¹ + b²\"]\n",
    "    Z2 --> A2[\"A² = σ(Z²)\"]\n",
    "    A2 --> L[\"L(A², Y)\"]\n",
    "    Y[\"Y<br/>(labels)\"] --> L\n",
    "    \n",
    "    L --> dA2[\"dA² = -Y/A² + (1-Y)/(1-A²)\"]\n",
    "    dA2 --> dZ2[\"dZ² = dA²·σ'(Z²)\"]\n",
    "    dZ2 --> dW2[\"dW² = 1/m·dZ²·A¹ᵀ\"]\n",
    "    dZ2 --> dB2[\"db² = 1/m·Σ dZ²\"]\n",
    "    dZ2 --> dA1[\"dA¹ = W²ᵀ·dZ²\"]\n",
    "    \n",
    "    dA1 --> dZ1[\"dZ¹ = dA¹·g'(Z¹)\"]\n",
    "    dZ1 --> dW1[\"dW¹ = 1/m·dZ¹·Xᵀ\"]\n",
    "    dZ1 --> dB1[\"db¹ = 1/m·Σ dZ¹\"]\n",
    "    \n",
    "    style X fill:#e1f5ff\n",
    "    style Y fill:#e1f5ff\n",
    "    style Z1 fill:#fff3e0\n",
    "    style A1 fill:#fff3e0\n",
    "    style Z2 fill:#fff3e0\n",
    "    style A2 fill:#fff3e0\n",
    "    style L fill:#ffebee\n",
    "    style dA2 fill:#f3e5f5\n",
    "    style dZ2 fill:#f3e5f5\n",
    "    style dA1 fill:#f3e5f5\n",
    "    style dZ1 fill:#f3e5f5\n",
    "    style dW2 fill:#e8f5e9\n",
    "    style dB2 fill:#e8f5e9\n",
    "    style dW1 fill:#e8f5e9\n",
    "    style dB1 fill:#e8f5e9\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bb27742",
   "metadata": {},
   "source": [
    "**Create a Mermaid diagram: Matrix dimension flow diagram showing how dimensions propagate through vectorized backpropagation operations**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69955095",
   "metadata": {},
   "source": [
    "## Matrix Dimension Flow in Vectorized Backpropagation\n",
    "\n",
    "```mermaid\n",
    "graph TD\n",
    "    X[\"X: n_x × m\"] --> Z[\"Z = W·X + b<br/>1 × m\"]\n",
    "    Z --> A[\"A = σ(Z)<br/>1 × m\"]\n",
    "    A --> L[\"L: scalar\"]\n",
    "    Y[\"Y: 1 × m\"] --> L\n",
    "    \n",
    "    L --> dA[\"dA: 1 × m\"]\n",
    "    dA --> dZ[\"dZ: 1 × m\"]\n",
    "    dZ --> dW[\"dW = 1/m·dZ·Xᵀ<br/>1 × n_x\"]\n",
    "    dZ --> dB[\"db = 1/m·Σ dZ<br/>1 × 1\"]\n",
    "    \n",
    "    W[\"W: 1 × n_x\"] -.-> dW\n",
    "    b[\"b: 1 × 1\"] -.-> dB\n",
    "    \n",
    "    style X fill:#e1f5ff\n",
    "    style Y fill:#e1f5ff\n",
    "    style Z fill:#fff3e0\n",
    "    style A fill:#fff3e0\n",
    "    style L fill:#ffebee\n",
    "    style dA fill:#f3e5f5\n",
    "    style dZ fill:#f3e5f5\n",
    "    style dW fill:#e8f5e9\n",
    "    style dB fill:#e8f5e9\n",
    "    style W fill:#eeeeee\n",
    "    style b fill:#eeeeee\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dd327b3",
   "metadata": {},
   "source": [
    "## Lesson 9"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab61d9c3",
   "metadata": {},
   "source": [
    "**Explain the core concepts of the lesson**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "474c6b51",
   "metadata": {},
   "source": [
    "## Core Concepts of Weight Initialization\n",
    "\n",
    "Weight initialization is a critical step in training neural networks. The way we initialize the parameters of a network—particularly the weights and biases—significantly impacts whether the network can learn effectively.\n",
    "\n",
    "**Key Concepts:**\n",
    "\n",
    "1. **Weight Initialization**: The process of assigning initial values to the weight matrices $w^{[l]}$ before training begins.\n",
    "\n",
    "2. **Symmetry Breaking Problem**: When all weights are initialized to the same value (e.g., zero), all hidden units in a layer compute identical functions, making them redundant and preventing the network from learning diverse features.\n",
    "\n",
    "3. **Random Initialization**: Using random values drawn from a distribution (typically Gaussian) to break symmetry and allow different hidden units to learn different features.\n",
    "\n",
    "4. **Gaussian Random Variables**: Random values sampled from a normal distribution with mean 0 and standard deviation 1, generated using `np.random.randn`.\n",
    "\n",
    "5. **Initialization Constant**: A small scaling factor (e.g., 0.01) applied to random weights to keep initial activations in regions where gradients are steep.\n",
    "\n",
    "6. **Saturation in Activation Functions**: When weights are too large, the pre-activation values $z^{[l]}$ push activations into flat regions of sigmoid or tanh functions, where gradients are nearly zero, slowing learning.\n",
    "\n",
    "7. **Bias Initialization**: Bias terms $b^{[l]}$ can be safely initialized to zero since they don't suffer from the symmetry problem that affects weights.\n",
    "\n",
    "8. **Parameter Initialization Strategy**: A systematic approach to choosing initial values for weights and biases to enable effective learning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db5c2139",
   "metadata": {},
   "source": [
    "**Explain intuitions and mental models for the lesson**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3191c040",
   "metadata": {},
   "source": [
    "## Intuitions and Mental Models\n",
    "\n",
    "**Why Zero Initialization Fails:**\n",
    "\n",
    "Imagine a neural network with multiple hidden units in a layer. If all weights are initialized to zero, then every hidden unit receives the same input and computes the same function. During backpropagation, all hidden units receive identical gradient updates, so they remain identical throughout training. This means the network effectively has only one hidden unit instead of many—a massive waste of capacity. The network cannot learn diverse features because all units are locked in symmetry.\n",
    "\n",
    "**How Random Initialization Breaks Symmetry:**\n",
    "\n",
    "By initializing weights to small random values, each hidden unit starts with a slightly different set of weights. This means each unit computes a different function on the input, producing different activations. During backpropagation, each unit receives different gradients, allowing them to diverge and specialize in learning different features. This diversity is essential for the network's learning capacity.\n",
    "\n",
    "**The Problem with Large Weights:**\n",
    "\n",
    "Consider the sigmoid function $\\sigma(z) = \\frac{1}{1 + e^{-z}}$. When $z$ is very large or very small, the function is nearly flat—the gradient $\\frac{d\\sigma}{dz}$ is close to zero. If weights are initialized too large, the pre-activation values $z^{[l]} = w^{[l]} x + b^{[l]}$ become very large, pushing activations into these flat regions. With tiny gradients, learning becomes extremely slow. This is called **saturation**.\n",
    "\n",
    "**The Sweet Spot:**\n",
    "\n",
    "Small random weights (e.g., scaled by 0.01) keep initial activations in the steep, middle regions of activation functions where gradients are large. This allows the network to learn quickly while maintaining the diversity needed for different hidden units to specialize.\n",
    "\n",
    "**Why Biases Are Different:**\n",
    "\n",
    "Bias terms don't have the symmetry problem because they are added to the output of each hidden unit independently. Even if all biases are zero, the hidden units still compute different functions (due to different weights), so they produce different activations. Therefore, initializing biases to zero is safe and conventional."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c8dba67",
   "metadata": {},
   "source": [
    "**Present and explain the key equations used in the lesson**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb81f197",
   "metadata": {},
   "source": [
    "## Key Equations\n",
    "\n",
    "**Weight Initialization Formula:**\n",
    "\n",
    "$$w^{[l]} = \\text{np.random.randn}(n^{[l]}, n^{[l-1]}) \\times \\text{small constant}$$\n",
    "\n",
    "Where:\n",
    "- $w^{[l]}$ is the weight matrix for layer $l$\n",
    "- $n^{[l]}$ is the number of units in layer $l$\n",
    "- $n^{[l-1]}$ is the number of units in layer $l-1$\n",
    "- `np.random.randn` generates random values from a standard normal distribution\n",
    "- The small constant (typically 0.01) scales the random values to keep them small\n",
    "\n",
    "**Pre-activation Computation:**\n",
    "\n",
    "$$z^{[l]} = w^{[l]} x + b^{[l]}$$\n",
    "\n",
    "Where:\n",
    "- $z^{[l]}$ is the pre-activation (linear combination) for layer $l$\n",
    "- $x$ is the input to layer $l$ (or the activation from the previous layer)\n",
    "- $b^{[l]}$ is the bias vector for layer $l$\n",
    "\n",
    "**Bias Initialization:**\n",
    "\n",
    "$$b^{[l]} = 0$$\n",
    "\n",
    "Bias terms are initialized to zero vectors, which is safe because biases don't create the symmetry problem that weights do."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b53c9e1",
   "metadata": {},
   "source": [
    "**Implement code primitive: Generate random weight matrix using np.random.randn with specified dimensions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c54e6639",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Generate random weight matrix\n",
    "n_current = 3  # number of units in current layer\n",
    "n_previous = 4  # number of units in previous layer\n",
    "\n",
    "W = np.random.randn(n_current, n_previous)\n",
    "print(\"Random weight matrix shape:\", W.shape)\n",
    "print(\"Random weight matrix:\\n\", W)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaf31d74",
   "metadata": {},
   "source": [
    "**Implement code primitive: Scale random weights by a small constant (e.g., 0.01)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "701f6396",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Generate and scale random weight matrix\n",
    "n_current = 3\n",
    "n_previous = 4\n",
    "scaling_constant = 0.01\n",
    "\n",
    "W = np.random.randn(n_current, n_previous) * scaling_constant\n",
    "print(\"Scaled weight matrix shape:\", W.shape)\n",
    "print(\"Scaled weight matrix:\\n\", W)\n",
    "print(\"\\nMean of weights:\", np.mean(W))\n",
    "print(\"Std of weights:\", np.std(W))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1604889",
   "metadata": {},
   "source": [
    "**Implement code primitive: Initialize bias terms to zero**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eb3e06b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Initialize bias terms to zero\n",
    "n_current = 3\n",
    "\n",
    "b = np.zeros((n_current, 1))\n",
    "print(\"Bias vector shape:\", b.shape)\n",
    "print(\"Bias vector:\\n\", b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64455331",
   "metadata": {},
   "source": [
    "**Implement code primitive: Demonstrate why zero initialization fails by showing symmetric gradient updates**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76a916f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Demonstrate the symmetry problem with zero initialization\n",
    "np.random.seed(42)\n",
    "\n",
    "# Zero initialization\n",
    "W_zero = np.zeros((2, 3))\n",
    "b_zero = np.zeros((2, 1))\n",
    "\n",
    "# Input\n",
    "X = np.random.randn(3, 5)  # 3 features, 5 samples\n",
    "\n",
    "# Forward pass with zero weights\n",
    "Z = W_zero @ X + b_zero\n",
    "print(\"Pre-activations with zero weights:\\n\", Z)\n",
    "print(\"\\nAll pre-activations are identical (all zeros)\")\n",
    "\n",
    "# Simulate identical gradients\n",
    "dZ = np.random.randn(2, 5)  # Gradient from next layer\n",
    "dW_zero = dZ @ X.T / 5\n",
    "print(\"\\nGradients for weights (zero init):\\n\", dW_zero)\n",
    "print(\"\\nBoth hidden units receive identical gradient updates\")\n",
    "print(\"This means they will remain identical throughout training.\")\n",
    "\n",
    "# Random initialization for comparison\n",
    "W_random = np.random.randn(2, 3) * 0.01\n",
    "Z_random = W_random @ X + b_zero\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Pre-activations with random weights:\\n\", Z_random)\n",
    "print(\"\\nPre-activations are different for each hidden unit\")\n",
    "\n",
    "dW_random = dZ @ X.T / 5\n",
    "print(\"\\nGradients for weights (random init):\\n\", dW_random)\n",
    "print(\"\\nEach hidden unit receives different gradient updates\")\n",
    "print(\"This allows them to diverge and learn different features.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "371c956d",
   "metadata": {},
   "source": [
    "**Create a Mermaid diagram: Flowchart showing the consequence of zero weight initialization: identical hidden units → identical activations → identical gradients → no learning diversity**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f091a8cf",
   "metadata": {},
   "source": [
    "## Consequence of Zero Weight Initialization\n",
    "\n",
    "```mermaid\n",
    "graph TD\n",
    "    A[\"All Weights Initialized to Zero\"] --> B[\"All Hidden Units Compute Identical Functions\"]\n",
    "    B --> C[\"All Hidden Units Produce Identical Activations\"]\n",
    "    C --> D[\"All Hidden Units Receive Identical Gradients\"]\n",
    "    D --> E[\"Weights Remain Identical After Update\"]\n",
    "    E --> F[\"No Learning Diversity\"]\n",
    "    F --> G[\"Network Cannot Learn Diverse Features\"]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a050499",
   "metadata": {},
   "source": [
    "**Create a Mermaid diagram: Diagram illustrating how large weights cause z values to saturate in sigmoid/tanh functions, resulting in small gradients**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f55ed5f",
   "metadata": {},
   "source": [
    "## Effect of Large Weights on Activation Functions\n",
    "\n",
    "```mermaid\n",
    "graph TD\n",
    "    A[\"Large Initial Weights\"] --> B[\"Large Pre-activation Values z\"]\n",
    "    B --> C[\"Sigmoid/Tanh Pushed to Extremes\"]\n",
    "    C --> D[\"Activation Function is Nearly Flat\"]\n",
    "    D --> E[\"Gradient dσ/dz ≈ 0\"]\n",
    "    E --> F[\"Vanishing Gradients\"]\n",
    "    F --> G[\"Very Slow Learning\"]\n",
    "    \n",
    "    H[\"Small Random Weights\"] --> I[\"Moderate Pre-activation Values z\"]\n",
    "    I --> J[\"Sigmoid/Tanh in Steep Region\"]\n",
    "    J --> K[\"Activation Function is Steep\"]\n",
    "    K --> L[\"Gradient dσ/dz is Large\"]\n",
    "    L --> M[\"Strong Gradient Signal\"]\n",
    "    M --> N[\"Effective Learning\"]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd69b236",
   "metadata": {},
   "source": [
    "## Lesson 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9d8addc",
   "metadata": {},
   "source": [
    "**Explain the core concepts of the lesson**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42270d57",
   "metadata": {},
   "source": [
    "## Core Concepts\n",
    "\n",
    "**Generative Adversarial Networks (GANs)** are a framework for learning to generate new data that resembles training data. GANs consist of two competing neural networks:\n",
    "\n",
    "1. **Generator**: Creates fake examples from random noise, attempting to fool the discriminator\n",
    "2. **Discriminator**: Distinguishes between real examples from the training data and fake examples generated by the generator\n",
    "\n",
    "These networks compete iteratively, with the generator improving at creating realistic data and the discriminator improving at detecting fakes. This adversarial process drives both networks toward better performance.\n",
    "\n",
    "**Generative Modeling** is the task of learning the underlying distribution of training data to generate new, similar examples. GANs are one approach to this problem.\n",
    "\n",
    "**GAN Stability** refers to the challenge that GANs can be difficult to train and require careful tuning of hyperparameters and architecture choices.\n",
    "\n",
    "**Semi-supervised Learning** is an application of GANs where the discriminator's learned representations can be used for classification tasks with limited labeled data.\n",
    "\n",
    "**Adversarial Examples** are carefully crafted inputs designed to fool machine learning models. They reveal vulnerabilities in neural networks that are distinct from traditional computer security problems.\n",
    "\n",
    "**Machine Learning Security** encompasses protecting ML systems from adversarial attacks at multiple levels:\n",
    "- **Application-level security**: Protecting the ML model itself from adversarial examples\n",
    "- **Network-level security**: Protecting data in transit and system infrastructure\n",
    "- **Machine-learning-level attacks**: Attacks that exploit properties of ML algorithms themselves"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f55d6988",
   "metadata": {},
   "source": [
    "**Explain intuitions and mental models for the lesson**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5e30452",
   "metadata": {},
   "source": [
    "## Intuitions and Mental Models\n",
    "\n",
    "**The Competing Networks Intuition**: Think of GANs as a game between a counterfeiter (generator) and a detective (discriminator). The counterfeiter tries to create fake currency that looks real, while the detective tries to catch fakes. As they compete, both improve—the counterfeiter becomes better at forgery and the detective becomes better at detection. Eventually, the counterfeiter produces currency so good that the detective cannot tell it apart from real currency.\n",
    "\n",
    "**GAN Training Challenges**: GANs currently work well sometimes but require careful tuning, similar to how deep learning was finicky 10 years ago before techniques like ReLU and batch normalization made it reliable. The field is still developing best practices for stable training.\n",
    "\n",
    "**Adversarial Vulnerability**: Adversarial examples show that machine learning models can be fooled by carefully crafted inputs. This reveals a new class of security vulnerabilities distinct from traditional computer security problems. A model might correctly classify an image of a cat, but a small, imperceptible perturbation can cause it to misclassify the same image as a dog.\n",
    "\n",
    "**Security by Design**: Building security into machine learning systems from the start is more effective than trying to patch vulnerabilities after deployment. This means considering adversarial robustness during model development, not as an afterthought.\n",
    "\n",
    "**Accessibility of Deep Learning**: The field of deep learning has evolved from struggling to work on high-dimensional data like images to having many open research directions. This makes it accessible to newcomers without requiring a Ph.D., as foundational techniques are now well-established and tools are readily available."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfe4eb80",
   "metadata": {},
   "source": [
    "**Implement code primitive: Implementing a basic GAN architecture with generator and discriminator networks**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf87dd72",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, latent_dim=100, output_dim=784):\n",
    "        super(Generator, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(latent_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, output_dim),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "    \n",
    "    def forward(self, z):\n",
    "        return self.model(z)\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, input_dim=784):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_dim, 512),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(256, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "latent_dim = 100\n",
    "generator = Generator(latent_dim=latent_dim)\n",
    "discriminator = Discriminator()\n",
    "\n",
    "z = torch.randn(32, latent_dim)\n",
    "fake_data = generator(z)\n",
    "fake_prediction = discriminator(fake_data)\n",
    "\n",
    "print(f\"Generated data shape: {fake_data.shape}\")\n",
    "print(f\"Discriminator prediction shape: {fake_prediction.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1956eb14",
   "metadata": {},
   "source": [
    "**Implement code primitive: Creating code to generate synthetic examples that resemble training data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75de68bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class SimpleGenerator(nn.Module):\n",
    "    def __init__(self, latent_dim=100):\n",
    "        super(SimpleGenerator, self).__init__()\n",
    "        self.fc1 = nn.Linear(latent_dim, 128)\n",
    "        self.fc2 = nn.Linear(128, 256)\n",
    "        self.fc3 = nn.Linear(256, 784)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.tanh = nn.Tanh()\n",
    "    \n",
    "    def forward(self, z):\n",
    "        x = self.relu(self.fc1(z))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.tanh(self.fc3(x))\n",
    "        return x\n",
    "\n",
    "generator = SimpleGenerator(latent_dim=100)\n",
    "\n",
    "batch_size = 16\n",
    "latent_dim = 100\n",
    "z = torch.randn(batch_size, latent_dim)\n",
    "\n",
    "synthetic_examples = generator(z)\n",
    "\n",
    "print(f\"Batch size: {batch_size}\")\n",
    "print(f\"Synthetic examples shape: {synthetic_examples.shape}\")\n",
    "print(f\"Value range: [{synthetic_examples.min().item():.3f}, {synthetic_examples.max().item():.3f}]\")\n",
    "print(f\"Mean: {synthetic_examples.mean().item():.3f}, Std: {synthetic_examples.std().item():.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7db2bc16",
   "metadata": {},
   "source": [
    "**Implement code primitive: Developing adversarial attack code to generate adversarial examples that fool classifiers**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1863554b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class SimpleClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleClassifier, self).__init__()\n",
    "        self.fc1 = nn.Linear(784, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        return self.fc2(x)\n",
    "\n",
    "def fgsm_attack(model, x, y, epsilon=0.1):\n",
    "    x.requires_grad = True\n",
    "    output = model(x)\n",
    "    loss = nn.CrossEntropyLoss()(output, y)\n",
    "    loss.backward()\n",
    "    \n",
    "    perturbation = epsilon * x.grad.sign()\n",
    "    adversarial_x = x + perturbation\n",
    "    adversarial_x = torch.clamp(adversarial_x, -1, 1)\n",
    "    \n",
    "    return adversarial_x.detach()\n",
    "\n",
    "model = SimpleClassifier()\n",
    "model.eval()\n",
    "\n",
    "x = torch.randn(8, 784)\n",
    "y = torch.randint(0, 10, (8,))\n",
    "\n",
    "adversarial_examples = fgsm_attack(model, x.clone(), y, epsilon=0.1)\n",
    "\n",
    "print(f\"Original examples shape: {x.shape}\")\n",
    "print(f\"Adversarial examples shape: {adversarial_examples.shape}\")\n",
    "print(f\"Perturbation magnitude: {(adversarial_examples - x).abs().max().item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c26ae081",
   "metadata": {},
   "source": [
    "**Create a Mermaid diagram: A flowchart showing the GAN training process with generator and discriminator competing iteratively**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c95f1369",
   "metadata": {},
   "source": [
    "```mermaid\n",
    "graph TD\n",
    "    A[Start Training] --> B[Sample random noise z]\n",
    "    B --> C[Generator creates fake data]\n",
    "    C --> D[Discriminator evaluates fake data]\n",
    "    D --> E[Discriminator evaluates real data]\n",
    "    E --> F{Discriminator Loss}\n",
    "    F --> G[Update Discriminator weights]\n",
    "    G --> H[Sample random noise z]\n",
    "    H --> I[Generator creates fake data]\n",
    "    I --> J[Discriminator evaluates fake data]\n",
    "    J --> K{Generator Loss}\n",
    "    K --> L[Update Generator weights]\n",
    "    L --> M{Training Complete?}\n",
    "    M -->|No| B\n",
    "    M -->|Yes| N[End Training]\n",
    "    \n",
    "    style B fill:#e1f5ff\n",
    "    style C fill:#fff3e0\n",
    "    style D fill:#f3e5f5\n",
    "    style E fill:#f3e5f5\n",
    "    style G fill:#e8f5e9\n",
    "    style L fill:#fce4ec\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "827b8d62",
   "metadata": {},
   "source": [
    "**Create a Mermaid diagram: A diagram illustrating the three levels of security: application-level, network-level, and machine-learning-level attacks**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3c9149b",
   "metadata": {},
   "source": [
    "```mermaid\n",
    "graph TD\n",
    "    A[Machine Learning System Security] --> B[Application-Level Security]\n",
    "    A --> C[Network-Level Security]\n",
    "    A --> D[Machine-Learning-Level Security]\n",
    "    \n",
    "    B --> B1[Protect ML Model]\n",
    "    B --> B2[Defend Against Adversarial Examples]\n",
    "    B --> B3[Input Validation]\n",
    "    \n",
    "    C --> C1[Secure Data in Transit]\n",
    "    C --> C2[Protect System Infrastructure]\n",
    "    C --> C3[Access Control]\n",
    "    \n",
    "    D --> D1[Adversarial Robustness]\n",
    "    D --> D2[Model Poisoning Defense]\n",
    "    D --> D3[Exploit ML Algorithm Properties]\n",
    "    \n",
    "    style B fill:#e3f2fd\n",
    "    style C fill:#f3e5f5\n",
    "    style D fill:#fff3e0\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d9d79b9",
   "metadata": {},
   "source": [
    "# Week 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2fb44e6",
   "metadata": {},
   "source": [
    "## Lesson 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36dcdc50",
   "metadata": {},
   "source": [
    "**Explain the core concepts of the lesson**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd5a5d49",
   "metadata": {},
   "source": [
    "## Core Concepts\n",
    "\n",
    "A **deep neural network** is a stack of multiple layers that process information sequentially from input to output. The key concepts for understanding deep networks are:\n",
    "\n",
    "- **Network Depth**: The number of layers in the network, denoted as $L$. This counts only the hidden layers and the output layer, excluding the input layer.\n",
    "\n",
    "- **Layer Counting Convention**: The input layer is indexed as layer 0, hidden layers are indexed as 1 through $L-1$, and the output layer is indexed as layer $L$. This means a network with $L$ layers has $L+1$ indexed positions (0 through $L$).\n",
    "\n",
    "- **Shallow vs. Deep Networks**: Logistic regression and single-hidden-layer networks are considered shallow models. Networks with many hidden layers are deep models. The distinction is a matter of degree rather than a binary classification.\n",
    "\n",
    "- **Network Capacity**: Very deep networks can learn functions that shallower models cannot. However, the optimal depth for a given problem is typically unknown in advance and should be treated as a hyperparameter to tune during model development.\n",
    "\n",
    "- **Units Per Layer**: Each layer $l$ contains $n^{[l]}$ units (neurons). The number of units can vary across layers and is a key architectural choice."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cd3aab9",
   "metadata": {},
   "source": [
    "**Explain intuitions and mental models for the lesson**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdd656ad",
   "metadata": {},
   "source": [
    "## Intuitions and Mental Models\n",
    "\n",
    "**Stacking as Composition**: Think of a deep neural network as a composition of functions. Each layer applies a transformation to the output of the previous layer. The deeper the network, the more transformations are applied, allowing the network to learn increasingly complex representations of the data.\n",
    "\n",
    "**Depth as Expressiveness**: A key intuition is that depth enables expressiveness. Shallow networks (like logistic regression or a single hidden layer) can only learn relatively simple decision boundaries. By stacking more layers, the network gains the ability to learn hierarchical features—lower layers might learn simple patterns, while deeper layers combine these patterns into more abstract concepts.\n",
    "\n",
    "**Hyperparameter Tuning**: The depth of a network is not something you can determine theoretically for a given problem. Instead, you should experiment with different depths and evaluate which works best for your specific task. This is why depth is treated as a hyperparameter rather than something fixed by the problem structure.\n",
    "\n",
    "**Gradual Increase in Complexity**: The distinction between shallow and deep networks is not sharp. A network with 2 hidden layers is deeper than one with 1, but whether it's \"truly deep\" depends on context. What matters is that adding more layers generally increases the model's capacity to learn complex functions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ddf4061",
   "metadata": {},
   "source": [
    "**Present and explain the key equations used in the lesson**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d12ad07b",
   "metadata": {},
   "source": [
    "## Key Equations\n",
    "\n",
    "The notation and equations for deep neural networks are:\n",
    "\n",
    "**Network Architecture**:\n",
    "$$L = \\text{total number of layers}$$\n",
    "$$n^{[l]} = \\text{number of units in layer } l$$\n",
    "\n",
    "**Forward Propagation**:\n",
    "For each layer $l$ from 1 to $L$, we compute:\n",
    "$$z^{[l]} = W^{[l]} a^{[l-1]} + b^{[l]}$$\n",
    "$$a^{[l]} = g(z^{[l]})$$\n",
    "\n",
    "where:\n",
    "- $W^{[l]}$ is the weight matrix for layer $l$\n",
    "- $b^{[l]}$ is the bias vector for layer $l$\n",
    "- $z^{[l]}$ is the pre-activation (linear combination) for layer $l$\n",
    "- $a^{[l]}$ is the activation (post-activation output) for layer $l$\n",
    "- $g$ is the activation function (e.g., ReLU, sigmoid, tanh)\n",
    "\n",
    "**Input and Output**:\n",
    "$$a^{[0]} = x$$\n",
    "$$a^{[L]} = \\hat{y}$$\n",
    "\n",
    "The input features are denoted as $a^{[0]}$, and the network's final prediction is $a^{[L]}$, the activation of the output layer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfcae55f",
   "metadata": {},
   "source": [
    "**Implement code primitive: Implement notation system for accessing layer-specific parameters: W[l], b[l], z[l], a[l] using dictionary or array indexing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02aff38d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Notation system for layer-specific parameters\n",
    "# Using dictionaries to store parameters indexed by layer\n",
    "\n",
    "W = {}  # Weight matrices: W[l] for layer l\n",
    "b = {}  # Bias vectors: b[l] for layer l\n",
    "z = {}  # Pre-activations: z[l] for layer l\n",
    "a = {}  # Activations: a[l] for layer l\n",
    "\n",
    "# Example: Initialize parameters for a 3-layer network\n",
    "import numpy as np\n",
    "\n",
    "# Layer 1: 3 units, input from 2 features\n",
    "W[1] = np.random.randn(3, 2) * 0.01\n",
    "b[1] = np.zeros((3, 1))\n",
    "\n",
    "# Layer 2: 4 units, input from 3 units in layer 1\n",
    "W[2] = np.random.randn(4, 3) * 0.01\n",
    "b[2] = np.zeros((4, 1))\n",
    "\n",
    "# Layer 3: 1 unit (output), input from 4 units in layer 2\n",
    "W[3] = np.random.randn(1, 4) * 0.01\n",
    "b[3] = np.zeros((1, 1))\n",
    "\n",
    "# Access parameters by layer\n",
    "print(f\"W[1] shape: {W[1].shape}\")\n",
    "print(f\"b[2] shape: {b[2].shape}\")\n",
    "print(f\"W[3] shape: {W[3].shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aef4e82",
   "metadata": {},
   "source": [
    "**Implement code primitive: Create a data structure to store network architecture: number of layers L and units per layer n[l]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcf39fe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data structure for network architecture\n",
    "\n",
    "# Total number of layers (excluding input layer)\n",
    "L = 3\n",
    "\n",
    "# Number of units in each layer (indexed from 0 to L)\n",
    "# n[0] = number of input features\n",
    "# n[1] to n[L] = number of units in each layer\n",
    "n = {\n",
    "    0: 2,   # Input layer: 2 features\n",
    "    1: 3,   # Hidden layer 1: 3 units\n",
    "    2: 4,   # Hidden layer 2: 4 units\n",
    "    3: 1    # Output layer: 1 unit\n",
    "}\n",
    "\n",
    "# Verify architecture\n",
    "print(f\"Number of layers L: {L}\")\n",
    "print(f\"Network architecture:\")\n",
    "for layer in range(L + 1):\n",
    "    print(f\"  Layer {layer}: {n[layer]} units\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ad8412c",
   "metadata": {},
   "source": [
    "**Implement code primitive: Implement forward propagation loop that iterates through layers l=1 to L, computing z[l] and a[l] sequentially**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a74298cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forward propagation through all layers\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "# Initialize input (a[0])\n",
    "a[0] = np.array([[0.5], [0.3]])  # Example input with 2 features\n",
    "\n",
    "# Forward propagation loop\n",
    "for l in range(1, L + 1):\n",
    "    # Compute pre-activation\n",
    "    z[l] = np.dot(W[l], a[l-1]) + b[l]\n",
    "    \n",
    "    # Apply activation function\n",
    "    if l < L:\n",
    "        # Use ReLU for hidden layers\n",
    "        a[l] = relu(z[l])\n",
    "    else:\n",
    "        # Use sigmoid for output layer\n",
    "        a[l] = sigmoid(z[l])\n",
    "    \n",
    "    print(f\"Layer {l}: z[{l}] shape = {z[l].shape}, a[{l}] shape = {a[l].shape}\")\n",
    "\n",
    "# Final prediction\n",
    "print(f\"\\nFinal prediction a[{L}]: {a[L]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0019985b",
   "metadata": {},
   "source": [
    "**Create a Mermaid diagram: Diagram showing a 4-layer neural network with input layer (layer 0), three hidden layers (layers 1-3), and output layer (layer 4), with layer indices and unit counts labeled**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00cc4f1b",
   "metadata": {},
   "source": [
    "## Network Architecture Visualization\n",
    "\n",
    "```mermaid\n",
    "graph LR\n",
    "    subgraph L0[\"Layer 0<br/>(Input)<br/>n⁽⁰⁾=2\"]\n",
    "        x1((x₁))\n",
    "        x2((x₂))\n",
    "    end\n",
    "    \n",
    "    subgraph L1[\"Layer 1<br/>(Hidden)<br/>n⁽¹⁾=3\"]\n",
    "        h1_1((a₁⁽¹⁾))\n",
    "        h1_2((a₂⁽¹⁾))\n",
    "        h1_3((a₃⁽¹⁾))\n",
    "    end\n",
    "    \n",
    "    subgraph L2[\"Layer 2<br/>(Hidden)<br/>n⁽²⁾=4\"]\n",
    "        h2_1((a₁⁽²⁾))\n",
    "        h2_2((a₂⁽²⁾))\n",
    "        h2_3((a₃⁽²⁾))\n",
    "        h2_4((a₄⁽²⁾))\n",
    "    end\n",
    "    \n",
    "    subgraph L3[\"Layer 3<br/>(Hidden)<br/>n⁽³⁾=3\"]\n",
    "        h3_1((a₁⁽³⁾))\n",
    "        h3_2((a₂⁽³⁾))\n",
    "        h3_3((a₃⁽³⁾))\n",
    "    end\n",
    "    \n",
    "    subgraph L4[\"Layer 4<br/>(Output)<br/>n⁽⁴⁾=1\"]\n",
    "        y((ŷ))\n",
    "    end\n",
    "    \n",
    "    x1 --> h1_1\n",
    "    x1 --> h1_2\n",
    "    x1 --> h1_3\n",
    "    x2 --> h1_1\n",
    "    x2 --> h1_2\n",
    "    x2 --> h1_3\n",
    "    \n",
    "    h1_1 --> h2_1\n",
    "    h1_1 --> h2_2\n",
    "    h1_1 --> h2_3\n",
    "    h1_1 --> h2_4\n",
    "    h1_2 --> h2_1\n",
    "    h1_2 --> h2_2\n",
    "    h1_2 --> h2_3\n",
    "    h1_2 --> h2_4\n",
    "    h1_3 --> h2_1\n",
    "    h1_3 --> h2_2\n",
    "    h1_3 --> h2_3\n",
    "    h1_3 --> h2_4\n",
    "    \n",
    "    h2_1 --> h3_1\n",
    "    h2_1 --> h3_2\n",
    "    h2_1 --> h3_3\n",
    "    h2_2 --> h3_1\n",
    "    h2_2 --> h3_2\n",
    "    h2_2 --> h3_3\n",
    "    h2_3 --> h3_1\n",
    "    h2_3 --> h3_2\n",
    "    h2_3 --> h3_3\n",
    "    h2_4 --> h3_1\n",
    "    h2_4 --> h3_2\n",
    "    h2_4 --> h3_3\n",
    "    \n",
    "    h3_1 --> y\n",
    "    h3_2 --> y\n",
    "    h3_3 --> y\n",
    "```\n",
    "\n",
    "This diagram shows a deep neural network with:\n",
    "- **Layer 0** (Input): 2 input features\n",
    "- **Layer 1** (Hidden): 3 units\n",
    "- **Layer 2** (Hidden): 4 units\n",
    "- **Layer 3** (Hidden): 3 units\n",
    "- **Layer 4** (Output): 1 output unit (prediction $\\hat{y}$)\n",
    "\n",
    "The total number of layers is $L = 4$ (counting only hidden and output layers, excluding the input layer)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d455055",
   "metadata": {},
   "source": [
    "## Lesson 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d84226c",
   "metadata": {},
   "source": [
    "**Explain the core concepts of the lesson**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cafd9e9",
   "metadata": {},
   "source": [
    "## Core Concepts of Forward Propagation\n",
    "\n",
    "Forward propagation is the process of computing predictions in a neural network by passing input data through each layer sequentially. The key concepts are:\n",
    "\n",
    "**Layer-wise Computation**: Each layer performs the same fundamental operation: compute a weighted sum of inputs, add a bias term, and apply an activation function.\n",
    "\n",
    "**Activation Computation**: For each layer $l$, we compute:\n",
    "- The pre-activation (linear combination): $z^{[l]} = w^{[l]} a^{[l-1]} + b^{[l]}$\n",
    "- The activation (after applying the activation function): $a^{[l]} = g(z^{[l]})$\n",
    "\n",
    "**Input as Layer Zero**: The input features $x$ can be viewed as the activations of layer zero, denoted $A^{[0]} = X$. This unified perspective means all layers follow the same computational pattern.\n",
    "\n",
    "**Vectorized Forward Propagation**: Instead of computing predictions for one training example at a time, we stack all training examples as columns in matrices and compute activations for the entire batch simultaneously using the same equations with capital letter notation.\n",
    "\n",
    "**Sequential Computation**: Layers must be computed in order from layer 1 to layer $L$ because each layer depends on the activations from the previous layer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "096337dc",
   "metadata": {},
   "source": [
    "**Explain intuitions and mental models for the lesson**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37bd8fad",
   "metadata": {},
   "source": [
    "## Intuitions and Mental Models\n",
    "\n",
    "**The Repeating Pattern**: Forward propagation in deep networks follows the same pattern repeated layer by layer: compute weighted sum, add bias, apply activation function. Once you understand one layer, you understand the entire network—you just repeat the process.\n",
    "\n",
    "**Unified Layer Equations**: By treating the input features as the activations of layer zero ($a^{[0]} = x$), all layer equations follow the same general form. This makes the implementation cleaner and more elegant.\n",
    "\n",
    "**Vectorization for Efficiency**: Vectorization stacks all training examples as columns in matrices, allowing you to compute activations for the entire training set simultaneously using the same equations. This is much faster than looping over individual examples.\n",
    "\n",
    "**Loops Over Layers Are Necessary**: Even though we usually avoid explicit loops in favor of vectorization, a for loop over layers is necessary and acceptable because each layer must be computed sequentially before the next layer can begin. You cannot compute layer 2 until layer 1 is complete.\n",
    "\n",
    "**Matrix Dimensions Matter**: Matrix dimensions are critical for debugging deep network implementations. Carefully tracking the shapes of weight matrices, biases, and activations throughout the computation helps catch errors early."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24bf206f",
   "metadata": {},
   "source": [
    "**Present and explain the key equations used in the lesson**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2fe3474",
   "metadata": {},
   "source": [
    "## Key Equations\n",
    "\n",
    "**Single Training Example**:\n",
    "\n",
    "For a single training example with input $a^{[l-1]}$, the forward propagation equations for layer $l$ are:\n",
    "\n",
    "$$z^{[l]} = w^{[l]} a^{[l-1]} + b^{[l]}$$\n",
    "\n",
    "$$a^{[l]} = g(z^{[l]})$$\n",
    "\n",
    "where:\n",
    "- $w^{[l]}$ is the weight matrix for layer $l$\n",
    "- $b^{[l]}$ is the bias vector for layer $l$\n",
    "- $g$ is the activation function (e.g., ReLU, sigmoid, tanh)\n",
    "- $z^{[l]}$ is the pre-activation\n",
    "- $a^{[l]}$ is the activation (output) of layer $l$\n",
    "\n",
    "**Vectorized Form (Entire Training Set)**:\n",
    "\n",
    "When processing $m$ training examples simultaneously, we use capital letter notation:\n",
    "\n",
    "$$Z^{[l]} = w^{[l]} A^{[l-1]} + b^{[l]}$$\n",
    "\n",
    "$$A^{[l]} = g(Z^{[l]})$$\n",
    "\n",
    "where:\n",
    "- $A^{[l-1]}$ is a matrix of shape $(n^{[l-1]}, m)$ containing all training examples as columns\n",
    "- $Z^{[l]}$ and $A^{[l]}$ are matrices of shape $(n^{[l]}, m)$\n",
    "- The bias $b^{[l]}$ is broadcast across all $m$ examples\n",
    "\n",
    "**Input as Layer Zero**:\n",
    "\n",
    "$$A^{[0]} = X$$\n",
    "\n",
    "where $X$ is the input matrix of shape $(n^{[0]}, m)$ containing all training examples as columns."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50896c69",
   "metadata": {},
   "source": [
    "**Implement code primitive: Implement single training example forward propagation by iterating through layers, computing z and a for each layer using weight matrices, biases, and activation functions.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02ce5438",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_propagation_single_example(x, parameters, L, activation_fn):\n",
    "    \"\"\"\n",
    "    Forward propagation for a single training example.\n",
    "    \n",
    "    Args:\n",
    "        x: Input vector of shape (n[0],)\n",
    "        parameters: Dictionary containing weights w[l] and biases b[l] for each layer\n",
    "        L: Number of layers\n",
    "        activation_fn: Function that applies activation (e.g., relu, sigmoid)\n",
    "    \n",
    "    Returns:\n",
    "        a: Final activation (output) of the network\n",
    "        cache: Dictionary storing z and a values for each layer\n",
    "    \"\"\"\n",
    "    cache = {}\n",
    "    a = x\n",
    "    cache['a0'] = a\n",
    "    \n",
    "    for l in range(1, L + 1):\n",
    "        w = parameters[f'w{l}']\n",
    "        b = parameters[f'b{l}']\n",
    "        \n",
    "        z = w @ a + b\n",
    "        a = activation_fn(z)\n",
    "        \n",
    "        cache[f'z{l}'] = z\n",
    "        cache[f'a{l}'] = a\n",
    "    \n",
    "    return a, cache"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aa15450",
   "metadata": {},
   "source": [
    "**Implement code primitive: Implement vectorized forward propagation using capital letter notation where matrices contain all training examples stacked as columns, applying the same layer equations to the entire batch.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdcb0b5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_propagation_vectorized(X, parameters, L, activation_fn):\n",
    "    \"\"\"\n",
    "    Vectorized forward propagation for all training examples.\n",
    "    \n",
    "    Args:\n",
    "        X: Input matrix of shape (n[0], m) where m is number of training examples\n",
    "        parameters: Dictionary containing weights w[l] and biases b[l] for each layer\n",
    "        L: Number of layers\n",
    "        activation_fn: Function that applies activation element-wise\n",
    "    \n",
    "    Returns:\n",
    "        A: Final activation (output) matrix of shape (n[L], m)\n",
    "        cache: Dictionary storing Z and A values for each layer\n",
    "    \"\"\"\n",
    "    cache = {}\n",
    "    A = X\n",
    "    cache['A0'] = A\n",
    "    \n",
    "    for l in range(1, L + 1):\n",
    "        w = parameters[f'w{l}']\n",
    "        b = parameters[f'b{l}']\n",
    "        \n",
    "        Z = w @ A + b\n",
    "        A = activation_fn(Z)\n",
    "        \n",
    "        cache[f'Z{l}'] = Z\n",
    "        cache[f'A{l}'] = A\n",
    "    \n",
    "    return A, cache"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ad377c8",
   "metadata": {},
   "source": [
    "**Implement a for loop structure that iterates from layer 1 to layer L, computing activations sequentially for each layer in the network.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "debcfae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_propagation_loop(A, parameters, L, activation_fn):\n",
    "    \"\"\"\n",
    "    Forward propagation with explicit layer-by-layer loop.\n",
    "    \n",
    "    Args:\n",
    "        A: Activation from previous layer (or input X for first iteration)\n",
    "        parameters: Dictionary with weights and biases\n",
    "        L: Total number of layers\n",
    "        activation_fn: Activation function to apply\n",
    "    \n",
    "    Returns:\n",
    "        A: Final output activation\n",
    "        cache: Stored activations and pre-activations\n",
    "    \"\"\"\n",
    "    cache = {}\n",
    "    cache['A0'] = A\n",
    "    \n",
    "    for l in range(1, L + 1):\n",
    "        A_prev = A\n",
    "        w = parameters[f'w{l}']\n",
    "        b = parameters[f'b{l}']\n",
    "        \n",
    "        Z = w @ A_prev + b\n",
    "        A = activation_fn(Z)\n",
    "        \n",
    "        cache[f'Z{l}'] = Z\n",
    "        cache[f'A{l}'] = A\n",
    "    \n",
    "    return A, cache"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76b957d1",
   "metadata": {},
   "source": [
    "**Create a Mermaid diagram: Flowchart showing the sequential steps of forward propagation: input x (or A0) → compute z1 → compute a1 → compute z2 → compute a2 → ... → compute zL → compute aL (output).**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ae0a03e",
   "metadata": {},
   "source": [
    "## Forward Propagation Flow\n",
    "\n",
    "```mermaid\n",
    "graph TD\n",
    "    A0[\"Input: A⁰ = X\"] --> Z1[\"Compute Z¹ = w¹A⁰ + b¹\"]\n",
    "    Z1 --> A1[\"Compute A¹ = g(Z¹)\"]\n",
    "    A1 --> Z2[\"Compute Z² = w²A¹ + b²\"]\n",
    "    Z2 --> A2[\"Compute A² = g(Z²)\"]\n",
    "    A2 --> Dots[\"...\"]\n",
    "    Dots --> ZL[\"Compute Z⁽ᴸ⁾ = w⁽ᴸ⁾A⁽ᴸ⁻¹⁾ + b⁽ᴸ⁾\"]\n",
    "    ZL --> AL[\"Compute A⁽ᴸ⁾ = g(Z⁽ᴸ⁾)\"]\n",
    "    AL --> Output[\"Output: Ŷ = A⁽ᴸ⁾\"]\n",
    "```\n",
    "\n",
    "The diagram shows the sequential nature of forward propagation: each layer's computation depends on the previous layer's activation, so layers must be computed in order from layer 1 to layer $L$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d794a38b",
   "metadata": {},
   "source": [
    "## Lesson 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ce681ef",
   "metadata": {},
   "source": [
    "**Explain the core concepts of the lesson**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5081896",
   "metadata": {},
   "source": [
    "## Core Concepts: Matrix Dimensions in Deep Neural Networks\n",
    "\n",
    "Understanding matrix dimensions is fundamental to implementing neural networks correctly. The key concepts are:\n",
    "\n",
    "**Weight Matrix Dimensions**: Each layer $l$ has a weight matrix $W^{[l]}$ with dimensions $n^{[l]} \\times n^{[l-1]}$, where $n^{[l]}$ is the number of units in layer $l$ and $n^{[l-1]}$ is the number of units in the previous layer.\n",
    "\n",
    "**Bias Vector Dimensions**: The bias vector $b^{[l]}$ has dimensions $n^{[l]} \\times 1$, matching the number of units in the current layer.\n",
    "\n",
    "**Activation Vector Dimensions**: For a single example, the activation vector $a^{[l]}$ has dimensions $n^{[l]} \\times 1$. In vectorized form with $m$ training examples, it becomes $n^{[l]} \\times m$.\n",
    "\n",
    "**Layer Notation and Indexing**: Layers are indexed starting from 0 (input layer) through $L$ (output layer). The superscript notation $[l]$ denotes the layer number, while subscripts denote individual units.\n",
    "\n",
    "**Dimension Consistency Checking**: For matrix multiplication $W^{[l]} \\cdot X$ to be valid, the number of columns in $W^{[l]}$ must equal the number of rows in $X$. This is a practical debugging technique that catches implementation errors early.\n",
    "\n",
    "**Vectorized Implementation Dimensions**: When processing multiple training examples simultaneously, we stack them horizontally. The input matrix $X$ has dimensions $n^{[0]} \\times m$, where $m$ is the number of training examples.\n",
    "\n",
    "**Broadcasting in Matrix Operations**: Bias vectors automatically broadcast across all examples during addition, so a $n^{[l]} \\times 1$ bias vector adds correctly to a $n^{[l]} \\times m$ matrix.\n",
    "\n",
    "**Gradient Matrix Dimensions**: During backpropagation, gradient matrices $dW^{[l]}$ and $db^{[l]}$ maintain the same dimensions as their corresponding parameters $W^{[l]}$ and $b^{[l]}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbc3df2a",
   "metadata": {},
   "source": [
    "**Explain intuitions and mental models for the lesson**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd256a20",
   "metadata": {},
   "source": [
    "## Intuitions and Mental Models\n",
    "\n",
    "**Dimension Compatibility as a Constraint**: Think of matrix dimensions like puzzle pieces—they must fit together correctly. When multiplying $W^{[l]}$ (dimensions $n^{[l]} \\times n^{[l-1]}$) by input $X$ (dimensions $n^{[l-1]} \\times m$), the inner dimensions ($n^{[l-1]}$) must match. The result has outer dimensions $n^{[l]} \\times m$. This is not arbitrary; it's a mathematical requirement.\n",
    "\n",
    "**Layers Define Shape**: Each layer's number of units determines the shape of all associated matrices and vectors. If layer $l$ has $n^{[l]}$ units, then $W^{[l]}$ will always have $n^{[l]}$ rows, and any activation vector from that layer will always have $n^{[l]}$ rows. This consistency makes the architecture predictable.\n",
    "\n",
    "**Stacking Examples Horizontally**: In vectorized implementations, instead of processing one training example at a time, we stack $m$ examples side-by-side. Each column represents one complete example. This transforms a single example's $n^{[l]} \\times 1$ vector into a $n^{[l]} \\times m$ matrix. The computation is identical; we just do it all at once.\n",
    "\n",
    "**Dimensions as a Debugging Tool**: Before running code, trace through the dimensions at each step. If dimensions don't match at any point, you've found a bug. This \"dimension checking\" is faster than debugging runtime errors and catches logical mistakes in your implementation.\n",
    "\n",
    "**Broadcasting Handles Bias Automatically**: The bias vector $b^{[l]}$ stays $n^{[l]} \\times 1$ regardless of how many examples you process. Python's broadcasting automatically replicates it across all $m$ examples when you add it to $Z^{[l]}$. You don't need to manually expand the bias; the framework handles it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3f07bb8",
   "metadata": {},
   "source": [
    "**Present and explain the key equations used in the lesson**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "831d6ef1",
   "metadata": {},
   "source": [
    "## Key Equations\n",
    "\n",
    "**Weight Matrix Dimensions**:\n",
    "$$W^{[l]} \\text{ has dimensions } n^{[l]} \\times n^{[l-1]}$$\n",
    "\n",
    "The weight matrix for layer $l$ has $n^{[l]}$ rows (one for each unit in the current layer) and $n^{[l-1]}$ columns (one for each input from the previous layer).\n",
    "\n",
    "**Bias Vector Dimensions**:\n",
    "$$b^{[l]} \\text{ has dimensions } n^{[l]} \\times 1$$\n",
    "\n",
    "The bias vector has one entry per unit in layer $l$.\n",
    "\n",
    "**Forward Propagation Equation**:\n",
    "$$Z^{[l]} = W^{[l]} X + b^{[l]}$$\n",
    "\n",
    "This is the core computation: matrix multiplication followed by bias addition.\n",
    "\n",
    "**Single Example Dimensions**:\n",
    "$$\\text{Single example: } z^{[l]} \\text{ is } n^{[l]} \\times 1$$\n",
    "\n",
    "For one training example, the pre-activation output is a column vector.\n",
    "\n",
    "**Vectorized Dimensions**:\n",
    "$$\\text{Vectorized: } Z^{[l]} \\text{ is } n^{[l]} \\times m$$\n",
    "$$\\text{Vectorized: } X \\text{ is } n^{[0]} \\times m$$\n",
    "\n",
    "When processing $m$ training examples simultaneously, matrices have an additional dimension.\n",
    "\n",
    "**Gradient Dimensions**:\n",
    "$$dW^{[l]} \\text{ has same dimensions as } W^{[l]}$$\n",
    "$$dZ^{[l]} \\text{ has same dimensions as } Z^{[l]}$$\n",
    "\n",
    "Gradient matrices maintain the same shape as their corresponding forward pass variables, ensuring consistent backpropagation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c0980e9",
   "metadata": {},
   "source": [
    "**Implement code primitive: Implement dimension checking by working through matrix multiplication rules for each layer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14c8010f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Define network architecture\n",
    "n_layers = 3\n",
    "n_units = [3, 4, 5, 2]  # n[0]=3 (input), n[1]=4, n[2]=5, n[3]=2 (output)\n",
    "m = 10  # number of training examples\n",
    "\n",
    "# Initialize parameters\n",
    "parameters = {}\n",
    "for l in range(1, n_layers + 1):\n",
    "    parameters[f'W[{l}]'] = np.random.randn(n_units[l], n_units[l-1])\n",
    "    parameters[f'b[{l}]'] = np.random.randn(n_units[l], 1)\n",
    "\n",
    "# Check dimensions through forward propagation\n",
    "X = np.random.randn(n_units[0], m)\n",
    "print(f\"Input X dimensions: {X.shape} (expected: ({n_units[0]}, {m}))\")\n",
    "\n",
    "A = X\n",
    "for l in range(1, n_layers + 1):\n",
    "    W = parameters[f'W[{l}]']\n",
    "    b = parameters[f'b[{l}]']\n",
    "    \n",
    "    print(f\"\\nLayer {l}:\")\n",
    "    print(f\"  W[{l}] dimensions: {W.shape} (expected: ({n_units[l]}, {n_units[l-1]}))\")\n",
    "    print(f\"  b[{l}] dimensions: {b.shape} (expected: ({n_units[l]}, 1))\")\n",
    "    print(f\"  A[{l-1}] dimensions: {A.shape} (expected: ({n_units[l-1]}, {m}))\")\n",
    "    \n",
    "    Z = np.dot(W, A) + b\n",
    "    print(f\"  Z[{l}] dimensions: {Z.shape} (expected: ({n_units[l]}, {m}))\")\n",
    "    \n",
    "    A = np.maximum(0, Z)  # ReLU activation\n",
    "    print(f\"  A[{l}] dimensions: {A.shape} (expected: ({n_units[l]}, {m}))\")\n",
    "\n",
    "print(f\"\\nFinal output dimensions: {A.shape} (expected: ({n_units[n_layers]}, {m}))\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dbd2572",
   "metadata": {},
   "source": [
    "**Implement code primitive: Verify that W^[l] dimensions (n^[l] by n^[l-1]) produce correct output shapes when multiplied by input**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e763727e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Test matrix multiplication dimension rules\n",
    "n_prev = 5   # n[l-1]\n",
    "n_curr = 3   # n[l]\n",
    "m = 8        # number of examples\n",
    "\n",
    "W = np.random.randn(n_curr, n_prev)\n",
    "A_prev = np.random.randn(n_prev, m)\n",
    "\n",
    "print(f\"W dimensions: {W.shape}\")\n",
    "print(f\"A_prev dimensions: {A_prev.shape}\")\n",
    "print(f\"Expected Z dimensions: ({n_curr}, {m})\")\n",
    "\n",
    "Z = np.dot(W, A_prev)\n",
    "print(f\"Actual Z dimensions: {Z.shape}\")\n",
    "\n",
    "assert Z.shape == (n_curr, m), f\"Dimension mismatch: got {Z.shape}, expected ({n_curr}, {m})\"\n",
    "print(\"✓ Dimension check passed: W @ A_prev produces correct shape\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6d0fd97",
   "metadata": {},
   "source": [
    "**Implement code primitive: Confirm bias vector dimensions (n^[l] by 1) match the output of matrix multiplication**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afc30b37",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Test bias addition with broadcasting\n",
    "n_curr = 4   # n[l]\n",
    "m = 6        # number of examples\n",
    "\n",
    "Z = np.random.randn(n_curr, m)\n",
    "b = np.random.randn(n_curr, 1)\n",
    "\n",
    "print(f\"Z dimensions: {Z.shape}\")\n",
    "print(f\"b dimensions: {b.shape}\")\n",
    "print(f\"Expected output dimensions: ({n_curr}, {m})\")\n",
    "\n",
    "Z_biased = Z + b\n",
    "print(f\"Actual output dimensions: {Z_biased.shape}\")\n",
    "\n",
    "assert Z_biased.shape == (n_curr, m), f\"Dimension mismatch: got {Z_biased.shape}, expected ({n_curr}, {m})\"\n",
    "print(\"✓ Dimension check passed: bias broadcasts correctly across examples\")\n",
    "\n",
    "# Verify that bias is applied to each example\n",
    "for j in range(m):\n",
    "    assert np.allclose(Z_biased[:, j], Z[:, j] + b[:, 0]), f\"Bias not applied correctly to example {j}\"\n",
    "print(\"✓ Bias correctly applied to all examples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "130bb1cf",
   "metadata": {},
   "source": [
    "**Implement code primitive: Implement vectorized forward propagation with stacked training examples (m columns)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f83c04f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Vectorized forward propagation\n",
    "np.random.seed(42)\n",
    "\n",
    "# Network architecture\n",
    "n_units = [2, 3, 4, 1]  # Input: 2, Hidden: 3, Hidden: 4, Output: 1\n",
    "m = 5  # 5 training examples\n",
    "\n",
    "# Initialize parameters\n",
    "parameters = {}\n",
    "for l in range(1, len(n_units)):\n",
    "    parameters[f'W[{l}]'] = np.random.randn(n_units[l], n_units[l-1]) * 0.01\n",
    "    parameters[f'b[{l}]'] = np.zeros((n_units[l], 1))\n",
    "\n",
    "# Input: m examples, each with n[0] features\n",
    "X = np.random.randn(n_units[0], m)\n",
    "print(f\"Input X shape: {X.shape}\")\n",
    "print(f\"X represents {m} examples, each with {n_units[0]} features\\n\")\n",
    "\n",
    "# Forward propagation\n",
    "A = X\n",
    "for l in range(1, len(n_units)):\n",
    "    W = parameters[f'W[{l}]']\n",
    "    b = parameters[f'b[{l}]']\n",
    "    \n",
    "    Z = np.dot(W, A) + b\n",
    "    A = np.maximum(0, Z) if l < len(n_units) - 1 else Z  # ReLU for hidden, linear for output\n",
    "    \n",
    "    print(f\"Layer {l}:\")\n",
    "    print(f\"  Z[{l}] shape: {Z.shape}\")\n",
    "    print(f\"  A[{l}] shape: {A.shape}\")\n",
    "    print(f\"  Each column is one example's activation\\n\")\n",
    "\n",
    "print(f\"Final output shape: {A.shape}\")\n",
    "print(f\"Output has {A.shape[1]} examples (columns) and {A.shape[0]} output unit(s) (rows)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f20aa91",
   "metadata": {},
   "source": [
    "**Implement code primitive: Verify that gradient matrices (dW, db) maintain the same dimensions as their corresponding parameters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b2d25b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Verify gradient dimensions match parameter dimensions\n",
    "n_curr = 3   # n[l]\n",
    "n_prev = 4   # n[l-1]\n",
    "m = 6        # number of examples\n",
    "\n",
    "# Forward pass\n",
    "W = np.random.randn(n_curr, n_prev)\n",
    "b = np.random.randn(n_curr, 1)\n",
    "A_prev = np.random.randn(n_prev, m)\n",
    "Z = np.dot(W, A_prev) + b\n",
    "\n",
    "print(f\"Parameter dimensions:\")\n",
    "print(f\"  W shape: {W.shape}\")\n",
    "print(f\"  b shape: {b.shape}\")\n",
    "\n",
    "# Simulated gradient computation (simplified backprop)\n",
    "dZ = np.random.randn(n_curr, m)  # gradient w.r.t. Z\n",
    "\n",
    "# Gradient computation\n",
    "dW = (1/m) * np.dot(dZ, A_prev.T)\n",
    "db = (1/m) * np.sum(dZ, axis=1, keepdims=True)\n",
    "\n",
    "print(f\"\\nGradient dimensions:\")\n",
    "print(f\"  dW shape: {dW.shape}\")\n",
    "print(f\"  db shape: {db.shape}\")\n",
    "\n",
    "print(f\"\\nDimension verification:\")\n",
    "assert dW.shape == W.shape, f\"dW shape {dW.shape} != W shape {W.shape}\"\n",
    "assert db.shape == b.shape, f\"db shape {db.shape} != b shape {b.shape}\"\n",
    "print(f\"✓ dW has same dimensions as W: {dW.shape}\")\n",
    "print(f\"✓ db has same dimensions as b: {db.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dcdaead",
   "metadata": {},
   "source": [
    "**Create a Mermaid diagram: A flowchart showing the dimension transformation through layers: input (n^[0] by m) -> W^[1] multiplication -> Z^[1] (n^[1] by m) -> activation -> A^[1] (n^[1] by m) -> next layer**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a23bbdd2",
   "metadata": {},
   "source": [
    "## Dimension Flow Through Layers\n",
    "\n",
    "```mermaid\n",
    "graph TD\n",
    "    A[\"Input X<br/>(n⁰ × m)\"] --> B[\"W¹ × X<br/>W¹: n¹ × n⁰<br/>Result: n¹ × m\"]\n",
    "    B --> C[\"Z¹ = W¹X + b¹<br/>(n¹ × m)\"]\n",
    "    C --> D[\"Activation<br/>A¹ = ReLU(Z¹)<br/>(n¹ × m)\"]\n",
    "    D --> E[\"W² × A¹<br/>W²: n² × n¹<br/>Result: n² × m\"]\n",
    "    E --> F[\"Z² = W²A¹ + b²<br/>(n² × m)\"]\n",
    "    F --> G[\"Activation<br/>A² = ReLU(Z²)<br/>(n² × m)\"]\n",
    "    G --> H[\"Continue to<br/>next layer...\"]\n",
    "    \n",
    "    style A fill:#e1f5ff\n",
    "    style C fill:#fff3e0\n",
    "    style D fill:#f3e5f5\n",
    "    style F fill:#fff3e0\n",
    "    style G fill:#f3e5f5\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d9bd58f",
   "metadata": {},
   "source": [
    "**Create a Mermaid diagram: A diagram showing how a single example dimension (n^[l] by 1) extends to vectorized dimension (n^[l] by m) by stacking m examples horizontally**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c91b6fd",
   "metadata": {},
   "source": [
    "## Single Example vs. Vectorized Dimensions\n",
    "\n",
    "```mermaid\n",
    "graph TD\n",
    "    A[\"Single Example<br/>a⁽ˡ⁾: n⁽ˡ⁾ × 1\"] --> B[\"Stack m Examples<br/>Horizontally\"]\n",
    "    B --> C[\"Vectorized Form<br/>A⁽ˡ⁾: n⁽ˡ⁾ × m\"]\n",
    "    \n",
    "    D[\"Example 1<br/>Column 1\"] --> E[\"Example 2<br/>Column 2\"]\n",
    "    E --> F[\"Example 3<br/>Column 3\"]\n",
    "    F --> G[\"... Example m<br/>Column m\"]\n",
    "    \n",
    "    C --> H[\"Each column is<br/>one complete example\"]\n",
    "    \n",
    "    style A fill:#e8f5e9\n",
    "    style C fill:#e3f2fd\n",
    "    style H fill:#fff9c4\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c702d2b",
   "metadata": {},
   "source": [
    "## Lesson 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "938380a4",
   "metadata": {},
   "source": [
    "**Explain the core concepts of the lesson**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3fd358e",
   "metadata": {},
   "source": [
    "## Core Concepts\n",
    "\n",
    "Deep neural networks are effective because they learn **hierarchical representations** of data. Rather than attempting to learn complex functions directly from raw input, deep networks build sophisticated representations by composing simpler ones across multiple layers.\n",
    "\n",
    "**Key concepts:**\n",
    "\n",
    "- **Feature Hierarchy**: Early layers detect simple, low-level features (e.g., edges in images, pitch in audio). Middle layers combine these into more complex features (e.g., shapes, phonemes). Deep layers recognize high-level abstractions (e.g., objects, words).\n",
    "\n",
    "- **Compositional Representation**: Each layer builds upon the previous layer's output, creating a compositional structure where complex functions are expressed as compositions of simpler functions.\n",
    "\n",
    "- **Depth as Computational Efficiency**: Deep networks can compute certain functions with exponentially fewer neurons than shallow networks. This is a fundamental principle from circuit theory: depth dramatically reduces the number of components needed to compute complex functions.\n",
    "\n",
    "- **Simple to Complex Functions**: The progression from simple to complex features mirrors how biological systems (like the human brain) process information, making deep networks both computationally efficient and biologically plausible."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f61e5970",
   "metadata": {},
   "source": [
    "**Explain intuitions and mental models for the lesson**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf6e6617",
   "metadata": {},
   "source": [
    "## Intuitions and Mental Models\n",
    "\n",
    "**Hierarchical Processing in Vision:**\n",
    "Imagine recognizing a face in a photograph. Your visual system doesn't directly recognize \"face\" from raw pixel values. Instead, it first detects simple features like edges and corners, then combines these into parts like eyes and noses, and finally recognizes the complete face. Deep networks work the same way—early layers act like edge detectors, middle layers recognize parts, and deep layers recognize whole objects.\n",
    "\n",
    "**The XOR Problem:**\n",
    "Consider computing XOR of many binary inputs. A shallow network with a single hidden layer would need exponentially many neurons (roughly $2^N$ for $N$ inputs). But a deep network can solve this with only logarithmic depth—by organizing XOR operations in a tree structure, we reduce the problem from exponential to logarithmic complexity. This illustrates why depth is powerful: it allows us to reuse computations efficiently.\n",
    "\n",
    "**Speech Recognition Across Layers:**\n",
    "In speech processing, the same hierarchical principle applies. Raw audio waveforms are first processed to extract low-level features like pitch and tone. These combine into phonemes (basic sound units), which combine into words, which combine into phrases and sentences. Each layer adds semantic meaning by composing the outputs of the previous layer.\n",
    "\n",
    "**Composition Over Direct Learning:**\n",
    "The fundamental insight is that deep networks are effective because they learn to compose simple functions into complex ones, rather than trying to learn complex functions directly from raw input. This compositional approach is more efficient, more generalizable, and mirrors how nature solves complex problems."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ec6280e",
   "metadata": {},
   "source": [
    "**Present and explain the key equations used in the lesson**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebaa162c",
   "metadata": {},
   "source": [
    "## Key Equations\n",
    "\n",
    "**XOR Tree Depth:**\n",
    "$$\\text{XOR tree depth} = O(\\log N)$$\n",
    "\n",
    "When computing XOR of $N$ binary inputs using a tree of XOR gates, the depth (number of layers) grows logarithmically with the number of inputs. This is because each layer can combine pairs of results from the previous layer, halving the number of remaining inputs at each step.\n",
    "\n",
    "**Shallow Network Complexity for XOR:**\n",
    "$$\\text{Shallow network hidden units for XOR} = O(2^N)$$\n",
    "\n",
    "A shallow network with a single hidden layer requires exponentially many hidden units to compute XOR of $N$ inputs. This exponential blowup demonstrates why depth is essential: a deep network can solve the same problem with far fewer total neurons by leveraging the hierarchical structure of the computation.\n",
    "\n",
    "These equations illustrate the fundamental trade-off: **depth reduces the number of components needed to compute complex functions**. A shallow network must learn the entire complex function in one step, while a deep network can decompose it into simpler sub-functions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43a64956",
   "metadata": {},
   "source": [
    "**Implement code primitive: Visualize hidden unit activations in a neural network to show what features early layers detect (e.g., edge orientations in images)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db53dd19",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Rectangle\n",
    "\n",
    "# Create a simple edge detection visualization\n",
    "fig, axes = plt.subplots(2, 3, figsize=(12, 8))\n",
    "fig.suptitle('Hidden Unit Activations: Feature Detection Across Layers', fontsize=14, fontweight='bold')\n",
    "\n",
    "# Layer 1: Edge Detection (horizontal, vertical, diagonal edges)\n",
    "edge_filters = [\n",
    "    np.array([[-1, -1, -1], [2, 2, 2], [-1, -1, -1]]),  # Horizontal edge\n",
    "    np.array([[-1, 2, -1], [-1, 2, -1], [-1, 2, -1]]),  # Vertical edge\n",
    "    np.array([[2, -1, -1], [-1, 2, -1], [-1, -1, 2]])   # Diagonal edge\n",
    "]\n",
    "\n",
    "# Create a simple image with edges\n",
    "image = np.zeros((7, 7))\n",
    "image[2:5, 2:5] = 1  # White square\n",
    "\n",
    "for idx, (ax, filt) in enumerate(zip(axes[0], edge_filters)):\n",
    "    # Simple convolution\n",
    "    activation = np.zeros((5, 5))\n",
    "    for i in range(5):\n",
    "        for j in range(5):\n",
    "            activation[i, j] = np.sum(image[i:i+3, j:j+3] * filt)\n",
    "    \n",
    "    ax.imshow(activation, cmap='hot')\n",
    "    ax.set_title(f'Layer 1: Edge Detector {idx+1}')\n",
    "    ax.axis('off')\n",
    "\n",
    "# Layer 2: Corner and Shape Detection (combinations of edges)\n",
    "for idx in range(3):\n",
    "    ax = axes[1, idx]\n",
    "    # Simulate combining edge detections\n",
    "    combined = np.zeros((5, 5))\n",
    "    for i in range(5):\n",
    "        for j in range(5):\n",
    "            combined[i, j] = np.sum([np.abs(np.sum(image[i:i+3, j:j+3] * filt)) for filt in edge_filters])\n",
    "    \n",
    "    ax.imshow(combined, cmap='viridis')\n",
    "    ax.set_title(f'Layer 2: Shape Feature {idx+1}')\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Layer 1 detects simple features: edges at different orientations\")\n",
    "print(\"Layer 2 combines these edges to detect more complex shapes\")\n",
    "print(\"Deeper layers would recognize complete objects by combining shapes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebb0766c",
   "metadata": {},
   "source": [
    "**Implement or trace through an XOR tree circuit to demonstrate how depth reduces the number of required components**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f2055cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def xor(a, b):\n",
    "    \"\"\"Compute XOR of two bits\"\"\"\n",
    "    return int(a != b)\n",
    "\n",
    "def xor_tree_depth(n):\n",
    "    \"\"\"Calculate depth needed for XOR tree with n inputs\"\"\"\n",
    "    if n <= 1:\n",
    "        return 0\n",
    "    return 1 + xor_tree_depth(n // 2)\n",
    "\n",
    "def xor_tree_compute(inputs):\n",
    "    \"\"\"Compute XOR of inputs using tree structure\"\"\"\n",
    "    if len(inputs) == 1:\n",
    "        return inputs[0]\n",
    "    \n",
    "    # Pair up inputs and compute XOR\n",
    "    next_level = []\n",
    "    for i in range(0, len(inputs), 2):\n",
    "        if i + 1 < len(inputs):\n",
    "            next_level.append(xor(inputs[i], inputs[i+1]))\n",
    "        else:\n",
    "            next_level.append(inputs[i])\n",
    "    \n",
    "    return xor_tree_compute(next_level)\n",
    "\n",
    "# Demonstrate XOR tree for 8 inputs\n",
    "test_inputs = [1, 0, 1, 1, 0, 1, 0, 1]\n",
    "result = xor_tree_compute(test_inputs)\n",
    "print(f\"XOR of {test_inputs} = {result}\")\n",
    "print(f\"Tree depth for {len(test_inputs)} inputs: {xor_tree_depth(len(test_inputs))}\")\n",
    "\n",
    "# Compare shallow vs deep complexity\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Plot 1: Depth comparison\n",
    "n_values = np.arange(1, 17)\n",
    "deep_depth = [xor_tree_depth(n) for n in n_values]\n",
    "shallow_units = [2**n for n in n_values]\n",
    "\n",
    "ax1.plot(n_values, deep_depth, 'o-', label='Deep Network Depth', linewidth=2, markersize=8)\n",
    "ax1.set_xlabel('Number of Inputs (N)', fontsize=12)\n",
    "ax1.set_ylabel('Depth / Hidden Units', fontsize=12)\n",
    "ax1.set_title('Deep Network: Logarithmic Depth', fontsize=13, fontweight='bold')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "ax1.legend(fontsize=11)\n",
    "\n",
    "# Plot 2: Shallow network complexity\n",
    "ax2.semilogy(n_values, shallow_units, 's-', label='Shallow Network Hidden Units', color='red', linewidth=2, markersize=8)\n",
    "ax2.set_xlabel('Number of Inputs (N)', fontsize=12)\n",
    "ax2.set_ylabel('Hidden Units (log scale)', fontsize=12)\n",
    "ax2.set_title('Shallow Network: Exponential Growth', fontsize=13, fontweight='bold')\n",
    "ax2.grid(True, alpha=0.3, which='both')\n",
    "ax2.legend(fontsize=11)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nFor 16 inputs:\")\n",
    "print(f\"  Deep network depth: {xor_tree_depth(16)} layers\")\n",
    "print(f\"  Shallow network hidden units: {2**16} neurons\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b7a0aee",
   "metadata": {},
   "source": [
    "**Create a Mermaid diagram: flowchart TD\n",
    "    A[Raw Input Image] --> B[Layer 1: Edge Detection]\n",
    "    B --> C[Layer 2: Face Parts]\n",
    "    C --> D[Layer 3: Face Recognition]\n",
    "    style A fill:#e1f5ff\n",
    "    style B fill:#fff3e0\n",
    "    style C fill:#f3e5f5\n",
    "    style D fill:#e8f5e9**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3febd721",
   "metadata": {},
   "source": [
    "## Visual Hierarchy: Image Recognition\n",
    "\n",
    "```mermaid\n",
    "flowchart TD\n",
    "    A[Raw Input Image] --> B[Layer 1: Edge Detection]\n",
    "    B --> C[Layer 2: Face Parts]\n",
    "    C --> D[Layer 3: Face Recognition]\n",
    "    style A fill:#e1f5ff\n",
    "    style B fill:#fff3e0\n",
    "    style C fill:#f3e5f5\n",
    "    style D fill:#e8f5e9\n",
    "```\n",
    "\n",
    "This diagram shows how a deep network processes visual information hierarchically. The raw pixel input is progressively transformed into increasingly abstract representations. Early layers detect simple edges and textures, middle layers recognize meaningful parts like eyes and noses, and deep layers recognize complete objects like faces. This compositional approach allows the network to build complex visual understanding from simple building blocks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb88bf41",
   "metadata": {},
   "source": [
    "**Create a Mermaid diagram: flowchart TD\n",
    "    A[Audio Input] --> B[Layer 1: Waveform Features]\n",
    "    B --> C[Layer 2: Phonemes]\n",
    "    C --> D[Layer 3: Words]\n",
    "    D --> E[Layer 4: Phrases/Sentences]\n",
    "    style A fill:#e1f5ff\n",
    "    style B fill:#fff3e0\n",
    "    style C fill:#f3e5f5\n",
    "    style D fill:#fce4ec\n",
    "    style E fill:#e8f5e9**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "def31235",
   "metadata": {},
   "source": [
    "## Visual Hierarchy: Speech Recognition\n",
    "\n",
    "```mermaid\n",
    "flowchart TD\n",
    "    A[Audio Input] --> B[Layer 1: Waveform Features]\n",
    "    B --> C[Layer 2: Phonemes]\n",
    "    C --> D[Layer 3: Words]\n",
    "    D --> E[Layer 4: Phrases/Sentences]\n",
    "    style A fill:#e1f5ff\n",
    "    style B fill:#fff3e0\n",
    "    style C fill:#f3e5f5\n",
    "    style D fill:#fce4ec\n",
    "    style E fill:#e8f5e9\n",
    "```\n",
    "\n",
    "This diagram illustrates the same hierarchical principle applied to speech recognition. Raw audio waveforms are first processed to extract low-level acoustic features like pitch and tone. These combine into phonemes (basic sound units), which are the building blocks of language. Phonemes combine into words, and words combine into phrases and sentences. Each layer adds semantic meaning by composing the outputs of the previous layer, demonstrating that the compositional approach is universal across different domains."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfde2304",
   "metadata": {},
   "source": [
    "**Create a Mermaid diagram: flowchart TD\n",
    "    X1 --> XOR1[XOR]\n",
    "    X2 --> XOR1\n",
    "    X3 --> XOR2[XOR]\n",
    "    X4 --> XOR2\n",
    "    XOR1 --> XOR3[XOR]\n",
    "    XOR2 --> XOR3\n",
    "    XOR3 --> Y[Output]\n",
    "    style Y fill:#e8f5e9**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f651915f",
   "metadata": {},
   "source": [
    "## XOR Tree Circuit\n",
    "\n",
    "```mermaid\n",
    "flowchart TD\n",
    "    X1 --> XOR1[XOR]\n",
    "    X2 --> XOR1\n",
    "    X3 --> XOR2[XOR]\n",
    "    X4 --> XOR2\n",
    "    XOR1 --> XOR3[XOR]\n",
    "    XOR2 --> XOR3\n",
    "    XOR3 --> Y[Output]\n",
    "    style Y fill:#e8f5e9\n",
    "```\n",
    "\n",
    "This diagram shows how a tree structure of XOR gates can compute the XOR of multiple inputs with logarithmic depth. Instead of requiring exponentially many neurons in a single hidden layer, the tree organizes the computation hierarchically. Each layer combines pairs of results from the previous layer, reducing the problem size by half at each step. For $N$ inputs, this tree structure requires only $O(\\log N)$ depth, compared to $O(2^N)$ hidden units in a shallow network. This is a concrete example of how depth dramatically reduces computational complexity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4acd7d24",
   "metadata": {},
   "source": [
    "## Lesson 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "334931de",
   "metadata": {},
   "source": [
    "**Explain the core concepts of the lesson**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32b0ad5a",
   "metadata": {},
   "source": [
    "## Core Concepts\n",
    "\n",
    "A neural network is built as a sequence of layers, where each layer performs two fundamental operations:\n",
    "\n",
    "**Forward Propagation**: Each layer takes the activation from the previous layer and computes the activation for the current layer using learnable parameters (weights and biases).\n",
    "\n",
    "**Backward Propagation**: After computing the loss, gradients flow backward through the network. Each layer receives the gradient of the loss with respect to its output activation and computes gradients with respect to its inputs and parameters.\n",
    "\n",
    "**Cache Mechanism**: During forward propagation, intermediate values (like the pre-activation $z^{[l]}$) are stored in a cache. These cached values are essential during backward propagation to compute gradients efficiently without recomputation.\n",
    "\n",
    "**Layer Computation**: A single layer performs a linear transformation followed by an activation function. The linear transformation combines the previous activation with weights and biases, and the activation function introduces non-linearity.\n",
    "\n",
    "**Gradient Flow**: Gradients propagate backward through the network, allowing us to compute how much each parameter should change to reduce the loss. This enables the parameter update step in training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6214a579",
   "metadata": {},
   "source": [
    "**Explain intuitions and mental models for the lesson**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35a70e45",
   "metadata": {},
   "source": [
    "## Intuitions and Mental Models\n",
    "\n",
    "**Complementary Operations**: Each layer in a neural network has two complementary operations: forward propagation computes activations from the previous layer, and backward propagation computes gradients flowing back through the network. Think of forward propagation as the \"thinking\" phase where the network processes input, and backward propagation as the \"learning\" phase where the network adjusts based on errors.\n",
    "\n",
    "**Caching for Efficiency**: The cache stores intermediate values (like $z$) computed during forward propagation so they can be reused during backward propagation without recomputation. This is like writing down your work during a calculation so you don't have to redo it when checking your answer.\n",
    "\n",
    "**Bidirectional Flow**: Forward propagation flows left-to-right through layers, while backward propagation flows right-to-left, computing gradients for all parameters along the way. Imagine information flowing in one direction during prediction, then flowing back in the opposite direction during learning.\n",
    "\n",
    "**Complete Training Cycle**: A complete training iteration involves a full forward pass to compute predictions, followed by a full backward pass to compute all parameter gradients, then updating all parameters simultaneously. This ensures all parameters are updated based on the same loss computation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91ca06df",
   "metadata": {},
   "source": [
    "**Present and explain the key equations used in the lesson**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95d2c22e",
   "metadata": {},
   "source": [
    "## Key Equations\n",
    "\n",
    "**Linear Transformation** (pre-activation):\n",
    "$$z^{[l]} = w^{[l]} a^{[l-1]} + b^{[l]}$$\n",
    "\n",
    "where $w^{[l]}$ are the weights, $a^{[l-1]}$ is the activation from the previous layer, and $b^{[l]}$ is the bias.\n",
    "\n",
    "**Activation Function**:\n",
    "$$a^{[l]} = g(z^{[l]})$$\n",
    "\n",
    "where $g$ is an activation function (e.g., ReLU, sigmoid, tanh) that introduces non-linearity.\n",
    "\n",
    "**Parameter Update for Weights**:\n",
    "$$w^{[l]} := w^{[l]} - \\alpha dw^{[l]}$$\n",
    "\n",
    "where $\\alpha$ is the learning rate and $dw^{[l]}$ is the gradient of the loss with respect to $w^{[l]}$.\n",
    "\n",
    "**Parameter Update for Biases**:\n",
    "$$b^{[l]} := b^{[l]} - \\alpha db^{[l]}$$\n",
    "\n",
    "where $db^{[l]}$ is the gradient of the loss with respect to $b^{[l]}$.\n",
    "\n",
    "These equations form the foundation of layer-wise computation and parameter optimization in deep neural networks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "525e284d",
   "metadata": {},
   "source": [
    "**Implement code primitive: Implement a forward function that takes activation from previous layer and parameters, computes linear transformation and activation, returns output activation and cache**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45820569",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def forward_layer(A_prev, W, b, activation='relu'):\n",
    "    \"\"\"\n",
    "    Forward propagation for a single layer.\n",
    "    \n",
    "    Args:\n",
    "        A_prev: Activation from previous layer, shape (n_prev, m)\n",
    "        W: Weight matrix, shape (n_curr, n_prev)\n",
    "        b: Bias vector, shape (n_curr, 1)\n",
    "        activation: Activation function ('relu' or 'sigmoid')\n",
    "    \n",
    "    Returns:\n",
    "        A: Activation of current layer, shape (n_curr, m)\n",
    "        cache: Tuple of (A_prev, W, b, Z) for backward propagation\n",
    "    \"\"\"\n",
    "    Z = np.dot(W, A_prev) + b\n",
    "    \n",
    "    if activation == 'relu':\n",
    "        A = np.maximum(0, Z)\n",
    "    elif activation == 'sigmoid':\n",
    "        A = 1 / (1 + np.exp(-Z))\n",
    "    else:\n",
    "        raise ValueError(\"Unknown activation function\")\n",
    "    \n",
    "    cache = (A_prev, W, b, Z)\n",
    "    return A, cache"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "417abfaa",
   "metadata": {},
   "source": [
    "**Implement code primitive: Implement a backward function that takes gradient of loss with respect to current activation and cache, computes gradients with respect to previous activation and parameters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6852abc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_layer(dA, cache, activation='relu'):\n",
    "    \"\"\"\n",
    "    Backward propagation for a single layer.\n",
    "    \n",
    "    Args:\n",
    "        dA: Gradient of loss with respect to current activation, shape (n_curr, m)\n",
    "        cache: Tuple of (A_prev, W, b, Z) from forward propagation\n",
    "        activation: Activation function ('relu' or 'sigmoid')\n",
    "    \n",
    "    Returns:\n",
    "        dA_prev: Gradient with respect to previous activation, shape (n_prev, m)\n",
    "        dW: Gradient with respect to weights, shape (n_curr, n_prev)\n",
    "        db: Gradient with respect to bias, shape (n_curr, 1)\n",
    "    \"\"\"\n",
    "    A_prev, W, b, Z = cache\n",
    "    m = A_prev.shape[1]\n",
    "    \n",
    "    if activation == 'relu':\n",
    "        dZ = dA * (Z > 0)\n",
    "    elif activation == 'sigmoid':\n",
    "        S = 1 / (1 + np.exp(-Z))\n",
    "        dZ = dA * S * (1 - S)\n",
    "    else:\n",
    "        raise ValueError(\"Unknown activation function\")\n",
    "    \n",
    "    dW = np.dot(dZ, A_prev.T) / m\n",
    "    db = np.sum(dZ, axis=1, keepdims=True) / m\n",
    "    dA_prev = np.dot(W.T, dZ)\n",
    "    \n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e883139",
   "metadata": {},
   "source": [
    "**Implement code primitive: Implement a full forward propagation loop that sequentially applies forward functions through all layers, storing caches at each step**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3170b5c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_propagation(X, parameters, activations):\n",
    "    \"\"\"\n",
    "    Forward propagation through all layers.\n",
    "    \n",
    "    Args:\n",
    "        X: Input data, shape (n_0, m)\n",
    "        parameters: Dictionary with keys 'W1', 'b1', 'W2', 'b2', ..., 'WL', 'bL'\n",
    "        activations: List of activation functions for each layer\n",
    "    \n",
    "    Returns:\n",
    "        AL: Output activation from final layer, shape (n_L, m)\n",
    "        caches: List of caches from each layer\n",
    "    \"\"\"\n",
    "    caches = []\n",
    "    A = X\n",
    "    L = len(activations)\n",
    "    \n",
    "    for l in range(1, L + 1):\n",
    "        A_prev = A\n",
    "        W = parameters[f'W{l}']\n",
    "        b = parameters[f'b{l}']\n",
    "        activation = activations[l - 1]\n",
    "        \n",
    "        A, cache = forward_layer(A_prev, W, b, activation)\n",
    "        caches.append(cache)\n",
    "    \n",
    "    return A, caches"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64ac6366",
   "metadata": {},
   "source": [
    "**Implement code primitive: Implement a full backward propagation loop that sequentially applies backward functions in reverse order, accumulating parameter gradients**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a9791af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_propagation(dAL, caches, activations):\n",
    "    \"\"\"\n",
    "    Backward propagation through all layers.\n",
    "    \n",
    "    Args:\n",
    "        dAL: Gradient of loss with respect to final activation, shape (n_L, m)\n",
    "        caches: List of caches from forward propagation\n",
    "        activations: List of activation functions for each layer\n",
    "    \n",
    "    Returns:\n",
    "        gradients: Dictionary with keys 'dW1', 'db1', 'dW2', 'db2', ..., 'dWL', 'dbL'\n",
    "    \"\"\"\n",
    "    gradients = {}\n",
    "    dA = dAL\n",
    "    L = len(caches)\n",
    "    \n",
    "    for l in reversed(range(1, L + 1)):\n",
    "        cache = caches[l - 1]\n",
    "        activation = activations[l - 1]\n",
    "        \n",
    "        dA_prev, dW, db = backward_layer(dA, cache, activation)\n",
    "        \n",
    "        gradients[f'dW{l}'] = dW\n",
    "        gradients[f'db{l}'] = db\n",
    "        dA = dA_prev\n",
    "    \n",
    "    return gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05dbb147",
   "metadata": {},
   "source": [
    "**Create a Mermaid diagram: flowchart showing forward propagation pipeline: input a0 flows through layer 1 (using w1, b1) to produce a1, then through layer 2 (using w2, b2) to produce a2, continuing to layer L producing aL as output, with z values cached at each step**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98cc572d",
   "metadata": {},
   "source": [
    "## Forward Propagation Pipeline\n",
    "\n",
    "```mermaid\n",
    "graph LR\n",
    "    A0[\"a⁽⁰⁾<br/>(Input)\"] --> L1[\"Layer 1<br/>W¹, b¹\"]\n",
    "    L1 --> Z1[\"z⁽¹⁾<br/>(Cache)\"]\n",
    "    Z1 --> Act1[\"g(z⁽¹⁾)\"]\n",
    "    Act1 --> A1[\"a⁽¹⁾\"]\n",
    "    \n",
    "    A1 --> L2[\"Layer 2<br/>W², b²\"]\n",
    "    L2 --> Z2[\"z⁽²⁾<br/>(Cache)\"]\n",
    "    Z2 --> Act2[\"g(z⁽²⁾)\"]\n",
    "    Act2 --> A2[\"a⁽²⁾\"]\n",
    "    \n",
    "    A2 --> Dots[\"...\"]\n",
    "    Dots --> LL[\"Layer L<br/>Wᴸ, bᴸ\"]\n",
    "    LL --> ZL[\"z⁽ᴸ⁾<br/>(Cache)\"]\n",
    "    ZL --> ActL[\"g(z⁽ᴸ⁾)\"]\n",
    "    ActL --> AL[\"aᴸ<br/>(Output)\"]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f6078d9",
   "metadata": {},
   "source": [
    "**Create a Mermaid diagram: flowchart showing backward propagation pipeline: starting from dL (gradient of loss), flowing backward through layer L to compute dz, dw, db and da(L-1), then continuing backward through previous layers in reverse order**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdb5add0",
   "metadata": {},
   "source": [
    "## Backward Propagation Pipeline\n",
    "\n",
    "```mermaid\n",
    "graph RL\n",
    "    DL[\"daᴸ<br/>(Loss Gradient)\"] --> BL[\"Layer L<br/>Backward\"]\n",
    "    BL --> DZL[\"dz⁽ᴸ⁾\"]\n",
    "    BL --> DWL[\"dWᴸ\"]\n",
    "    BL --> DBL[\"dbᴸ\"]\n",
    "    DZL --> DAL_1[\"da⁽ᴸ⁻¹⁾\"]\n",
    "    \n",
    "    DAL_1 --> BL_1[\"Layer L-1<br/>Backward\"]\n",
    "    BL_1 --> DZL_1[\"dz⁽ᴸ⁻¹⁾\"]\n",
    "    BL_1 --> DWL_1[\"dWᴸ⁻¹\"]\n",
    "    BL_1 --> DBL_1[\"dbᴸ⁻¹\"]\n",
    "    DZL_1 --> DAL_2[\"da⁽ᴸ⁻²⁾\"]\n",
    "    \n",
    "    DAL_2 --> Dots[\"...\"]\n",
    "    Dots --> B1[\"Layer 1<br/>Backward\"]\n",
    "    B1 --> DZ1[\"dz⁽¹⁾\"]\n",
    "    B1 --> DW1[\"dW¹\"]\n",
    "    B1 --> DB1[\"db¹\"]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eb6ac05",
   "metadata": {},
   "source": [
    "## Lesson 6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc01f986",
   "metadata": {},
   "source": [
    "**Explain the core concepts of the lesson**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4947276",
   "metadata": {},
   "source": [
    "## Core Concepts\n",
    "\n",
    "Forward and backward propagation are the fundamental mechanisms for training neural networks. They work together to compute predictions and update model parameters.\n",
    "\n",
    "**Forward Propagation** is the process of computing layer outputs sequentially from input to output. For each layer $l$, we compute:\n",
    "- The pre-activation $z^l = W^l a^{l-1} + b^l$\n",
    "- The activation $a^l = g(z^l)$ where $g$ is an activation function\n",
    "\n",
    "During forward propagation, we cache intermediate values (activations, pre-activations, weights, and biases) for use in the backward pass.\n",
    "\n",
    "**Backward Propagation** is the process of computing gradients of the loss with respect to all parameters and activations. It flows from the output layer back to the input layer, using the cached values from the forward pass. For each layer, we compute:\n",
    "- The gradient with respect to pre-activation: $dz^l = da^l \\odot g'(z^l)$\n",
    "- The gradient with respect to weights: $dW^l = dz^l (a^{l-1})^T$\n",
    "- The gradient with respect to bias: $db^l = dz^l$\n",
    "- The gradient with respect to previous activation: $da^{l-1} = (W^l)^T dz^l$\n",
    "\n",
    "**Cache Storage** enables efficient computation by storing values computed during the forward pass so they can be reused during backpropagation without recomputation.\n",
    "\n",
    "**Vectorization** allows us to process entire batches of examples simultaneously using matrix operations, making training efficient."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e722f8b8",
   "metadata": {},
   "source": [
    "**Explain intuitions and mental models for the lesson**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a59ba42",
   "metadata": {},
   "source": [
    "## Intuitions and Mental Models\n",
    "\n",
    "**Forward Pass as Information Flow**: Think of forward propagation as information flowing left-to-right through the network. Each layer transforms its input through a linear transformation (weights and bias) followed by a nonlinear activation. The output of one layer becomes the input to the next.\n",
    "\n",
    "**Backward Pass as Error Attribution**: Backward propagation computes how much each parameter contributed to the final error. The gradient flows right-to-left, telling us how to adjust each weight and bias to reduce the loss. The chain rule connects gradients across layers.\n",
    "\n",
    "**Mirrored Structure**: The backward pass mirrors the forward pass structure but in reverse. Where forward propagation applies $W^l$ to $a^{l-1}$, backward propagation applies $(W^l)^T$ to $dz^l$. Where forward propagation applies activation function $g$, backward propagation applies its derivative $g'$.\n",
    "\n",
    "**Caching as Efficiency**: During forward propagation, we store intermediate values (z, a, W, b) at each layer. During backward propagation, we retrieve these cached values to compute gradients. This avoids recomputing values and makes backpropagation efficient.\n",
    "\n",
    "**Loss Derivative as Initialization**: The backward recursion must start somewhere. For binary classification with cross-entropy loss, the derivative of the loss with respect to the final activation $a^L$ is $da^L = -\\frac{y}{a} + \\frac{1-y}{1-a}$. This initializes the backward chain."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9425bf62",
   "metadata": {},
   "source": [
    "**Present and explain the key equations used in the lesson**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "660fefac",
   "metadata": {},
   "source": [
    "## Key Equations\n",
    "\n",
    "**Forward Propagation**:\n",
    "\n",
    "$$z^l = W^l a^{l-1} + b^l$$\n",
    "\n",
    "$$a^l = g(z^l)$$\n",
    "\n",
    "where $g$ is an activation function (ReLU, sigmoid, etc.) and $a^0 = x$ is the input.\n",
    "\n",
    "**Backward Propagation - Gradient with respect to pre-activation**:\n",
    "\n",
    "$$dz^l = da^l \\odot g'(z^l)$$\n",
    "\n",
    "where $\\odot$ denotes element-wise multiplication and $g'$ is the derivative of the activation function.\n",
    "\n",
    "**Backward Propagation - Gradients with respect to parameters**:\n",
    "\n",
    "$$dW^l = dz^l (a^{l-1})^T$$\n",
    "\n",
    "$$db^l = dz^l$$\n",
    "\n",
    "**Backward Propagation - Gradient with respect to previous activation**:\n",
    "\n",
    "$$da^{l-1} = (W^l)^T dz^l$$\n",
    "\n",
    "**Backward Propagation - Recursive gradient computation**:\n",
    "\n",
    "$$dz^l = (W^{l+1})^T dz^{l+1} \\odot g'(z^l)$$\n",
    "\n",
    "**Loss Derivative for Binary Classification**:\n",
    "\n",
    "$$da^L = -\\frac{y}{a} + \\frac{1-y}{1-a}$$\n",
    "\n",
    "where $y$ is the true label and $a$ is the predicted probability from the sigmoid output."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c413ff5",
   "metadata": {},
   "source": [
    "**Implement code primitive: Implement forward propagation function that takes a^{l-1} and returns a^l and cache (z^l, W^l, b^l)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07d79b03",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def forward_propagation(a_prev, W, b, activation='relu'):\n",
    "    \"\"\"\n",
    "    Forward propagation for a single layer.\n",
    "    \n",
    "    Args:\n",
    "        a_prev: Activation from previous layer\n",
    "        W: Weight matrix\n",
    "        b: Bias vector\n",
    "        activation: Activation function ('relu' or 'sigmoid')\n",
    "    \n",
    "    Returns:\n",
    "        a: Activation output\n",
    "        cache: Tuple of (z, W, b, a_prev) for backward pass\n",
    "    \"\"\"\n",
    "    z = np.dot(W, a_prev) + b\n",
    "    \n",
    "    if activation == 'relu':\n",
    "        a = np.maximum(0, z)\n",
    "    elif activation == 'sigmoid':\n",
    "        a = 1 / (1 + np.exp(-z))\n",
    "    else:\n",
    "        raise ValueError(\"Unknown activation function\")\n",
    "    \n",
    "    cache = (z, W, b, a_prev)\n",
    "    return a, cache"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e689700",
   "metadata": {},
   "source": [
    "**Implement code primitive: Implement backward propagation function that takes da^l and returns da^{l-1}, dW^l, db^l using cached values**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dbcd7b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_propagation(da, cache, activation='relu'):\n",
    "    \"\"\"\n",
    "    Backward propagation for a single layer.\n",
    "    \n",
    "    Args:\n",
    "        da: Gradient of loss with respect to activation\n",
    "        cache: Tuple of (z, W, b, a_prev) from forward pass\n",
    "        activation: Activation function ('relu' or 'sigmoid')\n",
    "    \n",
    "    Returns:\n",
    "        da_prev: Gradient with respect to previous activation\n",
    "        dW: Gradient with respect to weights\n",
    "        db: Gradient with respect to bias\n",
    "    \"\"\"\n",
    "    z, W, b, a_prev = cache\n",
    "    \n",
    "    if activation == 'relu':\n",
    "        dz = da * (z > 0)\n",
    "    elif activation == 'sigmoid':\n",
    "        s = 1 / (1 + np.exp(-z))\n",
    "        dz = da * s * (1 - s)\n",
    "    else:\n",
    "        raise ValueError(\"Unknown activation function\")\n",
    "    \n",
    "    dW = np.dot(dz, a_prev.T)\n",
    "    db = dz\n",
    "    da_prev = np.dot(W.T, dz)\n",
    "    \n",
    "    return da_prev, dW, db"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc9ce50d",
   "metadata": {},
   "source": [
    "**Implement code primitive: Vectorize forward pass using matrix multiplication and broadcasting for batch processing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4503ab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_propagation_batch(A_prev, W, b, activation='relu'):\n",
    "    \"\"\"\n",
    "    Vectorized forward propagation for batch of examples.\n",
    "    \n",
    "    Args:\n",
    "        A_prev: Activation matrix (n_prev, m) where m is batch size\n",
    "        W: Weight matrix (n, n_prev)\n",
    "        b: Bias vector (n, 1)\n",
    "        activation: Activation function ('relu' or 'sigmoid')\n",
    "    \n",
    "    Returns:\n",
    "        A: Activation output (n, m)\n",
    "        cache: Tuple of (Z, W, b, A_prev) for backward pass\n",
    "    \"\"\"\n",
    "    Z = np.dot(W, A_prev) + b\n",
    "    \n",
    "    if activation == 'relu':\n",
    "        A = np.maximum(0, Z)\n",
    "    elif activation == 'sigmoid':\n",
    "        A = 1 / (1 + np.exp(-Z))\n",
    "    else:\n",
    "        raise ValueError(\"Unknown activation function\")\n",
    "    \n",
    "    cache = (Z, W, b, A_prev)\n",
    "    return A, cache"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36a3d29f",
   "metadata": {},
   "source": [
    "**Implement code primitive: Vectorize backward pass using matrix operations and np.sum with axis and keepdims parameters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d08235d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_propagation_batch(dA, cache, activation='relu'):\n",
    "    \"\"\"\n",
    "    Vectorized backward propagation for batch of examples.\n",
    "    \n",
    "    Args:\n",
    "        dA: Gradient matrix (n, m) where m is batch size\n",
    "        cache: Tuple of (Z, W, b, A_prev) from forward pass\n",
    "        activation: Activation function ('relu' or 'sigmoid')\n",
    "    \n",
    "    Returns:\n",
    "        dA_prev: Gradient with respect to previous activation (n_prev, m)\n",
    "        dW: Gradient with respect to weights (n, n_prev)\n",
    "        db: Gradient with respect to bias (n, 1)\n",
    "    \"\"\"\n",
    "    Z, W, b, A_prev = cache\n",
    "    m = A_prev.shape[1]\n",
    "    \n",
    "    if activation == 'relu':\n",
    "        dZ = dA * (Z > 0)\n",
    "    elif activation == 'sigmoid':\n",
    "        S = 1 / (1 + np.exp(-Z))\n",
    "        dZ = dA * S * (1 - S)\n",
    "    else:\n",
    "        raise ValueError(\"Unknown activation function\")\n",
    "    \n",
    "    dW = np.dot(dZ, A_prev.T) / m\n",
    "    db = np.sum(dZ, axis=1, keepdims=True) / m\n",
    "    dA_prev = np.dot(W.T, dZ)\n",
    "    \n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdd43ae4",
   "metadata": {},
   "source": [
    "**Implement code primitive: Initialize backward recursion with loss derivative for final layer in binary classification**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ab0139b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_derivative_binary_classification(Y, A):\n",
    "    \"\"\"\n",
    "    Compute derivative of binary cross-entropy loss with respect to final activation.\n",
    "    \n",
    "    Args:\n",
    "        Y: True labels (1, m) where m is batch size\n",
    "        A: Predicted probabilities (1, m)\n",
    "    \n",
    "    Returns:\n",
    "        dA: Gradient of loss with respect to A (1, m)\n",
    "    \"\"\"\n",
    "    dA = -(Y / A) + (1 - Y) / (1 - A)\n",
    "    return dA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9e0f04f",
   "metadata": {},
   "source": [
    "**Implement code primitive: Chain forward functions left-to-right starting with a^0 = x**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "885af2e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_pass_network(X, parameters, activations):\n",
    "    \"\"\"\n",
    "    Forward pass through entire network.\n",
    "    \n",
    "    Args:\n",
    "        X: Input data (n_0, m) where m is batch size\n",
    "        parameters: Dictionary with keys 'W1', 'b1', 'W2', 'b2', ...\n",
    "        activations: List of activation functions for each layer\n",
    "    \n",
    "    Returns:\n",
    "        A: Final output\n",
    "        caches: List of caches from each layer\n",
    "    \"\"\"\n",
    "    A = X\n",
    "    caches = []\n",
    "    L = len(activations)\n",
    "    \n",
    "    for l in range(L):\n",
    "        A_prev = A\n",
    "        W = parameters[f'W{l+1}']\n",
    "        b = parameters[f'b{l+1}']\n",
    "        A, cache = forward_propagation_batch(A_prev, W, b, activation=activations[l])\n",
    "        caches.append(cache)\n",
    "    \n",
    "    return A, caches"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d7c6649",
   "metadata": {},
   "source": [
    "**Implement code primitive: Chain backward functions right-to-left starting with da^L from loss derivative**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6848d66",
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_pass_network(Y, A_final, caches, activations):\n",
    "    \"\"\"\n",
    "    Backward pass through entire network.\n",
    "    \n",
    "    Args:\n",
    "        Y: True labels (1, m)\n",
    "        A_final: Final activation output (1, m)\n",
    "        caches: List of caches from forward pass\n",
    "        activations: List of activation functions for each layer\n",
    "    \n",
    "    Returns:\n",
    "        gradients: Dictionary with keys 'dW1', 'db1', 'dW2', 'db2', ...\n",
    "    \"\"\"\n",
    "    gradients = {}\n",
    "    L = len(caches)\n",
    "    \n",
    "    dA = loss_derivative_binary_classification(Y, A_final)\n",
    "    \n",
    "    for l in reversed(range(L)):\n",
    "        cache = caches[l]\n",
    "        dA, dW, db = backward_propagation_batch(dA, cache, activation=activations[l])\n",
    "        gradients[f'dW{l+1}'] = dW\n",
    "        gradients[f'db{l+1}'] = db\n",
    "    \n",
    "    return gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "131e2e44",
   "metadata": {},
   "source": [
    "**Create a Mermaid diagram: flowchart showing forward propagation chain: input x → layer 1 (ReLU) → layer 2 (ReLU) → layer 3 (sigmoid) → output y-hat → loss computation**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0df6f146",
   "metadata": {},
   "source": [
    "## Forward Propagation Flow\n",
    "\n",
    "```mermaid\n",
    "graph TD\n",
    "    A[\"Input x<br/>(a⁰)\"] --> B[\"Layer 1<br/>z¹ = W¹a⁰ + b¹<br/>a¹ = ReLU(z¹)\"]\n",
    "    B --> C[\"Layer 2<br/>z² = W²a¹ + b²<br/>a² = ReLU(z²)\"]\n",
    "    C --> D[\"Layer 3<br/>z³ = W³a² + b³<br/>a³ = sigmoid(z³)\"]\n",
    "    D --> E[\"Output ŷ<br/>(a³)\"]\n",
    "    E --> F[\"Loss Computation<br/>L = -y log(ŷ) - (1-y) log(1-ŷ)\"]\n",
    "    \n",
    "    style A fill:#e1f5ff\n",
    "    style B fill:#fff3e0\n",
    "    style C fill:#fff3e0\n",
    "    style D fill:#f3e5f5\n",
    "    style E fill:#e1f5ff\n",
    "    style F fill:#ffebee\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba56e722",
   "metadata": {},
   "source": [
    "**Create a Mermaid diagram: flowchart showing backward propagation chain: loss → da^L computation → backprop layer 3 → backprop layer 2 → backprop layer 1, with cached z values flowing into each backward step**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecba9cbe",
   "metadata": {},
   "source": [
    "## Backward Propagation Flow\n",
    "\n",
    "```mermaid\n",
    "graph TD\n",
    "    A[\"Loss L\"] --> B[\"Compute dA³<br/>dA³ = -y/a³ + (1-y)/(1-a³)\"]\n",
    "    B --> C[\"Backward Layer 3<br/>dz³ = dA³ ⊙ sigmoid'(z³)<br/>dW³ = dz³(a²)ᵀ<br/>db³ = dz³<br/>dA² = (W³)ᵀ dz³\"]\n",
    "    C --> D[\"Backward Layer 2<br/>dz² = dA² ⊙ ReLU'(z²)<br/>dW² = dz²(a¹)ᵀ<br/>db² = dz²<br/>dA¹ = (W²)ᵀ dz²\"]\n",
    "    D --> E[\"Backward Layer 1<br/>dz¹ = dA¹ ⊙ ReLU'(z¹)<br/>dW¹ = dz¹(a⁰)ᵀ<br/>db¹ = dz¹\"]\n",
    "    \n",
    "    F[\"Cache: z³, W³, b³, a²\"] -.-> C\n",
    "    G[\"Cache: z², W², b², a¹\"] -.-> D\n",
    "    H[\"Cache: z¹, W¹, b¹, a⁰\"] -.-> E\n",
    "    \n",
    "    style A fill:#ffebee\n",
    "    style B fill:#f3e5f5\n",
    "    style C fill:#fff3e0\n",
    "    style D fill:#fff3e0\n",
    "    style E fill:#fff3e0\n",
    "    style F fill:#e0f2f1\n",
    "    style G fill:#e0f2f1\n",
    "    style H fill:#e0f2f1\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93f01f20",
   "metadata": {},
   "source": [
    "## Lesson 7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77055cdb",
   "metadata": {},
   "source": [
    "**Explain the core concepts of the lesson**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b464aa0",
   "metadata": {},
   "source": [
    "## Core Concepts\n",
    "\n",
    "**Hyperparameters** are settings you control that determine how the actual parameters (weights $W$ and biases $B$) evolve during training. Unlike parameters, which are learned from data, hyperparameters are chosen by the practitioner before training begins.\n",
    "\n",
    "Key hyperparameters in deep learning include:\n",
    "\n",
    "- **Learning Rate** ($\\alpha$): Controls the step size in gradient descent. A small learning rate leads to slow convergence, while a large learning rate can cause the cost function to diverge.\n",
    "- **Number of Hidden Layers**: Determines the depth of the network architecture.\n",
    "- **Number of Hidden Units**: Determines the width of each hidden layer.\n",
    "- **Activation Function Selection**: The choice of activation function (ReLU, sigmoid, tanh, etc.) for hidden layers.\n",
    "\n",
    "**Hyperparameter Tuning** is the empirical process of selecting optimal hyperparameter values. Unlike parameter learning, there is no closed-form formula to predict the best hyperparameter values in advance. Instead, practitioners must test different combinations and evaluate their impact on the cost function $J(W, B)$ and overall model performance.\n",
    "\n",
    "**Cost Function Convergence** refers to how the loss decreases during training. Observing convergence behavior helps identify whether hyperparameters are appropriate for the problem at hand."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4846c36b",
   "metadata": {},
   "source": [
    "**Explain intuitions and mental models for the lesson**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2505c6d9",
   "metadata": {},
   "source": [
    "## Intuitions and Mental Models\n",
    "\n",
    "**The Learning Rate as a Step Size**: Think of gradient descent as walking down a hill. The learning rate $\\alpha$ determines how large each step you take is. If $\\alpha$ is too small, you move very slowly and may never reach the bottom in reasonable time. If $\\alpha$ is too large, you might overshoot the bottom and bounce around erratically—or even climb back up the hill.\n",
    "\n",
    "**Hyperparameters Control Parameter Evolution**: Hyperparameters are the \"knobs and dials\" you adjust to control how the actual parameters $W$ and $B$ change during training. They don't directly determine the final model; instead, they determine the process by which the model learns.\n",
    "\n",
    "**Empirical Testing is Necessary**: Deep learning does not have a universal formula for optimal hyperparameters. What works well for one problem may fail for another. This is why empirical testing—trying different values and observing results—is fundamental to the practice of deep learning.\n",
    "\n",
    "**Hyperparameter Transfer is Limited**: Hyperparameters that work well in one domain or application may not transfer to another. A learning rate that converges quickly for image classification might be too aggressive for natural language processing. This domain transfer challenge means you often need to re-tune hyperparameters when moving to new problems.\n",
    "\n",
    "**Hyperparameters Evolve Over Time**: As you work on a problem over time, the optimal hyperparameter values may change due to infrastructure changes, problem evolution, or shifts in the dataset. What was optimal last month may not be optimal today."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffaf67dc",
   "metadata": {},
   "source": [
    "**Present and explain the key equations used in the lesson**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f5f8c57",
   "metadata": {},
   "source": [
    "## Key Equations\n",
    "\n",
    "**Learning Rate**\n",
    "\n",
    "$$\\alpha \\text{ (learning rate)}$$\n",
    "\n",
    "The learning rate is a hyperparameter that scales the gradient step in the gradient descent update rule. It directly controls the magnitude of parameter updates at each iteration.\n",
    "\n",
    "**Cost Function**\n",
    "\n",
    "$$J(W, B) \\text{ (cost function)}$$\n",
    "\n",
    "The cost function measures the error of the model given parameters $W$ and $B$. During training, we use gradient descent to minimize $J(W, B)$ by iteratively updating the parameters. The trajectory of $J(W, B)$ over iterations reveals whether the learning rate and other hyperparameters are appropriate: the cost should generally decrease smoothly toward a minimum."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18b436bb",
   "metadata": {},
   "source": [
    "**Implement code primitive: Implement a hyperparameter search loop that tries different learning rate values and observes cost function behavior**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dd7bc80",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def simple_gradient_descent(X, y, learning_rate, iterations=100):\n",
    "    m = X.shape[0]\n",
    "    w = np.zeros(X.shape[1])\n",
    "    b = 0\n",
    "    cost_history = []\n",
    "    \n",
    "    for i in range(iterations):\n",
    "        predictions = X @ w + b\n",
    "        errors = predictions - y\n",
    "        cost = np.mean(errors ** 2)\n",
    "        cost_history.append(cost)\n",
    "        \n",
    "        dw = (2 / m) * (X.T @ errors)\n",
    "        db = (2 / m) * np.sum(errors)\n",
    "        \n",
    "        w -= learning_rate * dw\n",
    "        b -= learning_rate * db\n",
    "    \n",
    "    return w, b, cost_history\n",
    "\n",
    "np.random.seed(42)\n",
    "X = np.random.randn(100, 5)\n",
    "y = np.random.randn(100)\n",
    "\n",
    "learning_rates = [0.001, 0.01, 0.1, 0.5]\n",
    "results = {}\n",
    "\n",
    "for lr in learning_rates:\n",
    "    w, b, cost_history = simple_gradient_descent(X, y, lr, iterations=100)\n",
    "    results[lr] = cost_history\n",
    "\n",
    "for lr, costs in results.items():\n",
    "    plt.plot(costs, label=f'α = {lr}')\n",
    "\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Cost Function J(W, B)')\n",
    "plt.legend()\n",
    "plt.title('Cost Function Behavior for Different Learning Rates')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1e6d739",
   "metadata": {},
   "source": [
    "**Implement code primitive: Compare cost function trajectories for different learning rate settings to identify convergence speed and stability**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f7f9710",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_convergence(results):\n",
    "    convergence_metrics = {}\n",
    "    \n",
    "    for lr, cost_history in results.items():\n",
    "        final_cost = cost_history[-1]\n",
    "        initial_cost = cost_history[0]\n",
    "        cost_reduction = initial_cost - final_cost\n",
    "        \n",
    "        iterations_to_threshold = None\n",
    "        threshold = initial_cost * 0.1\n",
    "        for i, cost in enumerate(cost_history):\n",
    "            if cost < threshold:\n",
    "                iterations_to_threshold = i\n",
    "                break\n",
    "        \n",
    "        stability = np.std(np.diff(cost_history))\n",
    "        \n",
    "        convergence_metrics[lr] = {\n",
    "            'final_cost': final_cost,\n",
    "            'cost_reduction': cost_reduction,\n",
    "            'iterations_to_threshold': iterations_to_threshold,\n",
    "            'stability': stability\n",
    "        }\n",
    "    \n",
    "    return convergence_metrics\n",
    "\n",
    "metrics = analyze_convergence(results)\n",
    "\n",
    "for lr, metric in metrics.items():\n",
    "    print(f\"Learning Rate α = {lr}:\")\n",
    "    print(f\"  Final Cost: {metric['final_cost']:.4f}\")\n",
    "    print(f\"  Cost Reduction: {metric['cost_reduction']:.4f}\")\n",
    "    print(f\"  Iterations to 10% of Initial: {metric['iterations_to_threshold']}\")\n",
    "    print(f\"  Stability (std of cost changes): {metric['stability']:.6f}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b1185db",
   "metadata": {},
   "source": [
    "**Implement code primitive: Systematically test different numbers of hidden layers and hidden units to evaluate their impact on model performance**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bea7b8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_neural_network(X, y, hidden_layers, hidden_units, learning_rate=0.01, iterations=50):\n",
    "    m = X.shape[0]\n",
    "    input_size = X.shape[1]\n",
    "    \n",
    "    layers = [input_size] + [hidden_units] * hidden_layers + [1]\n",
    "    weights = [np.random.randn(layers[i], layers[i+1]) * 0.01 for i in range(len(layers)-1)]\n",
    "    biases = [np.zeros((1, layers[i+1])) for i in range(len(layers)-1)]\n",
    "    \n",
    "    cost_history = []\n",
    "    \n",
    "    for iteration in range(iterations):\n",
    "        a = X\n",
    "        activations = [a]\n",
    "        \n",
    "        for w, b in zip(weights, biases):\n",
    "            z = a @ w + b\n",
    "            a = np.maximum(0, z)\n",
    "            activations.append(a)\n",
    "        \n",
    "        output = activations[-1]\n",
    "        cost = np.mean((output - y.reshape(-1, 1)) ** 2)\n",
    "        cost_history.append(cost)\n",
    "        \n",
    "        delta = (output - y.reshape(-1, 1)) * 2 / m\n",
    "        \n",
    "        for i in range(len(weights) - 1, -1, -1):\n",
    "            dw = activations[i].T @ delta\n",
    "            db = np.sum(delta, axis=0, keepdims=True)\n",
    "            \n",
    "            weights[i] -= learning_rate * dw\n",
    "            biases[i] -= learning_rate * db\n",
    "            \n",
    "            if i > 0:\n",
    "                delta = (delta @ weights[i].T) * (activations[i] > 0)\n",
    "    \n",
    "    return cost_history\n",
    "\n",
    "np.random.seed(42)\n",
    "X = np.random.randn(100, 5)\n",
    "y = np.random.randn(100)\n",
    "\n",
    "architectures = [\n",
    "    (1, 10),\n",
    "    (2, 10),\n",
    "    (2, 20),\n",
    "    (3, 20),\n",
    "]\n",
    "\n",
    "architecture_results = {}\n",
    "\n",
    "for hidden_layers, hidden_units in architectures:\n",
    "    cost_history = simple_neural_network(X, y, hidden_layers, hidden_units, learning_rate=0.01, iterations=50)\n",
    "    architecture_results[(hidden_layers, hidden_units)] = cost_history\n",
    "\n",
    "for (layers, units), costs in architecture_results.items():\n",
    "    plt.plot(costs, label=f'{layers} layers, {units} units')\n",
    "\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Cost Function J(W, B)')\n",
    "plt.legend()\n",
    "plt.title('Cost Function for Different Network Architectures')\n",
    "plt.show()\n",
    "\n",
    "for (layers, units), costs in architecture_results.items():\n",
    "    print(f\"Architecture: {layers} hidden layers, {units} units per layer\")\n",
    "    print(f\"  Final Cost: {costs[-1]:.4f}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2413b42d",
   "metadata": {},
   "source": [
    "**Create a Mermaid diagram: Flowchart showing the iterative hyperparameter tuning cycle: propose values → implement → evaluate cost function → adjust → repeat**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "945612c3",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning Cycle\n",
    "\n",
    "```mermaid\n",
    "graph TD\n",
    "    A[\"Propose Hyperparameter Values<br/>(α, layers, units, activation)\"] --> B[\"Implement Model<br/>(Build network architecture)\"]\n",
    "    B --> C[\"Train Model<br/>(Run gradient descent)\"]\n",
    "    C --> D[\"Evaluate Cost Function<br/>(Observe J(W, B) trajectory)\"]\n",
    "    D --> E{\"Convergence<br/>Satisfactory?\"}\n",
    "    E -->|No| F[\"Adjust Hyperparameters<br/>(Increase/decrease α, change architecture)\"]\n",
    "    F --> A\n",
    "    E -->|Yes| G[\"Accept Hyperparameters<br/>(Deploy or test on validation set)\"]\n",
    "```\n",
    "\n",
    "This cycle reflects the empirical nature of deep learning. Since there is no formula to predict optimal hyperparameters in advance, practitioners iterate through this loop, proposing values, implementing and training the model, evaluating the cost function behavior, and adjusting based on observations. The process repeats until satisfactory convergence is achieved."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a0dfc60",
   "metadata": {},
   "source": [
    "## Lesson 8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce879455",
   "metadata": {},
   "source": [
    "**Explain the core concepts of the lesson**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "165436ca",
   "metadata": {},
   "source": [
    "## Core Concepts\n",
    "\n",
    "This lesson explores the relationship between deep learning and the brain, examining both the useful analogies and their fundamental limitations.\n",
    "\n",
    "**Neural Network Analogy**: Deep learning systems are often described as mimicking the brain's structure and function. However, this analogy is more metaphorical than literal.\n",
    "\n",
    "**Biological Neuron vs. Logistic Regression Unit**: A biological neuron receives signals from other neurons, performs a thresholding computation, and fires an action potential. A logistic regression unit receives numerical inputs, applies a sigmoid activation function, and produces an output. While structurally similar in description, the biological reality is vastly more complex.\n",
    "\n",
    "**Forward and Backward Propagation**: Deep learning networks learn through forward propagation (computing outputs from inputs) and backpropagation (computing gradients to update weights). This learning mechanism is mathematically elegant and computationally effective.\n",
    "\n",
    "**Function Approximation**: Neural networks are fundamentally function approximators. They learn to map inputs to outputs through training, not through biological simulation. This perspective is more accurate and useful than thinking of them as brain models.\n",
    "\n",
    "**Gradient Descent and Learning**: The learning algorithm in neural networks relies on gradient descent to minimize error. Whether the brain uses similar mechanisms remains an open question in neuroscience."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64758c18",
   "metadata": {},
   "source": [
    "**Explain intuitions and mental models for the lesson**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54f20291",
   "metadata": {},
   "source": [
    "## Intuitions and Mental Models\n",
    "\n",
    "**Neural networks as function approximators**: Think of a neural network as a flexible mathematical tool that learns to approximate any function mapping inputs to outputs. This is more accurate than imagining it as a miniature brain.\n",
    "\n",
    "**The seductive but oversimplified brain analogy**: The brain analogy is intuitive and historically useful for motivation, but it breaks down quickly under scrutiny. A single biological neuron is far more complex than a logistic regression unit—it has thousands of synapses, complex temporal dynamics, neuromodulators, and feedback mechanisms that we still don't fully understand.\n",
    "\n",
    "**Complexity mismatch**: The human brain contains roughly 86 billion neurons with trillions of connections. Modern deep learning networks have millions to billions of parameters. The scale is different, but more importantly, the mechanisms are fundamentally different. Neuroscience still doesn't fully understand how individual neurons compute or how learning occurs at the biological level.\n",
    "\n",
    "**The mystery of biological learning**: It remains unclear whether the brain uses backpropagation, gradient descent, or entirely different learning mechanisms. The brain's learning algorithm is one of neuroscience's greatest unsolved mysteries.\n",
    "\n",
    "**Practical success independent of biology**: Deep learning excels at learning complex functions through forward and backward propagation, regardless of whether these mechanisms resemble the brain. The field's maturity has shifted focus from biological plausibility to mathematical effectiveness.\n",
    "\n",
    "**Evolution of the analogy**: The brain analogy was once essential for intuition and motivation in the early days of neural networks. As the field has matured, this analogy has become less relevant and sometimes misleading."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9e0b66b",
   "metadata": {},
   "source": [
    "**Create a Mermaid diagram: graph TD\n",
    "    A[Biological Neuron] -->|receives signals| B[X1, X2, X3]\n",
    "    B -->|thresholding computation| C[Fire Decision]\n",
    "    C -->|sends pulse| D[Axon to Other Neurons]\n",
    "    E[Logistic Regression Unit] -->|receives inputs| F[A1, A2, A3]\n",
    "    F -->|sigmoid activation| G[Output]\n",
    "    A -.->|loose analogy| E**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a1cdb9c",
   "metadata": {},
   "source": [
    "## Biological Neuron vs. Logistic Regression Unit\n",
    "\n",
    "```mermaid\n",
    "graph TD\n",
    "    A[Biological Neuron] -->|receives signals| B[X1, X2, X3]\n",
    "    B -->|thresholding computation| C[Fire Decision]\n",
    "    C -->|sends pulse| D[Axon to Other Neurons]\n",
    "    E[Logistic Regression Unit] -->|receives inputs| F[A1, A2, A3]\n",
    "    F -->|sigmoid activation| G[Output]\n",
    "    A -.->|loose analogy| E\n",
    "```\n",
    "\n",
    "The diagram above illustrates the structural similarity and the loose analogy between biological neurons and logistic regression units. Both receive multiple inputs and produce an output based on a nonlinear computation. However, the biological neuron's actual mechanisms—involving ion channels, neurotransmitters, temporal integration, and feedback loops—are far more intricate than the simple sigmoid function used in a logistic regression unit."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
