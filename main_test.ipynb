{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "44b2bac7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\offic\\OneDrive\\Desktop\\Python-Learning\\Data Science\\LangGraph-V1.0\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_google_genai.chat_models import ChatGoogleGenerativeAI\n",
    "from langgraph.graph import START,END,StateGraph\n",
    "from langchain.messages import HumanMessage,AIMessage,SystemMessage,ToolMessage\n",
    "from langgraph.types import Command,Send,RetryPolicy,interrupt\n",
    "from typing import Literal,TypedDict,List\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_huggingface.embeddings import HuggingFaceEmbeddings\n",
    "from pydantic import Field,BaseModel\n",
    "from langchain_core.vectorstores import InMemoryVectorStore\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.runtime import Runtime\n",
    "import nbformat\n",
    "from nbformat.v4 import new_notebook, new_markdown_cell, new_code_cell\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "215b2f58",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65fe9a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CellFormat(BaseModel):\n",
    "    cell_type: Literal[\"code\",\"markdown\"] = Field(...,description=\"The type of cell either code or markdown.\")\n",
    "    cell_no: int=Field(...,description=\"The cell number in which this particular markdown or code will go in.\")\n",
    "    cell_content: str=Field(...,description=\"The content that is to be written in the cell.\")\n",
    "\n",
    "class Code(BaseModel):\n",
    "    topic: str=Field(...,description=\"The topic on which the Chapter is based upon.\")\n",
    "    cell: List[CellFormat]\n",
    "    \n",
    "class Context(TypedDict):\n",
    "    course_name: str\n",
    "    course_url : str\n",
    "    \n",
    "class Lesson(TypedDict):\n",
    "    lesson_number: int\n",
    "    lesson_name: str\n",
    "    content: str\n",
    "    \n",
    "    \n",
    "class LessonMemory(BaseModel):\n",
    "    lesson_id: int\n",
    "    key_concepts: List[str]\n",
    "    intuitions: List[str]\n",
    "    equations: List[str]\n",
    "    code_primitives: List[str]\n",
    "\n",
    "class Chapter(TypedDict):\n",
    "    chapter_number: int\n",
    "    chapter_name: str\n",
    "    lessons : List[LessonMemory]\n",
    "\n",
    "\n",
    "class SummarizerState(TypedDict):\n",
    "    folder_path: str\n",
    "    content: List[Chapter]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fcfd87f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b29a0592",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sanitize_text(text: str) -> str:\n",
    "    # Remove control characters\n",
    "    bad_chars = [\"\\f\", \"\\r\", \"\\x0b\", \"\\x0c\"]\n",
    "    for ch in bad_chars:\n",
    "        text = text.replace(ch, \"\")\n",
    "    \n",
    "    # Normalize spaces\n",
    "    text = \" \".join(text.split())\n",
    "    return text\n",
    "\n",
    "import re\n",
    "\n",
    "def fix_latex(text: str) -> str:\n",
    "    text = sanitize_text(text)\n",
    "\n",
    "    # Remove stray single $\n",
    "    text = re.sub(r'(?<!\\$)\\$(?!\\$)', '', text)\n",
    "\n",
    "    # Ensure $$ pairs close\n",
    "    if text.count(\"$$\") % 2 != 0:\n",
    "        text = text.replace(\"$$\", \"$\")  # fallback to inline\n",
    "\n",
    "    # Remove HTML math (model sometimes does this)\n",
    "    text = re.sub(r\"<\\/?sup>\", \"\", text)\n",
    "    text = re.sub(r\"<\\/?sub>\", \"\", text)\n",
    "\n",
    "    return text\n",
    "import re\n",
    "\n",
    "def fix_mermaid(text: str) -> str:\n",
    "    if \"```mermaid\" not in text:\n",
    "        return text\n",
    "\n",
    "    # Extract only inside mermaid block to avoid breaking other text\n",
    "    pattern = r\"```mermaid(.*?)```\"\n",
    "    matches = re.findall(pattern, text, flags=re.S)\n",
    "    if not matches:\n",
    "        return text\n",
    "\n",
    "    fixed_blocks = []\n",
    "    for block in matches:\n",
    "        code = block.strip()\n",
    "\n",
    "        # Ensure graph declaration is on its own line\n",
    "        code = re.sub(r\"(graph\\s+(LR|TD))\", r\"\\n\\1\\n\", code)\n",
    "\n",
    "        # Put keywords on newlines (LLM usually puts everything in one line)\n",
    "        code = re.sub(r\"\\s*(subgraph)\\s*\", r\"\\n\\1 \", code)\n",
    "        code = re.sub(r\"\\s*(end)\\s*\", r\"\\n\\1\\n\", code)\n",
    "\n",
    "        # Put edges on their own lines\n",
    "        code = re.sub(r\"\\s*-->\\s*\", r\" --> \", code)\n",
    "        code = re.sub(r\"(\\w[^\\n]*-->[^\\n]*)\", r\"\\n\\1\", code)\n",
    "\n",
    "        # Fix \"\" in labels  -> change \"\"Label\"\" to \"Label\"\n",
    "        code = re.sub(r'\\[\"\"\\s*([^\"]+)\\s*\"\\]', r'[\"\\1\"]', code)\n",
    "\n",
    "        # Fix accidental double trailing quotes: [\"abc\"\"] -> [\"abc\"]\n",
    "        code = re.sub(r'\\[\"([^\"]+)\"+\"\\]', r'[\"\\1\"]', code)\n",
    "\n",
    "        # Fix cases like Bedrooms\"\"\n",
    "        code = re.sub(r'\"+\"(\\])', r'\"\\1', code)\n",
    "\n",
    "        # Fix subgraph labels like   subgraph A Label with spaces\n",
    "        code = re.sub(\n",
    "            r\"subgraph\\s+([A-Za-z0-9_]+)\\s+\\[?\\\"?([^\\\"\\]]+ ?)\\\"?\\]?\",\n",
    "            r'subgraph \\1[\"\\2\"]',\n",
    "            code\n",
    "        )\n",
    "\n",
    "        fixed_blocks.append(\"```mermaid\\n\" + code.strip() + \"\\n```\")\n",
    "\n",
    "    # Replace old blocks with fixed blocks\n",
    "    new_text = text\n",
    "    for old, new in zip(matches, fixed_blocks):\n",
    "        new_text = new_text.replace(\"```mermaid\" + old + \"```\", new)\n",
    "\n",
    "    return new_text\n",
    "\n",
    "def clean_markdown_cell(text: str) -> str:\n",
    "    text = sanitize_text(text)\n",
    "    text = fix_latex(text)\n",
    "    text = fix_mermaid(text)\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4fdb4392",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = f\"\"\"You are extracting structured learning memory from a single lesson transcript.\n",
    "\n",
    "Your task is to convert the provided transcript into a LessonMemory object.\n",
    "Focus ONLY on this lesson. Do NOT summarize the entire course or the entire week.\n",
    "\n",
    "Guidelines:\n",
    "- Extract atomic concepts, not explanations.\n",
    "- Separate intuition from equations and code.\n",
    "- Do NOT invent content that does not appear in the transcript.\n",
    "- Do NOT repeat definitions that are explicitly stated as previously covered.\n",
    "- Assume the reader has access to prior weeks' summaries if provided.\n",
    "\n",
    "Diagram Rules (IMPORTANT):\n",
    "- Generate Mermaid diagrams ONLY when a visual representation significantly improves understanding.\n",
    "- Use diagrams for:\n",
    "  - data flow\n",
    "  - algorithm steps\n",
    "  - architectural relationships\n",
    "  - process pipelines\n",
    "- Do NOT create diagrams for simple lists or trivial concepts.\n",
    "- Mermaid code must be valid and self-contained.\n",
    "- Prefer simple Mermaid types:\n",
    "  - flowchart\n",
    "  - sequenceDiagram\n",
    "  - graph TD\n",
    "\n",
    "Output Rules:\n",
    "- Return ONLY valid JSON matching the LessonMemory schema.\n",
    "- Do NOT include markdown, commentary, or explanations outside the schema.\n",
    "- Do NOT generate notebook cells or formatting.\n",
    "\n",
    "LessonMemory fields:\n",
    "\n",
    "- lesson_id:\n",
    "  Use the provided lesson_id exactly.\n",
    "\n",
    "- week_id:\n",
    "  Use the provided week_id exactly.\n",
    "\n",
    "- title:\n",
    "  Short descriptive title for this lesson.\n",
    "\n",
    "- key_concepts:\n",
    "  List of atomic concepts introduced or used in this lesson.\n",
    "  Each item must be a short noun phrase (2-5 words).\n",
    "  Do NOT include explanations, examples, or verbs.\n",
    "\n",
    "- intuitions:\n",
    "  High-level mental models that help understanding.\n",
    "  Use simple language.\n",
    "  Avoid equations, symbols, or code.\n",
    "\n",
    "- equations:\n",
    "  Core mathematical expressions that appear in the lesson.\n",
    "  Use LaTeX-style math when appropriate.\n",
    "  Skip derivations.\n",
    "\n",
    "- code_primitives:\n",
    "  Describe concrete coding demonstrations or implementations shown or discussed.\n",
    "  Focus on intent, not full code.\n",
    "\n",
    "- visual_primitives:\n",
    "  A list of Mermaid diagram definitions as strings.\n",
    "  Each item must contain ONLY valid Mermaid syntax.\n",
    "  Do NOT wrap diagrams in markdown code fences.\n",
    "  Do NOT include explanations or comments.\n",
    "\n",
    "Context from previous weeks (if any):\n",
    "{{previous_week_context}}\n",
    "\n",
    "Lesson transcript:\n",
    "{{lesson_transcript}}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "83e92ac5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"You are extracting structured learning memory from a single lesson transcript.\\n\\nYour task is to convert the provided transcript into a LessonMemory object.\\nFocus ONLY on this lesson. Do NOT summarize the entire course or the entire week.\\n\\nGuidelines:\\n- Extract atomic concepts, not explanations.\\n- Separate intuition from equations and code.\\n- Do NOT invent content that does not appear in the transcript.\\n- Do NOT repeat definitions that are explicitly stated as previously covered.\\n- Assume the reader has access to prior weeks' summaries if provided.\\n\\nDiagram Rules (IMPORTANT):\\n- Generate Mermaid diagrams ONLY when a visual representation significantly improves understanding.\\n- Use diagrams for:\\n  - data flow\\n  - algorithm steps\\n  - architectural relationships\\n  - process pipelines\\n- Do NOT create diagrams for simple lists or trivial concepts.\\n- Mermaid code must be valid and self-contained.\\n- Prefer simple Mermaid types:\\n  - flowchart\\n  - sequenceDiagram\\n  - graph TD\\n\\nOutput Rules:\\n- Return ONLY valid JSON matching the LessonMemory schema.\\n- Do NOT include markdown, commentary, or explanations outside the schema.\\n- Do NOT generate notebook cells or formatting.\\n\\nLessonMemory fields:\\n\\n- lesson_id:\\n  Use the provided lesson_id exactly.\\n\\n- week_id:\\n  Use the provided week_id exactly.\\n\\n- title:\\n  Short descriptive title for this lesson.\\n\\n- key_concepts:\\n  List of atomic concepts introduced or used in this lesson.\\n  Each item must be a short noun phrase (2-5 words).\\n  Do NOT include explanations, examples, or verbs.\\n\\n- intuitions:\\n  High-level mental models that help understanding.\\n  Use simple language.\\n  Avoid equations, symbols, or code.\\n\\n- equations:\\n  Core mathematical expressions that appear in the lesson.\\n  Use LaTeX-style math when appropriate.\\n  Skip derivations.\\n\\n- code_primitives:\\n  Describe concrete coding demonstrations or implementations shown or discussed.\\n  Focus on intent, not full code.\\n\\n- visual_primitives:\\n  A list of Mermaid diagram definitions as strings.\\n  Each item must contain ONLY valid Mermaid syntax.\\n  Do NOT wrap diagrams in markdown code fences.\\n  Do NOT include explanations or comments.\\n\\nContext from previous weeks (if any):\\nManas\\n\\nLesson transcript:\\nTiwari\\n\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt.format(previous_week_context=\"Manas\",lesson_transcript=\"Tiwari\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6a46f05c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_content(state: SummarizerState) -> Command[Literal[\"gen_content\"]] :\n",
    "    path = state[\"folder_path\"]\n",
    "    folders =  [entry.name for entry in os.scandir(path) if entry.is_dir()]\n",
    "\n",
    "    chapters = []\n",
    "    for folder in folders:\n",
    "        \n",
    "        chapter_no,chapter_name = folder.split('.',1)\n",
    "        lesson_path = f\"{path}//{folder}\"\n",
    "        lessons = [f for f in os.listdir(lesson_path) if os.path.isfile(os.path.join(lesson_path, f))]\n",
    "\n",
    "        chapter: Chapter = {}\n",
    "        chapter[\"chapter_number\"]=chapter_no\n",
    "        chapter[\"chapter_name\"]=chapter_name\n",
    "        chapter[\"lessons\"] = []\n",
    "        for lesson in lessons:\n",
    "            lesson_no,lesson_name_raw = lesson.split(\"-\",1)\n",
    "            lesson_name,_ = lesson_name_raw.split(\".\",1)\n",
    "            with open(f\"{lesson_path}//{lesson}\",\"r\") as f:\n",
    "                content = f.read()\n",
    "            chapter[\"lessons\"].append({\"lesson_number\":lesson_no,\"lesson_name\":lesson_name,\"content\":content})\n",
    "        chapters.append(chapter)\n",
    "        \n",
    "    return Command(update={\"content\":chapters},goto=\"gen_content\")\n",
    "\n",
    "def gen_content(state:SummarizerState,runtime:Runtime[Context]) -> Command[Literal[\"create_nb\"]]:\n",
    "    course_name = runtime.context[\"course_name\"]\n",
    "    course_url = runtime.context[\"course_url\"]\n",
    "    \n",
    "    model = ChatGoogleGenerativeAI(model=\"gemini-2.5-flash\",temperature=0.2,top_p=0.9).with_structured_output(Code)\n",
    "    chapters = state[\"content\"]\n",
    "    prompt = PromptTemplate(\n",
    "        template=\"\"\"\n",
    "        You are an expert course note creator and teacher.\n",
    "\n",
    "        Your job is to READ and UNDERSTAND the full lecture transcript and then\n",
    "        create a HIGH-QUALITY, BEAUTIFULLY FORMATTED, EASY-TO-READ Jupyter Notebook\n",
    "        that feels visually comfortable and well-structured.\n",
    "\n",
    "        ***Please seperate code in to code cell not in Markdown cell.***\n",
    "        DO NOT paste the transcript.\n",
    "        DO NOT include unnecessary text.\n",
    "        Teach clearly, concisely, and elegantly.\n",
    "\n",
    "        ========================\n",
    "        COURSE CONTEXT\n",
    "        Course: {course_name}\n",
    "        Link: {course_link}\n",
    "\n",
    "        LECTURE TRANSCRIPT:\n",
    "        {content}\n",
    "        ========================\n",
    "\n",
    "\n",
    "        OBJECTIVE\n",
    "        Produce a notebook that is:\n",
    "        - clean\n",
    "        - well spaced\n",
    "        - visually pleasant\n",
    "        - minimal but meaningful\n",
    "        - highly readable for students\n",
    "\n",
    "        The notebook should NOT look cluttered or loud.\n",
    "\n",
    "\n",
    "        ========================\n",
    "        NOTEBOOK LAYOUT (STRICT)\n",
    "        Use headings CAREFULLY and CONSISTENTLY.\n",
    "\n",
    "        HEADING RULES\n",
    "        # is ONLY allowed ONCE for the main title.\n",
    "        ## for section titles\n",
    "        ### for subsections\n",
    "        Do NOT use huge heading spam.\n",
    "\n",
    "        Add small spacing using short paragraphs and bullet lists.\n",
    "        Prefer clarity over density.\n",
    "\n",
    "\n",
    "        ========================\n",
    "        NOTEBOOK CONTENT FORMAT\n",
    "        \n",
    "        1. Pretend a student studies ONLY this notebook.\n",
    "        Make it:\n",
    "        - structured\n",
    "        - elegant\n",
    "        - use bullets not long paragraphs.\n",
    "        - visually comfortable\n",
    "        - logically organized\n",
    "        - NOT overly long\n",
    "        - NOT overly short\n",
    "\n",
    "        2. Visual Understanding\n",
    "        ## Visual Understanding\n",
    "\n",
    "        RULE:\n",
    "        - Prefer **clear explanation**\n",
    "        - If appropriate:\n",
    "        - Use Mermaid ONLY for flow / conceptual diagrams\n",
    "        - NEVER try to draw charts or curves with Mermaid\n",
    "        - If visual relationship needs a graph → provide matplotlib code instead.\n",
    "\n",
    "\n",
    "        3. Important Formulas (Math Section)\n",
    "        ## Important Formulas\n",
    "\n",
    "        Use VALID LaTeX.\n",
    "        Rules:\n",
    "        - Wrap in $$ block math $$ OR \\( inline math \\)\n",
    "        - No broken braces\n",
    "        - If unsure about correctness → write English explanation instead.\n",
    "\n",
    "        ========================\n",
    "        STRICT VISUAL QUALITY RULES\n",
    "        - Do NOT use massive headings everywhere\n",
    "        - Title → # only ONCE\n",
    "        - Sections → ##\n",
    "        - Subsections → ###\n",
    "        - No walls of text\n",
    "        - Prefer bullets + spacing\n",
    "        - Keep notebook compact but meaningful\n",
    "        - Make it enjoyable to scroll through\n",
    "\n",
    "        MATH & SYMBOL RULES\n",
    "        - NEVER use HTML tags like <sup>, <sub>, <b>, <i>, <span>\n",
    "        - ALWAYS use LaTeX MathJax formatting\n",
    "        - For superscripts: 'n^curlybrackets[l]'\n",
    "        - Inline math: \\( ... \\)\n",
    "        - Block math: $$ ... $$\n",
    "        - If LaTeX is uncertain → write in plain text instead of broken formatting\n",
    "        - No broken formulas\n",
    "        - No unmatched braces\n",
    "        - If risky → explain in English instead\n",
    "\n",
    "\n",
    "        ========================\n",
    "        DIAGRAM RULES\n",
    "        Allowed Mermaid uses:\n",
    "        - flowcharts\n",
    "        - mindmaps\n",
    "        - conceptual or process diagrams\n",
    "        MERMAID RULES\n",
    "        - Subgraph names must NOT contain spaces\n",
    "        Use: Input_Layer , Hidden_Layer\n",
    "        - If you want a pretty title use:\n",
    "        subgraph Input_Layer[\"Input Layer\"]\n",
    "        - Node labels with spaces MUST be quoted:\n",
    "        X2[\"Zip Code\"]\n",
    "        - NEVER use # inside node labels\n",
    "        - Prefer flowchart relationships, not charts\n",
    "        Forbidden:\n",
    "        - charts\n",
    "        - line plots\n",
    "        - performance curves\n",
    "        - axes-based visuals\n",
    "\n",
    "        If chart needed:\n",
    "        Generate clean matplotlib Python code instead.\n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "        Return output STRICTLY in the structured schema also strictly avoid using long paragraphs use bullet points instead.\n",
    "\n",
    "        \"\"\",\n",
    "        input_variables=[\"course_name\",\"course_link\",\"content\"]\n",
    "    )\n",
    "    chain = prompt | model\n",
    "    inputs = [{\"course_name\":course_name,\"course_link\":course_url,\"content\":x} for x in chapters]\n",
    "\n",
    "    responses = chain.batch(inputs)\n",
    "    \n",
    "    \n",
    "    updated_chapters = [\n",
    "        {**chapter, \"generated_content\": responses[i]}\n",
    "        for i, chapter in enumerate(chapters)\n",
    "    ]\n",
    "\n",
    "    return Command(update={\"content\": updated_chapters},goto=\"create_nb\")\n",
    "    \n",
    "\n",
    "def create_nb(state:SummarizerState)->Command[Literal[END]]:\n",
    "    chapters = state[\"content\"]\n",
    "    nb = new_notebook()\n",
    "    nb.cells = []\n",
    "\n",
    "    for chapter in chapters :\n",
    "        code_details = chapter[\"generated_content\"]\n",
    "        nb.cells.append(new_markdown_cell(f\"# {code_details.topic}\"))\n",
    "\n",
    "        for cell in code_details.cell:\n",
    "            content = cell.cell_content\n",
    "\n",
    "            if isinstance(content, str):\n",
    "                content = content.encode(\"utf-8\").decode(\"unicode_escape\")\n",
    "\n",
    "            if cell.cell_type == \"markdown\":\n",
    "                content = clean_markdown_cell(content)\n",
    "                nb.cells.append(new_markdown_cell(content))\n",
    "            else:\n",
    "                nb.cells.append(new_code_cell(content))\n",
    "\n",
    "    with open(\"generated_notebook.ipynb\", \"w\", encoding=\"utf-8\") as f:\n",
    "        nbformat.write(nb, f)\n",
    "\n",
    "    print(\"Notebook generated Successfully !!\")\n",
    "    return Command(goto=END)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6f66379e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<langgraph.graph.state.CompiledStateGraph object at 0x000002598F869390>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "workflow = StateGraph(state_schema=SummarizerState,context_schema=Context)\n",
    "workflow.add_node(\"get_content\",get_content)\n",
    "workflow.add_node(\"gen_content\",gen_content)\n",
    "workflow.add_node(\"create_nb\",create_nb)\n",
    "workflow.add_edge(START,\"get_content\")\n",
    "workflow.add_edge(\"create_nb\",END)\n",
    "graph = workflow.compile()\n",
    "graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b181c00c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Notebook generated Successfully !!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\offic\\AppData\\Local\\Temp\\ipykernel_30536\\666092217.py:195: DeprecationWarning: invalid escape sequence '\\('\n",
      "  content = content.encode(\"utf-8\").decode(\"unicode_escape\")\n",
      "C:\\Users\\offic\\AppData\\Local\\Temp\\ipykernel_30536\\666092217.py:195: DeprecationWarning: invalid escape sequence '\\s'\n",
      "  content = content.encode(\"utf-8\").decode(\"unicode_escape\")\n",
      "C:\\Users\\offic\\AppData\\Local\\Temp\\ipykernel_30536\\666092217.py:195: DeprecationWarning: invalid escape sequence '\\h'\n",
      "  content = content.encode(\"utf-8\").decode(\"unicode_escape\")\n",
      "C:\\Users\\offic\\AppData\\Local\\Temp\\ipykernel_30536\\666092217.py:195: DeprecationWarning: invalid escape sequence '\\m'\n",
      "  content = content.encode(\"utf-8\").decode(\"unicode_escape\")\n",
      "C:\\Users\\offic\\AppData\\Local\\Temp\\ipykernel_30536\\666092217.py:195: DeprecationWarning: invalid escape sequence '\\p'\n",
      "  content = content.encode(\"utf-8\").decode(\"unicode_escape\")\n",
      "C:\\Users\\offic\\AppData\\Local\\Temp\\ipykernel_30536\\666092217.py:195: DeprecationWarning: invalid escape sequence '\\c'\n",
      "  content = content.encode(\"utf-8\").decode(\"unicode_escape\")\n",
      "C:\\Users\\offic\\AppData\\Local\\Temp\\ipykernel_30536\\666092217.py:195: DeprecationWarning: invalid escape sequence '\\l'\n",
      "  content = content.encode(\"utf-8\").decode(\"unicode_escape\")\n",
      "C:\\Users\\offic\\AppData\\Local\\Temp\\ipykernel_30536\\666092217.py:195: DeprecationWarning: invalid escape sequence '\\o'\n",
      "  content = content.encode(\"utf-8\").decode(\"unicode_escape\")\n"
     ]
    }
   ],
   "source": [
    "folder_path=\"./transcripts\"\n",
    "results = graph.invoke({\"folder_path\":folder_path},context=Context(course_name=\"neural-networks-deep-learning\",course_url=\"https://www.coursera.org/learn/neural-networks-deep-learning\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "84e7c301",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'folder_path': './transcripts',\n",
       " 'content': [{'chapter_number': '1',\n",
       "   'chapter_name': 'Introduction_to_deep_learning',\n",
       "   'lessons': [{'lesson_number': '01',\n",
       "     'lesson_name': 'welcome',\n",
       "     'content': \"Hello and welcome.\\nAs you probably know,\\ndeep learning has already transformed\\ntraditional internet businesses like web search and advertising.\\nBut deep learning is also enabling\\nbrand new products and businesses and ways of helping people to be created.\\nEverything ranging from better healthcare,\\nwhere deep learning is getting really good at reading\\nX-ray images to delivering personalized education,\\nto precision agriculture, to even self driving cars and many others.\\nIf you want to learn the tools of deep learning and\\nbe able to apply them to build these amazing things,\\nI want to help you get there.\\nWhen you finish the sequence of courses on Coursera,\\ncalled the specialization, you will be able to put\\ndeep learning onto your resume with confidence.\\nOver the next decade,\\nI think all of us have an opportunity to build an amazing world, amazing society,\\nthat is AI-powered,\\nand I hope that you will play a big role in the creation of this AI-powered society.\\nSo that, let's get started.\\nI think that AI is the new electricity.\\nStarting about 100 years ago,\\nthe electrification of our society transformed every major industry,\\neverything ranging from transportation, manufacturing,\\nto healthcare, to communications and many more.\\nAnd I think that today, we see a surprisingly clear path\\nfor AI to bring about an equally big transformation.\\nAnd of course, the part of AI that is rising\\nrapidly and driving a lot of these developments, is deep learning.\\nSo today, deep learning is one of the most\\nhighly sought after skills in the technology world.\\nAnd through this course, and a few courses after this one,\\nI want to help you to gain and master those skills.\\nSo here is what you will learn in this sequence of\\ncourses also called a specialization on Coursera.\\nIn the first course,\\nyou will learn about the foundations of neural networks,\\nyou will learn about neural networks and deep learning.\\nThis video that you are watching is part of\\nthis first course, which last four weeks in total.\\nAnd each of the five courses in the specialization will be about two to four weeks,\\nwith most of them actually shorter than four weeks.\\nBut in this first course,\\nyou will learn how to build a neural network, including\\na deep neural network, and how to train it on data.\\nAnd at the end of this course,\\nyou will be able to build a deep neural network to recognize, guess what?\\nCats. For some reason,\\nthere is a cat meme running around in deep learning.\\nAnd so, following tradition in this first course,\\nwe will build a cat recognizer.\\nThen in the second course,\\nyou will learn about the practical aspects of deep learning.\\nSo you will learn, now that you have built a neurak network,\\nhow to actually get it to perform well.\\nSo you learn about hyperparameter tuning, regularization,\\nhow to diagnose bias, and variants, and advance\\noptimization algorithms, like momentum, armrest, prop, and the ad authorization algorithm.\\nSometimes it seems like there is a lot of tuning,\\neven some black magic in how you build a new network.\\nSo the second course, which is just three weeks,\\nwill demystify some of that black magic.\\nIn the third course, which is just two weeks,\\nyou will learn how to structure your machine learning project.\\nIt turns out that the strategy for building\\na machine learning system has changed in the era of deep learning.\\nSo for example, the way you split your data into train,\\ndevelopment or dev, also called holdout cross-validation sets, and test sets,\\nhas changed in the era of deep learning.\\nSo what are the new best practices for doing that?\\nAnd whether if your training set and your test come from different distributions,\\nthat is happening a lot more in the era of deep learning.\\nSo, how do you deal with that?\\nAnd if you have heard of end-to-end deep learning,\\nyou will also learn more about that in this third course,\\nand see when you should use it and maybe when you shouldn't.\\nThe material in this third course is relatively unique.\\nI am going to share of you a lot of the hard-won lessons that I have learned,\\nbuilding and shipping quite a lot of deep learning products.\\nAs far as I know,\\nthis is largely material that is not taught in\\nmost universities that have deep learning courses.\\nBut I think it will really help you to get your deep learning systems to work well.\\nIn the next course,\\nwe will then talk about convolutional neural networks, often abbreviated CNNs.\\nConvolutional networks, or convolutional neural networks, are often applied to images.\\nSo you will learn how to build these models in course four.\\nFinally, in course five,\\nyou will learn sequence models and how to apply\\nthem to natural language processing and other problems.\\nSo sequence models includes models like\\nrecurrent neural networks, abbreviated RNNs, and LSTM models,\\nstands for a long short term memory models.\\nYou will learn what these terms mean in course five and be able to\\napply them to natural language processing problems.\\nSo you will learn these models in course five and be able to apply them to sequence data.\\nSo for example, natural language is just a sequence of words,\\nand you will also understand how these models can be applied to speech recognition,\\nor to music generation, and other problems.\\nSo through these courses,\\nyou will learn the tools of deep learning,\\nyou will be able to apply them to build amazing things,\\nand I hope many of you through this will also be able to advance your career.\\nSo with that, let's get started.\\nPlease go on to the next video where we will talk about\\ndeep learning applied to supervised learning.\"},\n",
       "    {'lesson_number': '02',\n",
       "     'lesson_name': 'what-is-a-neural-network',\n",
       "     'content': 'The term, Deep Learning, refers to training Neural Networks,\\nsometimes very large Neural Networks.\\nSo what exactly is a Neural Network?\\nIn this video, let\\'s try to give you some of the basic intuitions.\\nLet\\'s start with a Housing Price Prediction example.\\nLet\\'s say you have a data set with six houses, so you know the size of the houses\\nin square feet or square meters and you know the price of the house and you want\\nto fit a function to predict the price of a house as a function of its size.\\nSo if you are familiar with linear regression you might say, well let\\'s\\nput a straight line to this data, so, and we get a straight line like that.\\nBut to be put fancier, you might say, well we know that prices\\ncan never be negative, right?\\nSo instead of the straight line fit, which eventually will become negative,\\nlet\\'s bend the curve here.\\nSo it just ends up zero here.\\nSo this thick blue line ends up being your function for\\npredicting the price of the house as a function of its size.\\nWhere it is zero here and then there is a straight line fit to the right.\\nSo you can think of this function that you have just fit to housing prices\\nas a very simple neural network.\\nIt is almost the simplest possible neural network.\\nLet me draw it here.\\nWe have as the input to the neural network the size of a house which we call x.\\nIt goes into this node, this little circle and\\nthen it outputs the price which we call y.\\nSo this little circle, which is a single neuron in a neural network,\\nimplements this function that we drew on the left.\\nAnd all that the neuron does is it inputs the size, computes this linear function,\\ntakes a max of zero, and then outputs the estimated price.\\nAnd by the way in the neural network literature, you will see this function a lot.\\nThis function which goes to zero sometimes and\\nthen it\\'ll take of as a straight line.\\nThis function is called a ReLU function which stands for\\nrectified linear units.\\nSo R-E-L-U. And\\nrectify just means taking a max of 0 which is why you get a function shape like this.\\nYou don\\'t need to worry about ReLU units for\\nnow but it\\'s just something you will see again later in this course.\\nSo if this is a single neuron, neural network,\\nreally a tiny little neural network, a larger neural network\\nis then formed by taking many of the single neurons and stacking them together.\\nSo, if you think of this neuron that\\'s being like a single Lego brick, you then\\nget a bigger neural network by stacking together many of these Lego bricks.\\nLet\\'s see an example.\\nLetâ€™s say that instead of predicting the price of a house just from the size,\\nyou now have other features.\\nYou know other things about the house, such as the number of bedrooms,\\nwhich we would write as \"#bedrooms\", and you might think that one of the things\\nthat really affects the price of a house is family size, right?\\nSo can this house fit your family of three, or family of four, or\\nfamily of five?\\nAnd it\\'s really based on the size in square feet or square meters, and\\nthe number of bedrooms that determines whether or\\nnot a house can fit your family\\'s family size.\\nAnd then maybe you know the zip codes,\\nin different countries it\\'s called a postal code of a house.\\nAnd the zip code maybe as a feature tells you, walkability?\\nSo is this neighborhood highly walkable?\\nThink just walks to the grocery store?\\nWalk to school?\\nDo you need to drive?\\nAnd some people prefer highly walkable neighborhoods.\\nAnd then the zip code as well as the wealth maybe tells you, right.\\nCertainly in the United States but some other countries as well.\\nTells you how good is the school quality.\\nSo each of these little circles I\\'m drawing, can be one of those ReLU,\\nrectified linear units or some other slightly non linear function.\\nSo that based on the size and number of bedrooms,\\nyou can estimate the family size, their zip code, based on walkability,\\nbased on zip code and wealth can estimate the school quality.\\nAnd then finally you might think that well the way people decide how much they\\'re\\nwilling to pay for a house, is they look at the things that really matter to them.\\nIn this case family size, walkability, and school quality and\\nthat helps you predict the price.\\nSo in the example x is all of these four inputs.\\nAnd y is the price you\\'re trying to predict.\\nAnd so by stacking together a few of the single neurons or the simple predictors\\nwe have from the previous slide, we now have a slightly larger neural network.\\nHow you manage neural network is that when you implement it,\\nyou need to give it just the input x and\\nthe output y for a number of examples in your training set and\\nall these things in the middle, they will figure out by itself.\\nSo what you actually implement is this.\\nWhere, here, you have a neural network with four inputs.\\nSo the input features might be the size, number of bedrooms,\\nthe zip code or postal code, and the wealth of the neighborhood.\\nAnd so given these input features,\\nthe job of the neural network will be to predict the price y.\\nAnd notice also that each of these circles, these are called hidden units in\\nthe neural network, that each of them takes its inputs all four input features.\\nSo for example, rather than saying this first node represents family size and\\nfamily size depends only on the features X1 and X2.\\nInstead, we\\'re going to say, well neural network,\\nyou decide whatever you want this node to be.\\nAnd we\\'ll give you all four input features to compute whatever you want.\\nSo we say that layer that this is input layer and\\nthis layer in the middle of the neural network are densely connected.\\nBecause every input feature is connected to every one\\nof these circles in the middle.\\nAnd the remarkable thing about neural networks is that, given enough data about\\nx and y, given enough training examples with both x and y, neural networks\\nare remarkably good at figuring out functions that accurately map from x to y.\\nSo, that\\'s a basic neural network.\\nIt turns out that as you build out your own neural networks,\\nyou\\'ll probably find them to be most useful, most powerful\\nin supervised learning incentives, meaning that you\\'re trying to take an input x and\\nmap it to some output y, like we just saw in the housing price prediction example.\\nIn the next video let\\'s go over some more examples of supervised learning and\\nsome examples of where you might find your networks to be incredibly helpful for\\nyour applications as well.'},\n",
       "    {'lesson_number': '03',\n",
       "     'lesson_name': 'supervised-learning-with-neural-networks',\n",
       "     'content': \"There's been a lot of hype about neural networks.\\nAnd perhaps some of that hype is justified, given how well they're working.\\nBut it turns out that so\\nfar, almost all the economic value created by neural networks has been through\\none type of machine learning, called supervised learning.\\nLet's see what that means, and let's go over some examples.\\nIn supervised learning, you have some input x, and\\nyou want to learn a function mapping to some output y.\\nSo for example, just now we saw the housing price prediction application where\\nyou input some features of a home and try to output or estimate the price y.\\nHere are some other examples that neural networks have been applied to very\\neffectively.\\nPossibly the single most lucrative application of deep learning today is\\nonline advertising, maybe not the most inspiring, but certainly very lucrative,\\nin which, by inputting information about an ad to the website it's thinking\\nof showing you, and some information about the user, neural networks have\\ngotten very good at predicting whether or not you click on an ad.\\nAnd by showing you and\\nshowing users the ads that you are most likely to click on, this has been\\nan incredibly lucrative application of neural networks at multiple companies.\\nBecause the ability to show you ads that you're more likely to\\nclick on has a direct impact on the bottom\\nline of some of the very large online advertising companies.\\nComputer vision has also made huge strides in the last several years,\\nmostly due to deep learning.\\nSo you might input an image and want to output an index,\\nsay from 1 to 1,000 trying to tell you if this picture,\\nit might be any one of, say a 1000 different images.\\nSo, you might us that for photo tagging.\\nI think the recent progress in speech recognition has also been very exciting,\\nwhere you can now input an audio clip to a neural network, and\\nhave it output a text transcript.\\nMachine translation has also made huge strides thanks to deep learning where now\\nyou can have a neural network input an English sentence and directly output say,\\na Chinese sentence.\\nAnd in autonomous driving, you might input an image, say a picture of what's in\\nfront of your car as well as some information from a radar, and\\nbased on that, maybe a neural network can be trained to tell you the position\\nof the other cars on the road.\\nSo this becomes a key component in autonomous driving systems.\\nSo a lot of the value creation through neural networks has been through cleverly\\nselecting what should be x and what should be y for\\nyour particular problem, and then fitting this supervised learning component into\\noften a bigger system such as an autonomous vehicle.\\nIt turns out that slightly different types of neural networks are useful for\\ndifferent applications.\\nFor example, in the real estate application that we saw in the previous\\nvideo, we use a universally standard neural network architecture, right?\\nMaybe for real estate and online advertising might be a relatively\\nstandard neural network, like the one that we saw.\\nFor image applications we'll often use convolutional neural networks,\\noften abbreviated CNN.\\nAnd for sequence data.\\nSo for example, audio has a temporal component, right?\\nAudio is played out over time, so audio is most naturally represented\\nas a one-dimensional time series or as a one-dimensional temporal sequence.\\nAnd so for sequence data, you often use an RNN,\\na recurrent neural network.\\nLanguage, English and Chinese, the alphabets or the words come one at a time.\\nSo language is also most naturally represented as sequence data.\\nAnd so more complex versions of RNNs are often used for these applications.\\nAnd then, for more complex applications, like autonomous driving, where you have an\\nimage, that might suggest more of a CNN, convolution neural network, structure and\\nradar info which is something quite different.\\nYou might end up with a more custom, or\\nsome more complex, hybrid neural network architecture.\\nSo, just to be a bit more concrete about what are the standard CNN and\\nRNN architectures.\\nSo in the literature you might have seen pictures like this.\\nSo that's a standard neural net.\\nYou might have seen pictures like this.\\nWell this is an example of a Convolutional Neural Network, and we'll see in\\na later course exactly what this picture means and how can you implement this.\\nBut convolutional networks are often used for image data.\\nAnd you might also have seen pictures like this.\\nAnd you'll learn how to implement this in a later course.\\nRecurrent neural networks are very good for\\nthis type of one-dimensional sequence data that has maybe a temporal component.\\nYou might also have heard about applications of machine learning\\nto both Structured Data and Unstructured Data.\\nHere's what the terms mean.\\nStructured Data means basically databases of data.\\nSo, for example, in housing price prediction, you might have a database or\\nthe column that tells you the size and the number of bedrooms.\\nSo, this is structured data, or in predicting whether or not a user will\\nclick on an ad, you might have information about the user, such as the age,\\nsome information about the ad, and then labels why that you're trying to predict.\\nSo that's structured data, meaning that each of the features,\\nsuch as size of the house, the number of bedrooms, or\\nthe age of a user, has a very well defined meaning.\\nIn contrast, unstructured data refers to things like audio, raw audio,\\nor images where you might want to recognize what's in the image or text.\\nHere the features might be the pixel values in an image or\\nthe individual words in a piece of text.\\nHistorically, it has been much harder for\\ncomputers to make sense of unstructured data compared to structured data.\\nAnd in fact the human race has evolved to be very good at understanding\\naudio cues as well as images.\\nAnd then text was a more recent invention, but\\npeople are just really good at interpreting unstructured data.\\nAnd so one of the most exciting things about the rise of neural networks is that,\\nthanks to deep learning, thanks to neural networks, computers are now much better\\nat interpreting unstructured data as well compared to just a few years ago.\\nAnd this creates opportunities for many new exciting applications that use\\nspeech recognition, image recognition, natural language processing on text,\\nmuch more than was possible even just two or three years ago.\\nI think because people have a natural empathy to understanding unstructured\\ndata, you might hear about neural network successes on unstructured data\\nmore in the media because it's just cool when the neural network recognizes a cat.\\nWe all like that, and we all know what that means.\\nBut it turns out that a lot of short term economic value that neural\\nnetworks are creating has also been on structured data,\\nsuch as much better advertising systems, much better profit recommendations, and\\njust a much better ability to process the giant databases that\\nmany companies have to make accurate predictions from them.\\nSo in this course, a lot of the techniques we'll go over will apply\\nto both structured data and to unstructured data.\\nFor the purposes of explaining the algorithms,\\nwe will draw a little bit more on examples that use unstructured data.\\nBut as you think through applications of neural networks within your own team I\\nhope you find both uses for them in both structured and unstructured data.\\nSo neural networks have transformed supervised learning and\\nare creating tremendous economic value.\\nIt turns out though, that the basic technical ideas behind neural networks\\nhave mostly been around, sometimes for many decades.\\nSo why is it, then, that they're only just now taking off and working so well?\\nIn the next video, we'll talk about why it's only quite recently\\nthat neural networks have become this incredibly powerful tool that you can use.\"},\n",
       "    {'lesson_number': '04',\n",
       "     'lesson_name': 'why-is-deep-learning-taking-off',\n",
       "     'content': 'If the basic technical ideas behind\\ndeep learning behind your networks have\\nbeen around for decades why are they\\nonly just now taking off in this video\\nlet\\'s go over some of the main drivers\\nbehind the rise of deep learning because\\nI think this will help you to spot\\nthe best opportunities within your own\\norganization to apply these to over the\\nlast few years a lot of people have\\nasked me \"Andrew why is deep learning\\nsuddenly working so well?\" and when I\\nam asked that question this is usually the\\npicture I draw for them. Let\\'s say we\\nplot a figure where on the horizontal\\naxis we plot the amount of data we have\\nfor a task and let\\'s say on the vertical\\naxis we plot the performance on involved\\nlearning algorithms such as the accuracy\\nof our spam classifier or our ad click\\npredictor or the accuracy of our neural\\nnet for figuring out the position of\\nother cars for our self-driving car. It\\nturns out if you plot the performance of\\na traditional learning algorithm like\\nsupport vector machine or logistic\\nregression as a function of the amount\\nof data you have you might get a curve\\nthat looks like this where the\\nperformance improves for a while as you\\nadd more data but after a while the\\nperformance you know pretty much\\nplateaus right suppose your horizontal\\nlines enjoy that very well you know was\\nit they didn\\'t know what to do with huge\\namounts of data and what happened in our\\nsociety over the last 10 years maybe is\\nthat for a lot of problems we went from\\nhaving a relatively small amount of data\\nto having you know often a fairly large\\namount of data and all of this was\\nthanks to the digitization of a society\\nwhere so much human activity is now in\\nthe digital realm we spend so much time\\non the computers on websites on mobile\\napps and activities on digital devices\\ncreates data and thanks to the rise of\\ninexpensive cameras built into our cell\\nphones, accelerometers, all sorts of\\nsensors in the Internet of Things. We\\nalso just have been collecting one more\\nand more data. So over the last 20 years\\nfor a lot of applications we just\\naccumulate\\na lot more data more than traditional\\nlearning algorithms were able to\\neffectively take advantage of and what\\nnew network lead turns out that if you\\ntrain a small neural net then this\\nperformance maybe looks like that.\\nIf you train a somewhat larger Internet\\nthat\\'s called as a medium-sized internet.\\nTo fall in something a little bit between\\nand if you train a very large neural net\\nthen it\\'s the form and often just keeps\\ngetting better and better. So, a couple\\nobservations. One is if you want to hit\\nthis very high level of performance then\\nyou need two things first: often you need\\nto be able to train a big enough neural\\nnetwork in order to take advantage of\\nthe huge amount of data and second you\\nneed to be out here, on the x axis you do\\nneed a lot of data so we often say that\\nscale has been driving deep learning\\nprogress and by scale I mean both the\\nsize of the neural network, meaning just\\na new network, a lot of hidden units, a\\nlot of parameters, a lot of connections,\\nas well as the scale of the data. In fact,\\ntoday one of the most reliable ways to\\nget better performance in a neural\\nnetwork is often to either train a\\nbigger network or throw more data at it\\nand that only works up to a point\\nbecause eventually you run out of data\\nor eventually then your network is so\\nbig that it takes too long to train. But,\\njust improving scale has actually taken\\nus a long way in the world of learning\\nin order to make this diagram a bit more\\ntechnically precise and just add a few\\nmore things I wrote the amount of data\\non the x-axis. Technically, this is amount\\nof labeled data where by label data\\nI mean training examples we have both\\nthe input X and the label Y I went to\\nintroduce a little bit of notation that\\nwe\\'ll use later in this course. We\\'re\\ngoing to use lowercase alphabet m to\\ndenote the size of my training sets or\\nthe number of training examples\\nthis lowercase M so that\\'s the\\nhorizontal axis. A couple other details, to\\nthis figure,\\nin this regime of smaller training sets\\nthe relative ordering of the algorithms\\nis actually not very well defined so if\\nyou don\\'t have a lot of training data it is\\noften up to your skill at hand\\nengineering features that determines the\\nforeman so it\\'s quite possible that if\\nsomeone training an SVM is more\\nmotivated to hand engineer features and\\nsomeone training even larger neural nets,\\nthat may be in this small training set\\nregime, the SEM could do better\\nso you know in this region to the left\\nof the figure the relative ordering\\nbetween gene algorithms is not that well\\ndefined and performance depends much\\nmore on your skill at engine features\\nand other mobile details of the\\nalgorithms and there\\'s only in this some\\nbig data regime. Very large training sets,\\nvery large M regime in the right that we\\nmore consistently see large neural nets\\ndominating the other approaches. And so\\nif any of your friends ask you why are\\nneural nets taking off I would\\nencourage you to draw this picture for\\nthem as well. So I will say that in the\\nearly days in their modern rise of deep\\nlearning,\\nit was scaled data and scale of\\ncomputation just our ability to train\\nvery large neural networks\\neither on a CPU or GPU that enabled us\\nto make a lot of progress. But\\nincreasingly, especially in the last\\nseveral years, we\\'ve seen tremendous\\nalgorithmic innovation as well so I also\\ndon\\'t want to understate that.\\nInterestingly, many of the algorithmic\\ninnovations have been about trying to\\nmake neural networks run much faster so\\nas a concrete example one of the huge\\nbreakthroughs in neural networks has been\\nswitching from a sigmoid function, which\\nlooks like this, to a railer function,\\nwhich we talked about briefly in an\\nearly video, that looks like this. If you\\ndon\\'t understand the details of one\\nabout the state don\\'t worry about it but\\nit turns out that one of the problems of\\nusing sigmoid functions and machine\\nlearning is that there are these regions\\nhere where the slope of the function\\nwhere the\\ngradient is nearly zero and so learning\\nbecomes really slow, because when you\\nimplement gradient descent and gradient\\nis zero the parameters just change very\\nslowly. And so, learning is very slow\\nwhereas by changing the what\\'s called\\nthe activation function the neural\\nnetwork to use this function called the\\nvalue function of the rectified linear\\nunit, or RELU, the gradient is equal to\\n1 for all positive values of input.\\nright. And so, the gradient is much less\\nlikely to gradually shrink to 0 and\\nthe gradient here. the slope of this line\\nis 0 on the left but it turns out\\nthat just by switching to the sigmoid\\nfunction to the RELU function has\\nmade an algorithm called gradient\\ndescent work much faster and so this is\\nan example of maybe relatively simple\\nalgorithmic innovation. But ultimately, the\\nimpact of this algorithmic innovation\\nwas it really helped computation. so there\\nare actually quite a lot of examples like\\nthis of where we change the algorithm\\nbecause it allows that code to run much\\nfaster and this allows us to train\\nbigger neural networks, or to do so the\\nreason will decline even when we have\\na large network roam all the data. The\\nother reason that fast computation is\\nimportant is that it turns out the\\nprocess of training your network is\\nvery intuitive. Often, you have an idea\\nfor a neural network architecture and so\\nyou implement your idea and code.\\nImplementing your idea then lets you run\\nan experiment which tells you how well\\nyour neural network does and then by\\nlooking at it you go back to change the\\ndetails of your new network and then you\\ngo around this circle over and over and\\nwhen your new network takes a long time\\nto train it just takes a long time to go\\naround this cycle and there\\'s a huge\\ndifference in your productivity. Building\\neffective neural networks when you can\\nhave an idea and try it and see the work\\nin ten minutes, or maybe at most a day,\\nversus if you\\'ve to train your neural\\nnetwork for a month, which sometimes does\\nhappen,\\nbecause you get a result back you know\\nin ten minutes or maybe in a day you\\nshould just try a lot more ideas and be\\nmuch more likely to discover in your\\nnetwork. And it works well for your\\napplication and so faster computation\\nhas really helped in terms of speeding\\nup the rate at which you can get an\\nexperimental result back and this has\\nreally helped both practitioners of\\nneural networks as well as researchers\\nworking and deep learning iterate much\\nfaster and improve your ideas much\\nfaster. So, all this has also been a\\nhuge boon to the entire deep learning\\nresearch community which has been\\nincredible with just inventing\\nnew algorithms and making nonstop\\nprogress on that front. So these are some\\nof the forces powering the rise of deep\\nlearning but the good news is that these\\nforces are still working powerfully to\\nmake deep learning even better. Take data...\\nsociety is still throwing out more\\ndigital data. Or take computation, with\\nthe rise of specialized hardware like\\nGPUs and faster networking many types of\\nhardware, I\\'m actually quite confident\\nthat our ability to do very large neural\\nnetworks from a computation point\\nof view will keep on getting better and\\ntake algorithms relative to learning\\nresearch communities are continuously\\nphenomenal at elevating on the\\nalgorithms front. So because of this, I\\nthink that we can be optimistic answer\\nis that deep learning will\\nkeep on getting better for many years to\\ncome.\\nSo with that, let\\'s go on to the last video of\\nthe section where we\\'ll talk a little\\nbit more about what you learn from this\\ncourse.'},\n",
       "    {'lesson_number': '05',\n",
       "     'lesson_name': 'about-this-course',\n",
       "     'content': \"So you're just about to reach the end of\\nthe first week of material on the first course in this specialization.\\nLet me give you a quick sense of what you'll learn in the next few weeks as well.\\nAs I said in the first video,\\nthis specialization comprises five courses.\\nAnd right now, we're in the first of\\nthese five courses, which teaches you the most important foundations,\\nreally the most important building blocks of deep learning.\\nSo by the end of this first course,\\nyou will know how to build and get to work on a deep neural network.\\nSo here the details of what is in this first course.\\nThis course is four weeks of material.\\nAnd you're just coming up to the end of\\nthe first week when you saw an introduction to deep learning.\\nAt the end of each week,\\nthere are also be 10 multiple-choice questions that\\nyou can use to double check your understanding of the material.\\nSo when you're done watching this video,\\nI hope you're going to take a look at those questions.\\nIn the second week, you will then learn about the Basics of Neural Network Programming.\\nYou'll learn the structure of what we\\ncall the forward propagation and the back propagation\\nsteps of the algorithm and how to implement neural networks efficiently.\\nStarting from the second week,\\nyou also get to do a programming exercise\\nthat lets you practice the material you've just learned,\\nimplement the algorithms yourself and see it work for yourself.\\nI find it really satisfying when I learn about algorithm and I\\nget it coded up and I see it worked for myself.\\nSo I hope you enjoy that too.\\nHaving learned the framework for neural network programming in the third week,\\nyou will code up a single hidden layer neural network. All right.\\nSo you will learn about all the key concepts needed\\nto implement and get to work in neural network.\\nAnd then finally in week four,\\nyou will build a deep neural network and neural network with\\nmany layers and see it work for yourself.\\nSo, congratulations on finishing the videos up to this one.\\nI hope that you now have a good high-level sense of what's happening in deep learning.\\nAnd perhaps some of you are also excited to\\nhave some ideas of where you might want to apply deep learning yourself.\\nSo, I hope that after this video,\\nyou go on to take a look at the 10 multiple choice questions that follow this video\\non the course website and just use\\nthe 10 multiple choice questions to check your understanding.\\nAnd don't review, you don't get all the answers right the first time,\\nyou can try again and again until you get them all right.\\nI found them useful to make sure that I'm understanding all the concepts,\\nI hope you're that way too.\\nSo with that, congrats again for getting up to\\nhere and I look forward to seeing you in the week two videos.\"},\n",
       "    {'lesson_number': '06',\n",
       "     'lesson_name': 'geoffrey-hinton-interview',\n",
       "     'content': \"As part of this course by deeplearning.ai, I\\nhope to not just teach you the technical ideas in deep learning, but\\nalso introduce you to some of the people, some of the heroes in deep learning.\\nThe people that invented so\\nmany of these ideas that you learn about in this course or in this specialization.\\nIn these videos, I hope to also ask these leaders of deep learning\\nto give you career advice for how you can break into deep learning, for\\nhow you can do research or find a job in deep learning.\\nAs the first of this interview series,\\nI am delighted to present to you an interview with Geoffrey Hinton.\\nWelcome Geoff, and thank you for doing this interview with deeplearning.ai.\\nThank you for inviting me.\\nI think that at this point you more than anyone else on this planet has\\ninvented so many of the ideas behind deep learning.\\nAnd a lot of people have been calling you the godfather of deep learning.\\nAlthough it wasn't until we were chatting a few minutes ago, until I realized\\nyou think I'm the first one to call you that, which I'm quite happy to have done.\\nBut what I want to ask is, many people know you as a legend,\\nI want to ask about your personal story behind the legend.\\nSo how did you get involved in, going way back, how did you get involved in AI and\\nmachine learning and neural networks?\\nSo when I was at high school, I had a classmate who was always\\nbetter than me at everything, he was a brilliant mathematician.\\nAnd he came into school one day and said, did you know the brain uses holograms?\\nAnd I guess that was about 1966, and I said, sort of what's a hologram?\\nAnd he explained that in a hologram you can chop off half of it, and\\nyou still get the whole picture.\\nAnd that memories in the brain might be distributed over the whole brain.\\nAnd so I guess he'd read about Lashley's experiments,\\nwhere you chop off bits of a rat's brain and\\ndiscover that it's very hard to find one bit where it stores one particular memory.\\nSo that's what first got me interested in how does the brain store memories.\\nAnd then when I went to university,\\nI started off studying physiology and physics.\\nI think when I was at Cambridge,\\nI was the only undergraduate doing physiology and physics.\\nAnd then I gave up on that and\\ntried to do philosophy, because I thought that might give me more insight.\\nBut that seemed to me actually\\nlacking in ways of distinguishing when they said something false.\\nAnd so then I switched to psychology.\\nAnd in psychology they had very, very simple theories, and it seemed to me\\nit was sort of hopelessly inadequate to explaining what the brain was doing.\\nSo then I took some time off and became a carpenter.\\nAnd then I decided that I'd try AI, and went of to Edinburgh,\\nto study AI with Langer Higgins.\\nAnd he had done very nice work on neural networks, and\\nhe'd just given up on neural networks, and been very impressed by Winograd's thesis.\\nSo when I arrived he thought I was kind of doing this old fashioned stuff, and\\nI ought to start on symbolic AI.\\nAnd we had a lot of fights about that, but I just kept on doing what I believed in.\\nAnd then what?\\nI eventually got a PhD in AI, and then I couldn't get a job in Britain.\\nBut I saw this very nice advertisement for\\nSloan Fellowships in California, and I managed to get one of those.\\nAnd I went to California, and everything was different there.\\nSo in Britain, neural nets was regarded as kind of silly,\\nand in California, Don Norman and\\nDavid Rumelhart were very open to ideas about neural nets.\\nIt was the first time I'd been somewhere where thinking about how the brain works,\\nand thinking about how that might relate to psychology,\\nwas seen as a very positive thing.\\nAnd it was a lot of fun there,\\nin particular collaborating with David Rumelhart was great.\\nI see, great. So this was when you were at UCSD, and\\nyou and Rumelhart around what, 1982,\\nwound up writing the seminal backprop paper, right?\\nActually, it was more complicated than that.\\nWhat happened?\\nIn, I think, early 1982,\\nDavid Rumelhart and me, and Ron Williams,\\nbetween us developed the backprop algorithm,\\nit was mainly David Rumelhart's idea.\\nWe discovered later that many other people had invented it.\\nDavid Parker had invented, it probably after us, but before we'd published.\\nPaul Werbos had published it already quite a few years earlier, but\\nnobody paid it much attention.\\nAnd there were other people who'd developed very similar algorithms,\\nit's not clear what's meant by backprop.\\nBut using the chain rule to get derivatives was not a novel idea.\\nI see, why do you think it was your paper that helped so\\nmuch the community latch on to backprop?\\nIt feels like your paper marked an infection in the acceptance of this\\nalgorithm, whoever accepted it.\\nSo we managed to get a paper into Nature in 1986.\\nAnd I did quite a lot of political work to get the paper accepted.\\nI figured out that one of the referees was probably going to be Stuart Sutherland,\\nwho was a well known psychologist in Britain.\\nAnd I went to talk to him for a long time, and\\nexplained to him exactly what was going on.\\nAnd he was very impressed by the fact\\nthat we showed that backprop could learn representations for words.\\nAnd you could look at those representations, which are little vectors,\\nand you could understand the meaning of the individual features.\\nSo we actually trained it on little triples of words about family trees,\\nlike Mary has mother Victoria.\\nAnd you'd give it the first two words, and it would have to predict the last word.\\nAnd after you trained it,\\nyou could see all sorts of features in the representations of the individual words.\\nLike the nationality of the person there,\\nwhat generation they were, which branch of the family tree they were in, and so on.\\nThat was what made Stuart Sutherland really impressed with it, and\\nI think that's why the paper got accepted.\\nVery early word embeddings, and you're already seeing learned\\nfeatures of semantic meanings emerge from the training algorithm.\\nYes, so from a psychologist's point of view, what was interesting was it unified\\ntwo completely different strands of ideas about what knowledge was like.\\nSo there was the old psychologist's view that a concept is just a big\\nbundle of features, and there's lots of evidence for that.\\nAnd then there was the AI view of the time, which is a formal structurist view.\\nWhich was that a concept is how it relates to other concepts.\\nAnd to capture a concept, you'd have to do something like a graph structure or\\nmaybe a semantic net.\\nAnd what this back propagation example showed was, you could give it\\nthe information that would go into a graph structure, or in this case a family tree.\\nAnd it could convert that information into features in such a way that it could then\\nuse the features to derive new consistent information, ie generalize.\\nBut the crucial thing was this to and fro between the graphical representation or\\nthe tree structured representation of the family tree, and\\na representation of the people as big feature vectors.\\nAnd in fact that from the graph-like representation you could get feature\\nvectors.\\nAnd from the feature vectors, you could get more of the graph-like representation.\\nSo this is 1986?\\nIn the early 90s, Bengio showed that you can actually take real data,\\nyou could take English text, and apply the same techniques there, and\\nget embeddings for real words from English text, and that impressed people a lot.\\nI guess recently we've been talking a lot about how fast computers like GPUs and\\nsupercomputers that's driving deep learning.\\nI didn't realize that back between 1986 and the early 90's, it sounds like between\\nyou and Benjio there was already the beginnings of this trend.\\nYes, it was a huge advance.\\nIn 1986, I was using a list machine which was less than a tenth of a mega flop.\\nAnd by about 1993 or thereabouts, people were seeing ten mega flops.\\nI see. So there was a factor of 100,\\nand that's the point at which is was easy to use,\\nbecause computers were just getting faster.\\nOver the past several decades, you've invented so\\nmany pieces of neural networks and deep learning.\\nI'm actually curious, of all of the things you've invented,\\nwhich of the ones you're still most excited about today?\\nSo I think the most beautiful one is the work I do with\\nTerry Sejnowski on Boltzmann machines.\\nSo we discovered there was this really,\\nreally simple learning algorithm that applied to great big\\ndensity connected nets where you could only see a few of the nodes.\\nSo it would learn hidden representations and it was a very simple algorithm.\\nAnd it looked like the kind of thing you should be able to get in a brain because\\neach synapse only needed to know about the behavior of the two\\nneurons it was directly connected to.\\nAnd the information that was propagated was the same.\\nThere were two different phases, which we called wake and sleep.\\nBut in the two different phases,\\nyou're propagating information in just the same way.\\nWhere as in something like back propagation, there's a forward pass and\\na backward pass, and they work differently.\\nThey're sending different kinds of signals.\\nSo I think that's the most beautiful thing.\\nAnd for many years it looked just like a curiosity,\\nbecause it looked like it was much too slow.\\nBut then later on, I got rid of a little bit of the beauty, and it started letting\\nme settle down and just use one iteration, in a somewhat simpler net.\\nAnd that gave restricted Boltzmann machines,\\nwhich actually worked effectively in practice.\\nSo in the Netflix competition, for example,\\nrestricted Boltzmann machines were one of the ingredients of the winning entry.\\nAnd in fact, a lot of the recent resurgence of neural net and\\ndeep learning, starting about 2007, was the restricted Boltzmann machine,\\nand derestricted Boltzmann machine work that you and your lab did.\\nYes so that's another of the pieces of work I'm very happy with,\\nthe idea of that you could train your restricted Boltzmann machine, which just\\nhad one layer of hidden features and you could learn one layer of feature.\\nAnd then you could treat those features as data and do it again, and\\nthen you could treat the new features you learned as data and do it again,\\nas many times as you liked.\\nSo that was nice, it worked in practice.\\nAnd then UY Tay realized that the whole thing could be treated as a single model,\\nbut it was a weird kind of model.\\nIt was a model where at the top you had a restricted Boltzmann machine, but\\nbelow that you had a Sigmoid belief net which was something that\\ninvented many years early.\\nSo it was a directed model and\\nwhat we'd managed to come up with by training these restricted Boltzmann\\nmachines was an efficient way of doing inferences in Sigmoid belief nets.\\nSo, around that time,\\nthere were people doing neural nets, who would use densely connected nets, but\\ndidn't have any good ways of doing probabilistic imprints in them.\\nAnd you had people doing graphical models, unlike my children,\\nwho could do inference properly, but only in sparsely connected nets.\\nAnd what we managed to show was the way of learning these deep\\nbelief nets so that there's an approximate form of inference that's very fast,\\nit's just hands in a single forward pass and that was a very beautiful result.\\nAnd you could guarantee that each time you learn that extra layer of features\\nthere was a band, each time you learned a new layer, you got a new band, and\\nthe new band was always better than the old band.\\nThe variational bands, showing as you add layers.\\nYes, I remember that video.\\nSo that was the second thing that I was really excited about.\\nAnd I guess the third thing was the work I did with on variational methods.\\nIt turns out people in statistics had done similar work earlier,\\nbut we didn't know about that.\\nSo we managed to make\\nEN work a whole lot better by showing you didn't need to do a perfect E step.\\nYou could do an approximate E step.\\nAnd EN was a big algorithm in statistics.\\nAnd we'd showed a big generalization of it.\\nAnd in particular, in 1993, I guess, with Van Camp.\\nI did a paper, with I think, the first variational Bayes paper,\\nwhere we showed that you could actually do a version of Bayesian learning\\nthat was far more tractable, by approximating the true posterior with a.\\nAnd you could do that in neural net.\\nAnd I was very excited by that.\\nI see. Wow, right.\\nYep, I think I remember all of these papers.\\nYou and Hinton, approximate Paper, spent many hours reading over that.\\nAnd I think some of the algorithms you use today, or\\nsome of the algorithms that lots of people use almost every day, are what,\\nthings like dropouts, or I guess ReLU activations came from your group?\\nYes and no.\\nSo other people have thought about rectified linear units.\\nAnd we actually did some work with restricted Boltzmann machines showing\\nthat a ReLU was almost exactly equivalent to a whole stack of logistic units.\\nAnd that's one of the things that helped ReLUs catch on.\\nI was really curious about that.\\nThe value paper had a lot of math showing that this function\\ncan be approximated with this really complicated formula.\\nDid you do that math so your paper would get accepted into an academic conference,\\nor did all that math really influence the development of max of 0 and x?\\nThat was one of the cases where actually the math was important\\nto the development of the idea.\\nSo I knew about rectified linear units, obviously, and\\nI knew about logistic units.\\nAnd because of the work on Boltzmann machines,\\nall of the basic work was done using logistic units.\\nAnd so the question was,\\ncould the learning algorithm work in something with rectified linear units?\\nAnd by showing the rectified linear units were almost exactly equivalent to a stack\\nof logistic units, we showed that all the math would go through.\\nI see.\\nAnd it provided the inspiration for today, tons of people use ReLU and\\nit just works without- Yeah.\\nWithout necessarily needing to understand the same motivation.\\nYeah, one thing I noticed later when I went to Google.\\nI guess in 2014, I gave a talk at Google about using ReLUs and\\ninitializing with the identity matrix.\\nbecause the nice thing about ReLUs is that if you keep replicating the hidden\\nlayers and you initialize with the identity,\\nit just copies the pattern in the layer below.\\nAnd so I was showing that you could train networks with 300 hidden layers and\\nyou could train them really efficiently if you initialize with their identity.\\nBut I didn't pursue that any further and I really regret not pursuing that.\\nWe published one paper with showing you could initialize an active\\nshowing you could initialize recurringness like that.\\nBut I should have pursued it further because Later on these residual\\nnetworks is really that kind of thing.\\nOver the years I've heard you talk a lot about the brain.\\nI've heard you talk about relationship being backprop and the brain.\\nWhat are your current thoughts on that?\\nI'm actually working on a paper on that right now.\\nI guess my main thought is this.\\nIf it turns out the back prop is a really good algorithm for doing learning.\\nThen for sure evolution could've figured out how to implement it.\\nI mean you have cells that could turn into either eyeballs or teeth.\\nNow, if cells can do that, they can for sure implement backpropagation and\\npresumably this huge selective pressure for it.\\nSo I think the neuroscientist idea that it doesn't look plausible is just silly.\\nThere may be some subtle implementation of it.\\nAnd I think the brain probably has something that may not be exactly be\\nbackpropagation, but it's quite close to it.\\nAnd over the years, I've come up with a number of ideas about how this might work.\\nSo in 1987, working with Jay McClelland,\\nI came up with the recirculation algorithm,\\nwhere the idea is you send information round a loop.\\nAnd you try to make it so\\nthat things don't change as information goes around this loop.\\nSo the simplest version would be you have input units and hidden units, and\\nyou send information from the input to the hidden and then back to the input, and\\nthen back to the hidden and then back to the input and so on.\\nAnd what you want, you want to train an autoencoder,\\nbut you want to train it without having to do backpropagation.\\nSo you just train it to try and get rid of all variation in the activities.\\nSo the idea is that the learning rule for\\nsynapse is change the weighting proportion to the presynaptic input and\\nin proportion to the rate of change at the post synaptic input.\\nBut in recirculation, you're trying to make the post synaptic input,\\nyou're trying to make the old one be good and the new one be bad, so\\nyou're changing in that direction.\\nWe invented this algorithm before neuroscientists come up with\\nspike-timing-dependent plasticity.\\nSpike-timing-dependent plasticity is actually the same algorithm but the other\\nway round, where the new thing is good and the old thing is bad in the learning rule.\\nSo you're changing the weighting proportions to the preset outlook activity\\ntimes the new person outlook activity minus the old one.\\nLater on I realized in 2007, that if you took a stack of\\nRestricted Boltzmann machines and you trained it up.\\nAfter it was trained, you then had exactly the right conditions for\\nimplementing backpropagation by just trying to reconstruct.\\nIf you looked at the reconstruction era, that reconstruction era would\\nactually tell you the derivative of the discriminative performance.\\nAnd at the first deep learning workshop at in 2007, I gave a talk about that.\\nThat was almost completely ignored.\\nLater on, Joshua Benjo, took up the idea and\\nthat's actually done quite a lot of more work on that.\\nAnd I've been doing more work on it myself.\\nAnd I think this idea that if you have a stack of autoencoders, then you can\\nget derivatives by sending activity backwards and locate reconstructionaires,\\nis a really interesting idea and may well be how the brain does it.\\nOne other topic that I know you follow about and that I hear you're still\\nworking on is how to deal with multiple time skills in deep learning?\\nSo, can you share your thoughts on that?\\nYes, so actually, that goes back to my first years of graduate student.\\nThe first talk I ever gave was about using what I called fast weights.\\nSo weights that adapt rapidly, but decay rapidly.\\nAnd therefore can hold short term memory.\\nAnd I showed in a very simple system in 1973 that you could do\\ntrue recursion with those weights.\\nAnd what I mean by true recursion is that the neurons that is used\\nin representing things get re-used for representing things in the recursive core.\\nAnd the weights that is used for\\nactually knowledge get re-used in the recursive core.\\nAnd so that leads the question of when you pop out your recursive core,\\nhow do you remember what it was you were in the middle of doing?\\nWhere's that memory?\\nbecause you used the neurons for the recursive core.\\nAnd the answer is you can put that memory into fast weights, and\\nyou can recover the activities neurons from those fast weights.\\nAnd more recently working with Jimmy Ba,\\nwe actually got a paper in it by using fast weights for recursion like that.\\nI see.\\nSo that was quite a big gap.\\nThe first model was unpublished in 1973 and\\nthen Jimmy Ba's model was in 2015, I think, or 2016.\\nSo it's about 40 years later.\\nAnd, I guess, one other idea of Quite a few years now,\\nover five years, I think is capsules, where are you with that?\\nOkay, so I'm back to the state I'm used to being in.\\nWhich is I have this idea I really believe in and nobody else believes it.\\nAnd I submit papers about it and they would get rejected.\\nBut I really believe in this idea and I'm just going to keep pushing it.\\nSo it hinges on, there's a couple of key ideas.\\nOne is about how you represent multi dimensional entities, and you\\ncan represent multi-dimensional entities by just a little backdoor activities.\\nAs long as you know there's any one of them.\\nSo the idea is in each region of the image, you'll assume there's at most,\\none of the particular kind of feature.\\nAnd then you'll use a bunch of neurons, and\\ntheir activities will represent the different aspects to that feature,\\nlike within that region exactly what are its x and y coordinates?\\nWhat orientation is it at?\\nHow fast is it moving?\\nWhat color is it?\\nHow bright is it?\\nAnd stuff like that.\\nSo you can use a whole bunch of neurons to represent different dimensions of\\nthe same thing.\\nProvided there's only one of them.\\nThat's a very different way of doing representation\\nfrom what we're normally used to in neural nets.\\nNormally in neural nets, we just have a great big layer,\\nand all the units go off and do whatever they do.\\nBut you don't think of bundling them up into little groups that represent\\ndifferent coordinates of the same thing.\\nSo I think we should beat this extra structure.\\nAnd then the other idea that goes with that.\\nSo this means in the trut 367 00:23:09,280 --&gt; 00:23:11,270 Yes. To different subsets.\\nYes. To represent, right, rather than-\\nI call each of those subsets a capsule.\\nI see.\\nAnd the idea is a capsule is able to represent an instance of a feature, but\\nonly one.\\nAnd it represents all the different properties of that feature.\\nIt's a feature that has a lot of properties as opposed to\\na normal neuron and normal neural nets, which has just one scale of property.\\nYeah, I see yep.\\nAnd then what you can do if you've got that, is you can do something that normal\\nneural nets are very bad at, which is you can do what I call routine by agreement.\\nSo let's suppose you want to do segmentation and\\nyou have something that might be a mouth and something else that might be a nose.\\nAnd you want to know if you should put them together to make one thing.\\nSo the idea should have a capsule for\\na mouth that has the parameters of the mouth.\\nAnd you have a capsule for a nose that has the parameters of the nose.\\nAnd then to decipher whether to put them together or\\nnot, you get each of them to vote for what the parameters should be for a face.\\nNow if the mouth and the nose are in the right spacial relationship,\\nthey will agree.\\nSo when you get two captures at one level voting for the same set of parameters at\\nthe next level up, you can assume they're probably right,\\nbecause agreement in a high dimensional space is very unlikely.\\nAnd that's a very different way of doing filtering,\\nthan what we normally use in neural nets.\\nSo I think this routing by agreement is going to be crucial for\\ngetting neural nets to generalize much better from limited data.\\nI think it'd be very good at getting the changes in viewpoint,\\nvery good at doing segmentation.\\nAnd I'm hoping it will be much more statistically efficient than what we\\ncurrently do in neural nets.\\nWhich is, if you want to deal with changes in viewpoint,\\nyou just give it a whole bunch of changes in view point and training on them all.\\nI see, right, so rather than FIFO learning, supervised learning,\\nyou can learn this in some different way.\\nWell, I still plan to do it with supervised learning, but\\nthe mechanics of the forward paths are very different.\\nIt's not a pure forward path in the sense that there's little bits of iteration\\ngoing on, where you think you found a mouth and you think you found a nose.\\nAnd use a little bit of iteration to decide\\nwhether they should really go together to make a face.\\nAnd you can do back props from that iteration.\\nSo you can try and do it a little discriminatively,\\nand we're working on that now at my group in Toronto.\\nSo I now have a little Google team in Toronto, part of the Brain team.\\nThat's what I'm excited about right now.\\nI see, great, yeah.\\nLook forward to that paper when that comes out.\\nYeah, if it comes out [LAUGH].\\nYou worked in deep learning for several decades.\\nI'm actually really curious, how has your thinking,\\nyour understanding of AI changed over these years?\\nSo I guess a lot of my intellectual history has been around back propagation,\\nand how to use back propagation, how to make use of its power.\\nSo to begin with, in the mid 80s, we were using it for\\ndiscriminative learning and it was working well.\\nI then decided, by the early 90s,\\nthat actually most human learning was going to be unsupervised learning.\\nAnd I got much more interested in unsupervised learning, and\\nthat's when I worked on things like the Wegstein algorithm.\\nAnd your comments at that time really influenced my thinking as well.\\nSo when I was leading Google Brain, our first project spent a lot of\\nwork in unsupervised learning because of your influence.\\nRight, and I may have misled you.\\nBecause in the long run,\\nI think unsupervised learning is going to be absolutely crucial.\\nBut you have to sort of face reality.\\nAnd what's worked over the last ten years or so is supervised learning.\\nDiscriminative training, where you have labels, or\\nyou're trying to predict the next thing in the series, so that acts as the label.\\nAnd that's worked incredibly well.\\nI still believe that unsupervised learning is going to be crucial, and things will\\nwork incredibly much better than they do now when we get that working properly, but\\nwe haven't yet.\\nYeah, I think many of the senior people in deep learning,\\nincluding myself, remain very excited about it.\\nIt's just none of us really have almost any idea how to do it yet.\\nMaybe you do, I don't feel like I do.\\nVariational altering code is where you use the reparameterization tricks.\\nSeemed to me like a really nice idea.\\nAnd generative adversarial nets also seemed to me to be a really nice idea.\\nI think generative adversarial nets are one of\\nthe sort of biggest ideas in deep learning that's really new.\\nI'm hoping I can make capsules that successful, but\\nright now generative adversarial nets, I think, have been a big breakthrough.\\nWhat happened to sparsity and slow features,\\nwhich were two of the other principles for building unsupervised models?\\nI was never as big on sparsity as you were, buddy.\\nBut slow features, I think, is a mistake.\\nYou shouldn't say slow.\\nThe basic idea is right, but you shouldn't go for features that don't change,\\nyou should go for features that change in predictable ways.\\nSo here's a sort of basic principle about how you model anything.\\nYou take your measurements, and you're applying nonlinear\\ntransformations to your measurements until you get to\\na representation as a state vector in which the action is linear.\\nSo you don't just pretend it's linear like you do with common filters.\\nBut you actually find a transformation from the observables to\\nthe underlying variables where linear operations,\\nlike matrix multipliers on the underlying variables, will do the work.\\nSo for example, if you want to change viewpoints.\\nIf you want to produce the image from another viewpoint,\\nwhat you should do is go from the pixels to coordinates.\\nAnd once you got to the coordinate representation,\\nwhich is a kind of thing I'm hoping captures will find.\\nYou can then do a matrix multiplier to change viewpoint, and\\nthen you can map it back to pixels.\\nRight, that's why you did all that.\\nI think that's a very, very general principle.\\nThat's why you did all that work on face synthesis, right?\\nWhere you take a face and compress it to very low dimensional vector, and so\\nyou can fiddle with that and get back other faces.\\nI had a student who worked on that, I didn't do much work on that myself.\\nNow I'm sure you still get asked all the time,\\nif someone wants to break into deep learning, what should they do?\\nSo what advice would you have?\\nI'm sure you've given a lot of advice to people in one on one settings, but for\\nthe global audience of people watching this video.\\nWhat advice would you have for them to get into deep learning?\\nOkay, so my advice is sort of read the literature, but don't read too much of it.\\nSo this is advice I got from my advisor, which is very unlike what most people say.\\nMost people say you should spend several years reading the literature and\\nthen you should start working on your own ideas.\\nAnd that may be true for some researchers, but for creative researchers I think\\nwhat you want to do is read a little bit of the literature.\\nAnd notice something that you think everybody is doing wrong,\\nI'm contrary in that sense.\\nYou look at it and it just doesn't feel right.\\nAnd then figure out how to do it right.\\nAnd then when people tell you, that's no good, just keep at it.\\nAnd I have a very good principle for helping people keep at it,\\nwhich is either your intuitions are good or they're not.\\nIf your intuitions are good, you should follow them and\\nyou'll eventually be successful.\\nIf your intuitions are not good, it doesn't matter what you do.\\nI see [LAUGH].\\nInspiring advice, might as well go for it.\\nYou might as well trust your intuitions.\\nThere's no point not trusting them.\\nI see, yeah.\\nI usually advise people to not just read, but replicate published papers.\\nAnd maybe that puts a natural limiter on how many you could do,\\nbecause replicating results is pretty time consuming.\\nYes, it's true that when you're trying to replicate a published\\nyou discover all over little tricks necessary to make it work.\\nThe other advice I have is, never stop programming.\\nBecause if you give a student something to do, if they're a bad student,\\nthey'll come back and say, it didn't work.\\nAnd the reason it didn't work would be some little decision they made,\\nthat they didn't realize is crucial.\\nAnd if you give it to a good student, like for example.\\nYou can give him anything and he'll come back and say, it worked.\\nI remember doing this once, and I said, but wait a minute.\\nSince we last talked,\\nI realized it couldn't possibly work for the following reason.\\nAnd said, yeah, I realized that right away, so I assumed you didn't mean that.\\n[LAUGH] I see, yeah, that's great, yeah.\\nLet's see, any other advice for\\npeople that want to break into AI and deep learning?\\nI think that's basically, read enough so you start developing intuitions.\\nAnd then, trust your intuitions and go for it,\\ndon't be too worried if everybody else says it's nonsense.\\nAnd I guess there's no way to know if others are right or\\nwrong when they say it's nonsense, but you just have to go for it, and then find out.\\nRight, but there is one thing, which is, if you think it's a really good idea,\\nand other people tell you it's complete nonsense,\\nthen you know you're really on to something.\\nSo one example of that is when and I first came up with variational methods.\\nI sent mail explaining it to a former student of mine called Peter Brown,\\nwho knew a lot about.\\nAnd he showed it to people who worked with him,\\ncalled the brothers, they were twins, I think.\\nAnd he then told me later what they said, and they said,\\neither this guy's drunk, or he's just stupid, so\\nthey really, really thought it was nonsense.\\nNow, it could have been partly the way I explained it,\\nbecause I explained it in intuitive terms.\\nBut when you have what you think is a good idea and\\nother people think is complete rubbish, that's the sign of a really good idea.\\nI see, and research topics,\\nnew grad students should work on capsules and\\nmaybe unsupervised learning, any other?\\nOne good piece of advice for new grad students is,\\nsee if you can find an advisor who has beliefs similar to yours.\\nBecause if you work on stuff that your advisor feels deeply about,\\nyou'll get a lot of good advice and time from your advisor.\\nIf you work on stuff your advisor's not interested in,\\nall you'll get is, you get some advice, but it won't be nearly so useful.\\nI see, and last one on advice for learners,\\nhow do you feel about people entering a PhD program?\\nVersus joining a top company, or a top research group?\\nYeah, it's complicated, I think right now, what's happening is,\\nthere aren't enough academics trained in deep learning to educate all the people\\nthat we need educated in universities.\\nThere just isn't the faculty bandwidth there, but\\nI think that's going to be temporary.\\nI think what's happened is, most departments have been very slow to\\nunderstand the kind of revolution that's going on.\\nI kind of agree with you, that it's not quite a second industrial revolution, but\\nit's something on nearly that scale.\\nAnd there's a huge sea change going on,\\nbasically because our relationship to computers has changed.\\nInstead of programming them, we now show them, and they figure it out.\\nThat's a completely different way of using computers, and\\ncomputer science departments are built around the idea of programming computers.\\nAnd they don't understand that sort of,\\nthis showing computers is going to be as big as programming computers.\\nExcept they don't understand that half the people in the department should be people\\nwho get computers to do things by showing them.\\nSo my department refuses to acknowledge that it should have lots and\\nlots of people doing this.\\nThey think they got a couple, maybe a few more, but not too many.\\nAnd in that situation,\\nyou have to remind the big companies to do quite a lot of the training.\\nSo Google is now training people, we call brain residence,\\nI suspect the universities will eventually catch up.\\nI see, right, in fact, maybe a lot of students have figured this out.\\nA lot of top 50 programs, over half of the applicants are actually\\nwanting to work on showing, rather than programming.\\nYeah, cool, yeah, in fact, to give credit where it's due,\\nwhereas a deep learning AI is creating a deep learning specialization.\\nAs far as I know, their first deep learning MOOC was actually yours taught\\non Coursera, back in 2012, as well.\\nAnd somewhat strangely,\\nthat's when you first published the RMS algorithm, which also is a rough.\\nRight, yes, well, as you know, that was because you invited me to do the MOOC.\\nAnd then when I was very dubious about doing, you kept pushing me to do it, so\\nit was very good that I did, although it was a lot of work.\\nYes, and thank you for doing that, I remember you complaining to me,\\nhow much work it was.\\nAnd you staying out late at night, but I think many, many learners have\\nbenefited for your first MOOC, so I'm very grateful to you for it, so.\\nThat's good, yeah Yeah, over the years,\\nI've seen you embroiled in debates about paradigms for AI, and\\nwhether there's been a paradigm shift for AI.\\nWhat are your, can you share your thoughts on that?\\nYes, happily, so I think that in the early days, back in the 50s,\\npeople like von Neumann and Turing didn't believe in symbolic AI,\\nthey were far more inspired by the brain.\\nUnfortunately, they both died much too young, and their voice wasn't heard.\\nAnd in the early days of AI,\\npeople were completely convinced that the representations you need for\\nintelligence were symbolic expressions of some kind.\\nSort of cleaned up logic, where you could do non-monotonic things, and not quite\\nlogic, but something like logic, and that the essence of intelligence was reasoning.\\nWhat's happened now is, there's a completely different view,\\nwhich is that what a thought is, is just a great big vector of neural activity,\\nso contrast that with a thought being a symbolic expression.\\nAnd I think the people who thought that thoughts were symbolic expressions just\\nmade a huge mistake.\\nWhat comes in is a string of words, and what comes out is a string of words.\\nAnd because of that, strings of words are the obvious way to represent things.\\nSo they thought what must be in between was a string of words, or\\nsomething like a string of words.\\nAnd I think what's in between is nothing like a string of words.\\nI think the idea that thoughts must be in some kind of language is as silly as\\nthe idea that understanding the layout of a spatial scene\\nmust be in pixels, pixels come in.\\nAnd if we could, if we had a dot matrix printer attached to us,\\nthen pixels would come out, but what's in between isn't pixels.\\nAnd so I think thoughts are just these great big vectors, and\\nthat big vectors have causal powers.\\nThey cause other big vectors, and\\nthat's utterly unlike the standard AI view that thoughts are symbolic expressions.\\nI see, good,\\nI guess AI is certainly coming round to this new point of view these days.\\nSome of it,\\nI think a lot of people in AI still think thoughts have to be symbolic expressions.\\nThank you very much for doing this interview.\\nIt was fascinating to hear how deep learning has evolved over the years,\\nas well as how you're still helping drive it into the future, so thank you, Jeff.\"}],\n",
       "   'generated_content': Code(topic='Introduction to Deep Learning', cell=[CellFormat(cell_type='markdown', cell_no=1, cell_content='# Chapter 1: Introduction to Deep Learning'), CellFormat(cell_type='markdown', cell_no=2, cell_content='## 1.1 Welcome to the Specialization'), CellFormat(cell_type='markdown', cell_no=3, cell_content='Deep Learning is transforming industries from web search to healthcare, personalized education, precision agriculture, and self-driving cars. This specialization aims to equip you with the tools to apply deep learning and confidently add it to your resume. AI is the new electricity, poised to bring about an equally significant transformation.'), CellFormat(cell_type='markdown', cell_no=4, cell_content='### Specialization Overview (5 Courses):'), CellFormat(cell_type='markdown', cell_no=5, cell_content='- **Course 1: Neural Networks and Deep Learning (4 weeks)**\\n  - Foundations of neural networks, including deep neural networks.\\n  - How to build and train a neural network on data.\\n  - *Project: Build a cat recognizer.*'), CellFormat(cell_type='markdown', cell_no=6, cell_content='- **Course 2: Improving Deep Neural Networks: Hyperparameter Tuning, Regularization and Optimization (3 weeks)**\\n  - Practical aspects of deep learning: getting networks to perform well.\\n  - Topics: hyperparameter tuning, regularization, diagnosing bias/variance, advanced optimization algorithms (Momentum, RMSprop, Adam).'), CellFormat(cell_type='markdown', cell_no=7, cell_content='- **Course 3: Structuring Machine Learning Projects (2 weeks)**\\n  - Strategy for building ML systems in the deep learning era.\\n  - Data splitting (train, dev/holdout, test sets).\\n  - Dealing with different training/test distributions.\\n  - End-to-end deep learning: when to use it.\\n  - Shares hard-won lessons not commonly taught in universities.'), CellFormat(cell_type='markdown', cell_no=8, cell_content='- **Course 4: Convolutional Neural Networks (CNNs)**\\n  - Learn about CNNs, primarily applied to image data.'), CellFormat(cell_type='markdown', cell_no=9, cell_content='- **Course 5: Sequence Models**\\n  - Focus on recurrent neural networks (RNNs) and Long Short-Term Memory (LSTM) models.\\n  - Applications: Natural Language Processing (NLP), speech recognition, music generation.'), CellFormat(cell_type='markdown', cell_no=10, cell_content='These courses will equip you with deep learning tools, enable you to build amazing applications, and help advance your career.'), CellFormat(cell_type='markdown', cell_no=11, cell_content='## 1.2 What is a Neural Network?'), CellFormat(cell_type='markdown', cell_no=12, cell_content=\"Deep Learning involves training Neural Networks, which can sometimes be very large. Let's build intuition starting with a simple example.\"), CellFormat(cell_type='markdown', cell_no=13, cell_content='### Housing Price Prediction Example'), CellFormat(cell_type='markdown', cell_no=14, cell_content='- **Input (x):** Size of the house (sq ft/meters).\\n- **Output (y):** Price of the house.\\n- A basic model might use linear regression, but prices cannot be negative. Thus, the function is bent to output zero for small sizes and then follows a linear trend.'), CellFormat(cell_type='markdown', cell_no=15, cell_content='This function (zero then linear) is a very simple neural network with a single neuron. This neuron takes the size as input, computes a linear function, takes the `max(0, result)`, and outputs the estimated price. This specific activation function is called a **ReLU (Rectified Linear Unit)**.'), CellFormat(cell_type='markdown', cell_no=16, cell_content='### Building a Larger Neural Network'), CellFormat(cell_type='markdown', cell_no=17, cell_content=\"Larger neural networks are formed by stacking many such single neurons (like Lego bricks). Instead of just size, consider additional features:\\n- **Input Features (x):** Size, #bedrooms, zip code, wealth of neighborhood.\\n- These inputs can feed into 'hidden units' (neurons) in a middle layer.\\n- **Hidden Units:** Can represent intermediate concepts like:\\n    - `Family Size` (from size and #bedrooms)\\n    - `Walkability` (from zip code)\\n    - `School Quality` (from zip code and wealth)\\n- These intermediate concepts then combine to predict the final **Output (y):** Price.\"), CellFormat(cell_type='markdown', cell_no=18, cell_content='In practice, when you implement a neural network, you provide the input `x` and output `y` for training examples. The network *learns* the connections and intermediate features by itself. Each hidden unit typically takes input from *all* features in the preceding layer, making them **densely connected**.'), CellFormat(cell_type='markdown', cell_no=19, cell_content='Neural networks are remarkably good at figuring out complex functions that map from `x` to `y` when given enough training data. They are particularly powerful in **supervised learning** settings.'), CellFormat(cell_type='markdown', cell_no=20, cell_content='## 1.3 Supervised Learning with Neural Networks'), CellFormat(cell_type='markdown', cell_no=21, cell_content='Most economic value from neural networks comes from **supervised learning**, where you learn a function mapping an input `x` to an output `y`.'), CellFormat(cell_type='markdown', cell_no=22, cell_content='### Examples of Supervised Learning Applications'), CellFormat(cell_type='markdown', cell_no=23, cell_content='- **Online Advertising:** Input (ad info, user info) -> Output (click prediction). Highly lucrative.\\n- **Computer Vision:** Input (image) -> Output (object index for photo tagging).\\n- **Speech Recognition:** Input (audio clip) -> Output (text transcript).\\n- **Machine Translation:** Input (English sentence) -> Output (Chinese sentence).\\n- **Autonomous Driving:** Input (image, radar info) -> Output (position of other cars).'), CellFormat(cell_type='markdown', cell_no=24, cell_content='A key aspect of applying neural networks effectively is choosing the right `x` and `y` for your problem, and then integrating this supervised learning component into a larger system.'), CellFormat(cell_type='markdown', cell_no=25, cell_content='### Different Neural Network Architectures'), CellFormat(cell_type='markdown', cell_no=26, cell_content='Different types of neural networks are best suited for different data types:\\n- **Standard Neural Networks:** Used for applications like real estate prediction and online advertising.\\n- **Convolutional Neural Networks (CNNs):** Often used for image applications.\\n- **Recurrent Neural Networks (RNNs):** Ideal for sequence data, such as audio (temporal sequence) and natural language (sequence of words).\\n- **Hybrid Architectures:** For complex problems like autonomous driving (combining image and radar data), a custom or hybrid architecture might be used.'), CellFormat(cell_type='markdown', cell_no=27, cell_content='### Structured vs. Unstructured Data'), CellFormat(cell_type='markdown', cell_no=28, cell_content='- **Structured Data:** Organized databases where each feature has a clear, well-defined meaning (e.g., house size, number of bedrooms, user age, ad information).\\n- **Unstructured Data:** Raw data like audio, images (pixel values), or text (individual words). Historically, computers struggled with this, but deep learning has made significant strides in interpreting it.'), CellFormat(cell_type='markdown', cell_no=29, cell_content=\"Neural networks' ability to process unstructured data has opened up many new applications (speech, image, NLP). While media often highlights unstructured data successes, deep learning also creates substantial economic value on structured data (e.g., improved advertising, product recommendations).\"), CellFormat(cell_type='markdown', cell_no=30, cell_content='## 1.4 Why is Deep Learning Taking Off?'), CellFormat(cell_type='markdown', cell_no=31, cell_content='The core technical ideas behind neural networks have existed for decades. Their recent success is driven by three main factors:'), CellFormat(cell_type='markdown', cell_no=32, cell_content='### 1. Data Scale'), CellFormat(cell_type='markdown', cell_no=33, cell_content='- **Traditional algorithms** (SVM, logistic regression) see performance improvements with more data, but eventually plateau.\\n- **Neural Networks**, especially **very large ones**, show continuous performance improvement as the amount of labeled data (`m`) increases.\\n- The digitization of society, mobile apps, websites, and sensors (cell phones, IoT) has led to an explosion of data, which large neural networks can effectively leverage. This phenomenon is often referred to as **scale** (both network size and data size).'), CellFormat(cell_type='markdown', cell_no=34, cell_content='```mermaid\\ngraph TD\\n    A[Amount of Data] --> B{Performance of Learning Algorithms}\\n    B --> C[Traditional Algorithms: Plateaus]\\n    B --> D[Small Neural Network: Plateaus sooner]\\n    B --> E[Medium Neural Network: Plateaus later]\\n    B --> F[Very Large Neural Network: Continues to improve]\\n```'), CellFormat(cell_type='markdown', cell_no=35, cell_content='In regimes with smaller datasets, feature engineering skill is more critical. However, in the big data regime, large neural networks consistently outperform other approaches.'), CellFormat(cell_type='markdown', cell_no=36, cell_content='### 2. Computational Scale'), CellFormat(cell_type='markdown', cell_no=37, cell_content='- The ability to train very large neural networks efficiently on hardware like CPUs and GPUs has been crucial.\\n- **Algorithmic innovations** have also significantly sped up computation.\\n    - **Example: Sigmoid vs. ReLU:** Switching from sigmoid activation functions (which have flat regions with near-zero gradients, slowing learning) to ReLU (which has a gradient of 1 for positive inputs) has made gradient descent algorithms much faster.'), CellFormat(cell_type='markdown', cell_no=38, cell_content='- **Faster Computation enables faster iteration:** The process of training a neural network involves an iterative cycle: *Idea -> Code -> Experiment -> Analyze -> Idea*. Quicker experiment results (minutes/hours instead of months) allow practitioners and researchers to try more ideas and iterate much faster, accelerating progress.'), CellFormat(cell_type='markdown', cell_no=39, cell_content='These trends (more data, better computation through hardware and algorithms) are still strongly at play, suggesting continued improvement in deep learning for years to come.'), CellFormat(cell_type='markdown', cell_no=40, cell_content='## 1.5 About This Course (Course 1, Week 1)'), CellFormat(cell_type='markdown', cell_no=41, cell_content=\"This course (Course 1 of 5) covers the foundational building blocks of deep learning. By the end, you'll be able to build and run a deep neural network.\"), CellFormat(cell_type='markdown', cell_no=42, cell_content='### Course Structure (4 Weeks)'), CellFormat(cell_type='markdown', cell_no=43, cell_content=\"- **Week 1:** Introduction to Deep Learning (what you've just covered).\\n  - Includes 10 multiple-choice questions to check understanding.\"), CellFormat(cell_type='markdown', cell_no=44, cell_content='- **Week 2:** Basics of Neural Network Programming.\\n  - Learn `forward propagation` and `back propagation` steps.\\n  - Efficient implementation of neural networks.\\n  - Includes a programming exercise.'), CellFormat(cell_type='markdown', cell_no=45, cell_content='- **Week 3:** Coding a Single Hidden Layer Neural Network.\\n  - Implement key concepts for a neural network with one hidden layer.'), CellFormat(cell_type='markdown', cell_no=46, cell_content='- **Week 4:** Building a Deep Neural Network.\\n  - Construct a neural network with multiple layers.'), CellFormat(cell_type='markdown', cell_no=47, cell_content='Congratulations on completing the Week 1 videos! Make sure to complete the accompanying multiple-choice questions to solidify your understanding.'), CellFormat(cell_type='markdown', cell_no=48, cell_content='## 1.6 Interview with Geoffrey Hinton'), CellFormat(cell_type='markdown', cell_no=49, cell_content=\"An interview with Geoffrey Hinton, often called the 'godfather of deep learning,' discussing his journey, key inventions, and insights.\"), CellFormat(cell_type='markdown', cell_no=50, cell_content='### Personal Story'), CellFormat(cell_type='markdown', cell_no=51, cell_content='- Interest in brain memory sparked in high school by a friend discussing holograms and distributed memories.\\n- Explored various fields (physiology, physics, philosophy, psychology) before pursuing AI in Edinburgh.\\n- Advocated for neural networks despite initial resistance in Britain, then found openness at UCSD (California).\\n- Collaborated with David Rumelhart.'), CellFormat(cell_type='markdown', cell_no=52, cell_content='### Backpropagation'), CellFormat(cell_type='markdown', cell_no=53, cell_content=\"- Developed with David Rumelhart and Ron Williams in 1982, though others (Paul Werbos, David Parker) had similar ideas earlier.\\n- Their 1986 *Nature* paper was seminal due to political work and demonstrating backprop's ability to learn word representations (early word embeddings for family trees).\\n- Showed that backprop could unify psychological views of concepts (feature bundles) and AI views (relations between concepts) by converting graph information into feature vectors and vice versa.\\n- Early 1990s: Yoshua Bengio applied similar techniques to real English text.\"), CellFormat(cell_type='markdown', cell_no=54, cell_content=\"### Hinton's Most Exciting Inventions\"), CellFormat(cell_type='markdown', cell_no=55, cell_content='1.  **Boltzmann Machines (with Terry Sejnowski):** A beautiful, simple learning algorithm for densely connected nets that learn hidden representations. Inspired by brain function, with wake and sleep phases for information propagation. Initially too slow in practice.'), CellFormat(cell_type='markdown', cell_no=56, cell_content='2.  **Restricted Boltzmann Machines (RBMs) and Deep Belief Nets:** Simplified Boltzmann Machines that were practical. A stack of RBMs could be trained layer-by-layer to form Deep Belief Nets, enabling efficient inference and guaranteed performance improvements with each added layer. This work (around 2007) was a significant catalyst for the resurgence of deep learning.'), CellFormat(cell_type='markdown', cell_no=57, cell_content='3.  **Variational Methods (with David Van Camp, 1993):** Showed how to make Bayesian learning more tractable by approximating the true posterior distribution, a significant generalization of the EM algorithm.'), CellFormat(cell_type='markdown', cell_no=58, cell_content='### Other Contributions and Current Work'), CellFormat(cell_type='markdown', cell_no=59, cell_content=\"- **ReLU Activations:** His group's work showing ReLU's equivalence to a stack of logistic units helped its adoption.\\n- **Initializing with Identity Matrix:** For ReLUs, enables training very deep networks (e.g., 300 layers) efficiently, pre-dating residual networks.\\n- **Backprop and the Brain:** Believes the brain likely implements an algorithm similar to backpropagation. Explored `recirculation algorithm` (1987) and using RBM reconstruction error for derivatives (2007) as brain-plausible mechanisms.\\n- **Multiple Time Scales / Fast Weights:** Early work (1973) on \")])},\n",
       "  {'chapter_number': '2',\n",
       "   'chapter_name': 'Neural_networks_basics',\n",
       "   'lessons': [{'lesson_number': '01',\n",
       "     'lesson_name': 'binary-classification',\n",
       "     'content': \"Hello, and welcome back.\\nIn this week we're going to go over the basics of neural network programming.\\nIt turns out that when you implement a neural network there\\nare some techniques that are going to be really important.\\nFor example, if you have a training set of m training examples,\\nyou might be used to processing the training set by having a four loop\\nstep through your m training examples.\\nBut it turns out that when you're implementing a neural network,\\nyou usually want to process your entire training set\\nwithout using an explicit four loop to loop over your entire training set.\\nSo, you'll see how to do that in this week's materials.\\nAnother idea, when you organize the computation of a neural network,\\nusually you have what's called a forward pass or forward propagation step,\\nfollowed by a backward pass or what's called a backward propagation step.\\nAnd so in this week's materials, you also get an introduction about why\\nthe computations in learning a neural network can be organized in this forward\\npropagation and a separate backward propagation.\\nFor this week's materials I want to convey these ideas using\\nlogistic regression in order to make the ideas easier to understand.\\nBut even if you've seen logistic regression before, I think that there'll\\nbe some new and interesting ideas for you to pick up in this week's materials.\\nSo with that, let's get started.\\nLogistic regression is an algorithm for binary classification.\\nSo let's start by setting up the problem.\\nHere's an example of a binary classification problem.\\nYou might have an input of an image, like that, and\\nwant to output a label to recognize this image as either being a cat,\\nin which case you output 1, or not-cat in which case you output 0,\\nand we're going to use y to denote the output label.\\nLet's look at how an image is represented in a computer.\\nTo store an image your computer stores three separate matrices\\ncorresponding to the red, green, and blue color channels of this image.\\nSo if your input image is 64 pixels by 64 pixels,\\nthen you would have 3 64 by 64 matrices\\ncorresponding to the red, green and blue pixel intensity values for your images.\\nAlthough to make this little slide I drew these as much smaller matrices, so\\nthese are actually 5 by 4 matrices rather than 64 by 64.\\nSo to turn these pixel intensity values- Into a feature vector, what we're\\ngoing to do is unroll all of these pixel values into an input feature vector x.\\nSo to unroll all these pixel intensity values into a feature vector, what we're\\ngoing to do is define a feature vector x corresponding to this image as follows.\\nWe're just going to take all the pixel values 255, 231, and so on.\\n255, 231, and so on until we've listed all the red pixels.\\nAnd then eventually 255 134 255, 134 and so\\non until we get a long feature vector listing out all the red,\\ngreen and blue pixel intensity values of this image.\\nIf this image is a 64 by 64 image, the total dimension\\nof this vector x will be 64 by 64 by 3 because that's\\nthe total numbers we have in all of these matrixes.\\nWhich in this case, turns out to be 12,288,\\nthat's what you get if you multiply all those numbers.\\nAnd so we're going to use nx=12288\\nto represent the dimension of the input features x.\\nAnd sometimes for brevity, I will also just use lowercase n\\nto represent the dimension of this input feature vector.\\nSo in binary classification, our goal is to learn a classifier that can input\\nan image represented by this feature vector x.\\nAnd predict whether the corresponding label y is 1 or 0,\\nthat is, whether this is a cat image or a non-cat image.\\nLet's now lay out some of the notation that we'll\\nuse throughout the rest of this course.\\nA single training example is represented by a pair,\\n(x,y) where x is an x-dimensional feature\\nvector and y, the label, is either 0 or 1.\\nYour training sets will comprise lower-case m training examples.\\nAnd so your training sets will be written (x1, y1) which is the input and\\noutput for your first training example (x(2), y(2)) for\\nthe second training example up to (xm, ym) which is your last training example.\\nAnd then that altogether is your entire training set.\\nSo I'm going to use lowercase m to denote the number of training samples.\\nAnd sometimes to emphasize that this is the number of train examples,\\nI might write this as M = M train.\\nAnd when we talk about a test set,\\nwe might sometimes use m subscript test to denote the number of test examples.\\nSo that's the number of test examples.\\nFinally, to output all of the training examples into a more compact notation,\\nwe're going to define a matrix, capital X.\\nAs defined by taking you training set inputs x1, x2 and\\nso on and stacking them in columns.\\nSo we take X1 and put that as a first column of this matrix,\\nX2, put that as a second column and so on down to Xm,\\nthen this is the matrix capital X.\\nSo this matrix X will have M columns, where M is the number of train\\nexamples and the number of railroads, or the height of this matrix is NX.\\nNotice that in other causes, you might see the matrix capital\\nX defined by stacking up the train examples in rows like so,\\nX1 transpose down to Xm transpose.\\nIt turns out that when you're implementing neural networks using\\nthis convention I have on the left, will make the implementation much easier.\\nSo just to recap, x is a nx by m dimensional matrix, and\\nwhen you implement this in Python,\\nyou see that x.shape, that's the python command for\\nfinding the shape of the matrix, that this an nx, m.\\nThat just means it is an nx by m dimensional matrix.\\nSo that's how you group the training examples, input x into matrix.\\nHow about the output labels Y?\\nIt turns out that to make your implementation of a neural network easier,\\nit would be convenient to also stack Y In columns.\\nSo we're going to define capital Y to be equal to Y 1, Y 2,\\nup to Y m like so.\\nSo Y here will be a 1 by m dimensional matrix.\\nAnd again, to use the notation without the shape of Y will be 1, m.\\nWhich just means this is a 1 by m matrix.\\nAnd as you implement your neural network later in this course you'll find that a useful\\nconvention would be to take the data associated with different training\\nexamples, and by data I mean either x or y, or other quantities you see later.\\nBut to take the stuff or\\nthe data associated with different training examples and\\nto stack them in different columns, like we've done here for both x and y.\\nSo, that's a notation we'll use for a logistic regression and for\\nneural networks networks later in this course.\\nIf you ever forget what a piece of notation means, like what is M or\\nwhat is N or\\nwhat is something else, we've also posted on the course website a notation guide\\nthat you can use to quickly look up what any particular piece of notation means.\\nSo with that, let's go on to the next video where we'll start to fetch out\\nlogistic regression using this notation.\"},\n",
       "    {'lesson_number': '02',\n",
       "     'lesson_name': 'logistic-regression',\n",
       "     'content': \"In this video, we'll go over logistic regression.\\nThis is a learning algorithm that you use when the output labels Y\\nin a supervised learning problem are all either zero or one,\\nso for binary classification problems.\\nGiven an input feature vector X maybe corresponding to\\nan image that you want to recognize as either a cat picture or not a cat picture,\\nyou want an algorithm that can output a prediction,\\nwhich we'll call Y hat,\\nwhich is your estimate of Y.\\nMore formally, you want Y hat to be the probability of the chance that,\\nY is equal to one given the input features X.\\nSo in other words, if X is a picture,\\nas we saw in the last video,\\nyou want Y hat to tell you,\\nwhat is the chance that this is a cat picture?\\nSo X, as we said in the previous video,\\nis an X dimensional vector,\\ngiven that the parameters of logistic regression will\\nbe W which is also an X dimensional vector,\\ntogether with b which is just a real number.\\nSo given an input X and the parameters W and b,\\nhow do we generate the output Y hat?\\nWell, one thing you could try, that doesn't work,\\nwould be to have Y hat be w transpose X plus B,\\nkind of a linear function of the input X.\\nAnd in fact, this is what you use if you were doing linear regression.\\nBut this isn't a very good algorithm for binary classification\\nbecause you want Y hat to be the chance that Y is equal to one.\\nSo Y hat should really be between zero and one,\\nand it's difficult to enforce that because W transpose X\\nplus B can be much bigger than one or it can even be negative,\\nwhich doesn't make sense for probability.\\nThat you want it to be between zero and one.\\nSo in logistic regression, our output is instead going to be Y hat\\nequals the sigmoid function applied to this quantity.\\nThis is what the sigmoid function looks like.\\nIf on the horizontal axis I plot Z, then the function sigmoid of Z looks like this.\\nSo it goes smoothly from zero up to one.\\nLet me label my axes here,\\nthis is zero and it crosses the vertical axis as 0.5.\\nSo this is what sigmoid of Z looks like. And we're going to use Z to denote this quantity,\\nW transpose X plus B.\\nHere's the formula for the sigmoid function.\\nSigmoid of Z, where Z is a real number,\\nis one over one plus E to the negative Z.\\nSo notice a couple of things.\\nIf Z is very large, then E to the negative Z will be close to zero.\\nSo then sigmoid of Z will be\\napproximately one over one plus something very close to zero,\\nbecause E to the negative of very large number will be close to zero.\\nSo this is close to 1.\\nAnd indeed, if you look in the plot on the left,\\nif Z is very large the sigmoid of Z is very close to one.\\nConversely, if Z is very small,\\nor it is a very large negative number,\\nthen sigmoid of Z becomes one over one plus E to the negative Z,\\nand this becomes, it's a huge number.\\nSo this becomes, think of it as one over one plus a number that is very,\\nvery big, and so,\\nthat's close to zero.\\nAnd indeed, you see that as Z becomes a very large negative number,\\nsigmoid of Z goes very close to zero.\\nSo when you implement logistic regression,\\nyour job is to try to learn parameters W and B so that\\nY hat becomes a good estimate of the chance of Y being equal to one.\\nBefore moving on, just another note on the notation.\\nWhen we programmed neural networks,\\nwe'll usually keep the parameter W and parameter B separate,\\nwhere here, B corresponds to an inter-spectrum.\\nIn some other courses,\\nyou might have seen a notation that handles this differently.\\nIn some conventions you define an extra feature called X0 and that equals a one.\\nSo that now X is in R of NX plus one.\\nAnd then you define Y hat to be equal to sigma of theta transpose X.\\nIn this alternative notational convention,\\nyou have vector parameters theta,\\ntheta zero, theta one, theta two,\\ndown to theta NX And so,\\ntheta zero, place a row a B,\\nthat's just a real number,\\nand theta one down to theta NX play the role of W. It turns out,\\nwhen you implement your neural network,\\nit will be easier to just keep B and W as separate parameters.\\nAnd so, in this class,\\nwe will not use any of this notational convention that I just wrote in red.\\nIf you've not seen this notation before in other courses, don't worry about it.\\nIt's just that for those of you that have seen this notation I wanted\\nto mention explicitly that we're not using that notation in this course.\\nBut if you've not seen this before,\\nit's not important and you don't need to worry about it.\\nSo you have now seen what the logistic regression model looks like.\\nNext to change the parameters W and B you need to define a cost function.\\nLet's do that in the next video.\"},\n",
       "    {'lesson_number': '03',\n",
       "     'lesson_name': 'logistic-regression-cost-function',\n",
       "     'content': \"In the previous video, you saw the logistic regression model\\nto train the parameters W and B, of logistic regression model.\\nYou need to define a cost function, let's take a look at the cost function.\\nYou can use to train logistic regression to recap this is what we have defined from\\nthe previous slide.\\nSo you output Y hat is sigmoid of W transports experts be where sigmoid of\\nZ is as defined here.\\nSo to learn parameters for your model, you're given a training set of\\ntraining examples and it seems natural that you want to find parameters W and B.\\nSo that at least on the training set, the outputs you have the predictions you have\\non the training set, which I will write as\\ny hat I that that will be close to the ground troops labels y I that you\\ngot in the training set.\\nSo to fill in a little bit more detail for the equation on top,\\nwe had said that y hat as defined at the top for a training example X.\\nAnd of course for each training example, we're using these superscripts with\\nround brackets with parentheses to index into different train examples.\\nYour prediction on training example I which is y hat I is\\ngoing to be obtained by taking the sigmoid function and\\napplying it to W transposed X I the input for the training example plus B.\\nAnd you can also define Z I as follows where Z I is equal to,\\nyou know, W transport Z I plus B.\\nSo throughout this course we're going to use this notational convention\\nthat the super strip parentheses I refers to data be an X or Y or Z.\\nOr something else associated with the I've training example associated\\nwith the life example, okay, that's what the superscript I in parenthesis means.\\nNow let's see what loss function or\\nan error function we can use to measure how well our album is doing.\\nOne thing you could do is define the loss when your algorithm outputs y hat and\\nthe true label is y to be maybe the square error or one half a square error.\\nIt turns out that you could do this, but\\nin logistic regression people don't usually do this.\\nBecause when you come to learn the parameters, you find that the optimization\\nproblem, which we'll talk about later becomes non convex.\\nSo you end up with optimization problem, you're with multiple local optima.\\nSo great in dissent, may not find a global optimum, if you didn't understand the last\\ncouple of comments, don't worry about it, Ww'll get to it in a later video.\\nBut the intuition to take away is that dysfunction L called the loss\\nfunction is a function will need to define to measure how good our\\noutput y hat is when the true label is y.\\nAnd squared era seems like it might be a reasonable choice except that\\nit makes great in descent not work well.\\nSo in logistic regression were actually define a different loss function\\nthat plays a similar role as squared error but\\nwill give us an optimization problem that is convex.\\nAnd so we'll see in a later video becomes much easier to optimize, so\\nwhat we use in logistic regression is actually the following loss function,\\nwhich I'm just going right out here is negative.\\ny log y hat plus 1 minus y log 1 minus,\\ny hat here's some intuition on why this loss function makes sense.\\nKeep in mind that if we're using squared error then you want to square\\nerror to be as small as possible.\\nAnd with this logistic regression,\\nlost function will also want this to be as small as possible.\\nTo understand why this makes sense, let's look at the two cases,\\nin the first case let's say y is equal to 1, then the loss function.\\ny hat comma Y is just this first term right in this negative science,\\nit's negative log y hat if y is equal to 1.\\nBecause if y equals 1, then the second term 1 minus Y is equal to 0, so\\nthis says if y equals 1, you want negative log y hat to be as small as possible.\\nSo that means you want log y hat to be large to be as big as possible,\\nand that means you want y hat to be large.\\nBut because y hat is you know the sigmoid function, it can never be bigger than one.\\nSo this is saying that if y is equal to 1, you want,\\ny hat to be as big as possible, but it can't ever be bigger than one.\\nSo saying you want, y hat to be close to one as well,\\nthe other case is Y equals zero, if Y equals 0.\\nThen this first term in the loss function is equal to 0 because y equals 0,\\nand in the second term defines the loss function.\\nSo the loss becomes negative Log 1 minus y hat, and so\\nif in your learning procedure you try to make the loss function small.\\nWhat this means is that you want, Log 1 minus y hat\\nto be large and because it's a negative sign there.\\nAnd then through a similar piece of reasoning, you can conclude that this\\nloss function is trying to make y hat as small as possible, and\\nagain, because y hat has to be between zero and 1.\\nThis is saying that if y is equal to zero then your loss function will\\npush the parameters to make y hat as close to zero as possible.\\nNow there are a lot of functions with roughly this effect that if y is equal to\\none, try to make y hat large and y is equal to zero or\\ntry to make y hat small.\\nWe just gave here in green a somewhat informal justification for\\nthis particular loss function we will provide an optional video later\\nto give a more formal justification for y.\\nIn logistic regression, we like to use the loss function with this particular form.\\nFinally, the last function was defined with respect to a single training example.\\nIt measures how well you're doing on a single training example,\\nI'm now going to define something called the cost function,\\nwhich measures how are you doing on the entire training set.\\nSo the cost function j, which is applied to your parameters W and B,\\nis going to be the average, really one of the m of the sun\\nof the loss function apply to each of the training examples.\\nIn turn, we're here, y hat is of course the prediction output by your logistic\\nregression algorithm using, you know, a particular set of parameters W and B.\\nAnd so just to expand this out, this is equal to negative one of them,\\nsome from I equals one through of the definition of the lost function above.\\nSo this is y I log y hat I plus 1 minus Y,\\nI log 1minus y hat I I guess it can put square brackets here.\\nSo the minus sign is outside everything else, so the terminology I'm going\\nto use is that the loss function is applied to just a single training example.\\nLike so and the cost function is the cost of your parameters, so in training\\nyour logistic regression model, we're going to try to find parameters W and B.\\nThat minimize the overall cost function J, written at the bottom.\\nSo you've just seen the setup for the logistic regression algorithm,\\nthe loss function for training example, and the overall cost function for\\nthe parameters of your algorithm.\\nIt turns out that logistic regression can be viewed as a very,\\nvery small neural network.\\nIn the next video, we'll go over that so\\nyou can start gaining intuition about what neural networks do.\\nSo with that let's go on to the next video about how to view logistic regression as\\na very small neural network.\"},\n",
       "    {'lesson_number': '04',\n",
       "     'lesson_name': 'gradient-descent',\n",
       "     'content': \"You've seen the logistic regression model, you've seen the loss function that\\nmeasures how well you're doing on the single training example.\\nYou've also seen the cost function that measures how well your parameters W and\\nB are doing on your entire training set.\\nNow let's talk about how you can use the gradient descent algorithm to train or\\nto learn the parameters W on your training set.\\nTo recap here is the familiar logistic regression algorithm and\\nwe have on the second line the cost function J,\\nwhich is a function of your parameters W and B.\\nAnd that's defined as the average is one over m times has some of\\nthis loss function.\\nAnd so the loss function measures how well your algorithms outputs.\\nY hat I on each of the training examples stacks up compares to the boundary\\nlables Y I on each of the training examples. The\\nfull formula is expanded out on the right.\\nSo the cost function measures how well your parameters w and\\nb are doing on the training set.\\nSo in order to learn a set of parameters w and b,\\nit seems natural that we want to find w and b.\\nThat make the cost function J of w, b as small as possible.\\nSo, here's an illustration of gradient descent.\\nIn this diagram, the horizontal axes represent your space of parameters w and\\nb in practice w can be much higher dimensional, but for the purposes of\\nplotting, let's illustrate w as a singular number and b as a singular number.\\nThe cost function J of w,\\nb is then some surface above these horizontal axes w and b.\\nSo the height of the surface represents the value of J, b at a certain point.\\nAnd what we want to do really is to find the value of w and\\nb that corresponds to the minimum of the cost function J.\\nIt turns out that this particular cost function J is a convex function.\\nSo it's just a single big bowl, so this is a convex function and\\nthis is as opposed to functions that look like this,\\nwhich are non convex and has lots of different local optimal.\\nSo the fact that our cost function J of w, b as defined here is convex,\\nis one of the huge reasons why we use this particular cost function J for\\nlogistic regression.\\nSo to find a good value for the parameters,\\nwhat we'll do is initialize w and\\nb to some initial value may be denoted by that little red dot.\\nAnd for logistic regression, almost any initialization method works.\\nUsually you Initialize the values of 0.\\nRandom initialization also works, but people don't usually do that for\\nlogistic regression.\\nBut because this function is convex, no matter where you initialize,\\nyou should get to the same point or roughly the same point.\\nAnd what gradient descent does is it starts at that initial point and\\nthen takes a step in the steepest downhill direction.\\nSo after one step of gradient descent, you might end up there because it's\\ntrying to take a step downhill in the direction of steepest descent or\\nas quickly down who as possible.\\nSo that's one iteration of gradient descent.\\nAnd after iterations of gradient descent,\\nyou might stop there, three iterations and so on.\\nI guess this is not hidden by the back of the plot until eventually, hopefully you\\nconverge to this global optimum or get to something close to the global optimum.\\nSo this picture illustrates the gradient descent algorithm.\\nLet's write a little bit more of the details for the purpose of illustration,\\nlet's say that there's some function J of w that you want to minimize and\\nmaybe that function looks like this to make this easier to draw.\\nI'm going to ignore b for\\nnow just to make this one dimensional plot instead of a higher dimensional plot.\\nSo gradient descent does this.\\nWe're going to repeatedly carry out the following update.\\nWe'll take the value of w and update it.\\nGoing to use colon equals to represent updating w.\\nSo set w to w minus alpha times and\\nthis is a derivative d of J w d w.\\nAnd we repeatedly do that until the algorithm converges.\\nSo a couple of points in the notation alpha here is the learning rate and\\ncontrols how big a step we take on each iteration are gradient descent,\\nwe'll talk later about some ways for choosing the learning rate,\\nalpha and second this quantity here, this is a derivative.\\nThis is basically the update of the change you want to make to the parameters w,\\nwhen we start to write code to implement gradient descent,\\nwe're going to use the convention that the variable name in our code,\\nd w will be used to represent this derivative term.\\nSo when you write code, you write something like w equals or\\ncold equals w minus alpha time's d w.\\nSo we use d w to be the variable name to represent this derivative term.\\nNow, let's just make sure that this gradient descent update makes sense.\\nLet's say that w was over here.\\nSo you're at this point on the cost function J of w.\\nRemember that the definition of a derivative is the slope\\nof a function at the point.\\nSo the slope of the function is really,\\nthe height divided by the width right of the lower triangle.\\nHere, in this tension to J of w at that point.\\nAnd so here the derivative is positive.\\nW gets updated as w minus a learning rate times the derivative,\\nthe derivative is positive.\\nAnd so you end up subtracting from w.\\nSo you end up taking a step to the left and so gradient descent with,\\nmake your algorithm slowly decrease the parameter.\\nIf you had started off with this large value of w.\\nAs another example, if w was over here,\\nthen at this point the slope here or dJ detail, you will be negative.\\nAnd so they driven to send update with subtract alpha times a negative number.\\nAnd so end up slowly increasing w.\\nSo you end up you're making w bigger and\\nbigger with successive generations of gradient descent.\\nSo that hopefully whether you initialize on the left, wonder right,\\ncreate into central move you towards this global minimum here.\\nIf you're not familiar with derivatives of calculus and\\nwhat this term d J of w d w means.\\nDon't worry too much about it.\\nWe'll talk some more about derivatives in the next video.\\nIf you have a deep knowledge of calculus,\\nyou might be able to have a deeper intuitions about how neural networks work.\\nBut even if you're not that familiar with calculus in the next few videos\\nwill give you enough intuitions about derivatives and\\nabout calculus that you'll be able to effectively use neural networks.\\nBut the overall intuition for\\nnow is that this term represents the slope of the function and we want to\\nknow the slope of the function at the current setting of the parameters so\\nthat we can take these steps of steepest descent so that we know what\\ndirection to step in in order to go downhill on the cost function J.\\nSo we wrote our gradient descent for J of w.\\nIf only w was your parameter in logistic regression.\\nYour cost function is a function above w and b.\\nIn that case the inner loop of gradient descent,\\nthat is this thing here the thing you have to repeat becomes as follows.\\nYou end up updating w as w minus the learning rate\\ntimes the derivative of J of wb respect to w and\\nyou update b as b minus the learning rate times\\nthe derivative of the cost function respect to b.\\nSo these two equations at the bottom of the actual update you implement as in\\nthe side, I just want to mention one notation, all convention and calculus.\\nThat is a bit confusing to some people.\\nI don't think it's super important that you understand calculus but\\nin case you see this, I want to make sure that you don't think too much of this.\\nWhich is that in calculus this term here we actually write as follows,\\nthat funny squiggle symbol.\\nSo this symbol, this is actually just the lower\\ncase d in a fancy font, in a stylized font.\\nBut when you see this expression, all this means is this is the of J of w,\\nb or really the slope of the function J of w,\\nb how much that function slopes in the w direction.\\nAnd the rule of the notation and calculus, which I think is in total logical.\\nBut the rule in the notation for calculus, which I think just makes things\\nmuch more complicated than you need to be is that if J is a function of two or\\nmore variables, then instead of using lower case d.\\nYou use this funny symbol.\\nThis is called a partial derivative symbol, but don't worry about this.\\nAnd if J is a function of only one variable, then you use lower case d.\\nSo the only difference between whether you use this funny partial derivative symbol\\nor lower case d.\\nAs we did on top is whether J is a function of two or more variables.\\nIn which case use this symbol, the partial derivative symbol or\\nJ is only a function of one variable.\\nThen you use lower case d.\\nThis is one of those funny rules of notation and\\ncalculus that I think just make things more complicated than they need to be.\\nBut if you see this partial derivative symbol,\\nall it means is you're measuring the slope of the function with\\nrespect to one of the variables, and similarly to adhere to the,\\nformally correct mathematical notation calculus because here J has two inputs.\\nNot just one.\\nThis thing on the bottom should be written with this partial derivative simple,\\nbut it really means the same thing as, almost the same thing as lowercase d.\\nFinally, when you implement this in code,\\nwe're going to use the convention that this quantity really the amount\\nI wish you update w will denote as the variable d w in your code.\\nAnd this quantity, right, the amount by which you want\\nto update b with the note by the variable db in your code.\\nAll right. So\\nthat's how you can implement gradient descent.\\nNow if you haven't seen calculus for a few years, I know that that might seem like\\na lot more derivatives and calculus than you might be comfortable with so far.\\nBut if you're feeling that way, don't worry about it.\\nIn the next video will give you better intuition about derivatives.\\nAnd even without the deep mathematical understanding of calculus,\\nwith just an intuitive understanding of calculus,\\nyou will be able to make your networks work effectively so\\nthat let's go into the next video, we'll talk a little bit more about derivatives.\"},\n",
       "    {'lesson_number': '05',\n",
       "     'lesson_name': 'derivatives',\n",
       "     'content': \"In this video, I want to help you gain an intuitive understanding,\\nof calculus and the derivatives.\\nNow, maybe you're thinking that you haven't seen calculus since your college days,\\nand depending on when you graduated,\\nmaybe that was quite some time back.\\nNow, if that's what you're thinking, don't worry,\\nyou don't need a deep understanding of calculus in order\\nto apply neural networks and deep learning very effectively.\\nSo, if you're watching this video or some of the later videos and you're wondering,\\nwell, is this stuff really for me,\\nthis calculus looks really complicated.\\nMy advice to you is the following, which is that,\\nwatch the videos and then if you could do\\nthe homeworks and complete the programming homeworks successfully,\\nthen you can apply deep learning.\\nIn fact, when you see later is that in week four,\\nwe'll define a couple of types of functions that will enable you to\\nencapsulate everything that needs to be done with respect to calculus,\\nthat these functions called forward functions and backward functions that you learn about.\\nThat lets you put everything you need to know about calculus into these functions,\\nso that you don't need to worry about them anymore beyond that.\\nBut I thought that in this foray into deep learning that this week,\\nwe should open up the box and peer a little bit further into the details of calculus.\\nBut really, all you need is an intuitive understanding of this\\nin order to build and successfully apply these algorithms.\\nFinally, if you are among that maybe smaller group of people that are expert in calculus,\\nif you are very familiar with calculus derivatives,\\nit's probably okay for you to skip this video.\\nBut for everyone else, let's dive in,\\nand try to gain an intuitive understanding of derivatives.\\nI plotted here the function f(a) equals 3a.\\nSo, it's just a straight line.\\nTo get intuition about derivatives,\\nlet's look at a few points on this function.\\nLet say that a is equal to two.\\nIn that case, f of a,\\nwhich is equal to three times a is equal to six.\\nSo, if a is equal to two,\\nthen f of a will be equal to six.\\nLet's say we give the value of a just a little bit of a nudge.\\nI'm going to just bump up a,\\na little bit, so that it is now 2.001.\\nSo, I'm going to give a like a tiny little nudge, to the right.\\nSo now, let's say 2.001,\\njust plot this into scale,\\n2.001, this 0.001 difference is too small to show on this plot,\\njust give a little nudge to that right.\\nNow, f(a),\\nis equal to three times that.\\nSo, it's 6.003, so we plot this over here.\\nThis is not to scale, this is 6.003.\\nSo, if you look at this little triangle here that I'm highlighting in green,\\nwhat we see is that if I nudge a 0.001 to the right,\\nthen f of a goes up by 0.003.\\nThe amounts that f of a,\\nwent up is three times as big as the amount that I nudge the a to the right.\\nSo, we're going to say that,\\nthe slope or the derivative of the function f of a,\\nat a equals to or when a is equals two to the slope is three.\\nThe term derivative basically means slope,\\nit's just that derivative sounds like a scary and more\\nintimidating word, whereas a slope is a friendlier way to describe the concept of derivative.\\nSo, whenever you hear derivative,\\njust think slope of the function.\\nMore formally, the slope is defined as the height\\ndivided by the width of this little triangle that we have in green.\\nSo, this is 0.003 over 0.001,\\nand the fact that the slope is equal to three or the derivative is equal to three,\\njust represents the fact that when you nudge a to the right by 0.001, by tiny amount,\\nthe amount at f of a goes up is three times as big as the amount that you nudged it,\\nthat you nudged a in the horizontal direction.\\nSo, that's all that the slope of a line is.\\nNow, let's look at this function at a different point.\\nLet's say that a is now equal to five.\\nIn that case, f of a,\\nthree times a is equal to 15.\\nSo, let's see that again,\\ngive a, a nudge to the right.\\nA tiny little nudge, it's now bumped up to 5.001,\\nf of a is three times that.\\nSo, f of a is equal to 15.003.\\nSo, once again, when I bump a to the right,\\nnudg a to the right by 0.001,\\nf of a goes up three times as much.\\nSo the slope, again, at a = 5, is also three.\\nSo, the way we write this,\\nthat the slope of the function f is equal to three:\\nWe say, d f(a)\\nda and this just means,\\nthe slope of the function f(a)\\nwhen you nudge the variable a,\\na tiny little amount, this is equal to three.\\nAn alternative way to write this derivative formula is as follows.\\nYou can also write this as,\\nd da of f(a).\\nSo, whether you put f(a) on top or whether you write it down here, it doesn't matter.\\nBut all this equation means is that,\\nif I nudge a to the right a little bit,\\nI expect f(a) to go up by three times as much as I nudged the value of little a.\\nNow, for this video I explained derivatives,\\ntalking about what happens if we nudged the variable a by 0.001.\\nIf you want a formal mathematical definition of the derivatives:\\nDerivatives are defined with an even smaller value of how much you nudge a to the right.\\nSo, it's not 0.001.\\nIt's not 0.000001.\\nIt's not 0.00000000 and so on 1.\\nIt's even smaller than that,\\nand the formal definition of derivative says,\\nwhenever you nudge a to the right by an infinitesimal amount,\\nbasically an infinitely tiny, tiny amount.\\nIf you do that, this f(a) go up three times as much as whatever was the tiny,\\ntiny, tiny amount that you nudged a to the right.\\nSo, that's actually the formal definition of a derivative.\\nBut for the purposes of our intuitive understanding,\\nwhich I'll talk about nudging a to the right by this small amount 0.001.\\nEven if it's 0.001 isn't exactly tiny, tiny infinitesimal.\\nNow, one property of the derivative is that,\\nno matter where you take the slope of this function,\\nit is equal to three,\\nwhether a is equal to two or a is equal to five.\\nThe slope of this function is equal to three,\\nmeaning that whatever is the value of a,\\nif you increase it by 0.001,\\nthe value of f of a goes up by three times as much.\\nSo, this function has a safe slope everywhere.\\nOne way to see that is that,\\nwherever you draw this little triangle.\\nThe height, divided by the width,\\nalways has a ratio of three to one.\\nSo, I hope this gives you a sense of what the slope or\\nthe derivative of a function means for a straight line,\\nwhere in this example the slope of the function was three everywhere.\\nIn the next video, let's take a look at a slightly more complex example,\\nwhere the slope to the function can be different at different points on the function.\"},\n",
       "    {'lesson_number': '06',\n",
       "     'lesson_name': 'more-derivative-examples',\n",
       "     'content': \"In this video, I'll show you a slightly more complex example\\nwhere the slope of the function can be different to different points in the function.\\nLet's start with an example.\\nYou have plotted the function f(a) = aÂ².\\nLet's take a look at the point a=2.\\nSo aÂ² or f(a) = 4.\\nLet's nudge a slightly to the right, so now a=2.001.\\nf(a) which is aÂ² is going to be approximately 4.004.\\nIt turns out that the exact value,\\nyou call the calculator and figured this out is actually 4.004001.\\nI'm just going to say 4.004 is close enough.\\nSo what this means is that when a=2,\\nlet's draw this on the plot.\\nSo what we're saying is that if a=2,\\nthen f(a) = 4 and here is the x and y axis are not drawn to scale.\\nTechnically, does vertical height should be much higher than\\nthis horizontal height so the x and y axis are not on the same scale.\\nBut if I now nudge a to 2.001 then f(a) becomes roughly 4.004.\\nSo if we draw this little triangle again,\\nwhat this means is that if I nudge a to the right by 0.001,\\nf(a) goes up four times as much by 0.004.\\nSo in the language of calculus,\\nwe say that a slope that is the derivative of f(a) at\\na=2 is 4 or to write this out of our calculus notation,\\nwe say that d/da of f(a) = 4 when a=2.\\nNow one thing about this function f(a) = aÂ²\\nis that the slope is different for different values of a.\\nThis is different than the example we saw on the previous slide.\\nSo let's look at a different point.\\nIf a=5, so instead of a=2,\\nand now a=5 then aÂ²=25, so that's f(a).\\nIf I nudge a to the right again,\\nit's tiny little nudge to a,\\nso now a=5.001 then f(a) will be approximately 25.010.\\nSo what we see is that by nudging a up by .001,\\nf(a) goes up ten times as much.\\nSo we have that d/da f(a) = 10 when\\na=5 because f(a) goes up ten times as\\nmuch as a does when I make a tiny little nudge to a.\\nSo one way to see why did derivatives is different at different points is that if you\\ndraw that little triangle right at different locations on this,\\nyou'll see that the ratio of the height of the triangle over\\nthe width of the triangle is very different at different points on the curve.\\nSo here, the slope=4 when a=2, a=10, when a=5.\\nNow if you pull up a calculus textbook,\\na calculus textbook will tell you that d/da of f(a),\\nso f(a) = aÂ²,\\nso that's d/da of aÂ².\\nOne of the formulas you find are the calculus textbook is that this thing,\\nthe slope of the function aÂ², is equal to 2a.\\nNot going to prove this, but the way you find this out is that\\nyou open up a calculus textbook to\\nthe table formulas and they'll tell you that derivative of 2 of aÂ² is 2a.\\nAnd indeed, this is consistent with what we've worked out.\\nNamely, when a=2, the slope of function to a is 2x2=4.\\nAnd when a=5 then the slope of the function 2xa is 2x5=10.\\nSo, if you ever pull up a calculus textbook and you see this formula,\\nthat the derivative of aÂ²=2a,\\nall that means is that for any given value of a,\\nif you nudge upward by 0.001 already your tiny little value,\\nyou will expect f(a) to go up by 2a.\\nThat is the slope or the derivative times\\nother much you had nudged to the right the value of a.\\nNow one tiny little detail,\\nI use these approximate symbols here and this wasn't exactly 4.004,\\nthere's an extra .001 hanging out there.\\nIt turns out that this extra .001,\\nthis little thing here is because we were nudging a to the right by 0.001,\\nif we're instead nudging it to the right by\\nthis infinitesimally small value then this extra every term will go\\naway and you find that the amount that f(a) goes out is exactly equal\\nto the derivative times the amount that you nudge a to the right.\\nAnd the reason why is not 4.004 exactly is because derivatives are defined using\\nthis infinitesimally small nudges to a rather than 0.001 which is not.\\nAnd while 0.001 is small,\\nit's not infinitesimally small.\\nSo that's why the amount that f(a) went up isn't exactly given\\nby the formula but it's only a kind of approximately given by the derivative.\\nTo wrap up this video,\\nlet's just go through a few more quick examples.\\nThe example you've already seen is that if f(a) = aÂ² then\\nthe calculus textbooks formula table will tell you that the derivative is equal to 2a.\\nAnd so the example we went through was it if (a) = 2,\\nf(a) = 4, and we nudge a,\\nsince it's a little bit bigger than f(a) is about\\n4.004 and so f(a) went up four times as much and indeed when a=2,\\nthe derivatives is equal to 4.\\nLet's look at some other examples.\\nLet's say, instead the f(a) = aÂ³.\\nIf you go to a calculus textbook and look up the table of formulas,\\nyou see that the slope of this function, again,\\nthe derivative of this function is equal to 3aÂ².\\nSo you can get this formula out of the calculus textbook.\\nSo what this means?\\nSo the way to interpret this is as follows.\\nLet's take a=2 as an example again.\\nSo f(a) or aÂ³=8, that's two to the power of three.\\nSo we give a a tiny little nudge,\\nyou find that f(a) is about 8.012 and feel free to check this.\\nTake 2.001 to the power of three,\\nyou find this is very close to 8.012.\\nAnd indeed, when a=2 that's 3x2Â² does equal to 3x4,\\nyou see that's 12.\\nSo the derivative formula predicts that if you nudge a to the right by tiny little bit,\\nf(a) should go up 12 times as much.\\nAnd indeed, this is true when a went up by .001,\\nf(a) went up 12 times as much by .012.\\nJust one last example and then we'll wrap up.\\nLet's say that f(a) is equal to the log function.\\nSo on the right log of a,\\nI'm going to use this as the base e logarithm.\\nSo some people write that as log(a).\\nSo if you go to calculus textbook,\\nyou find that when you take the derivative of log(a).\\nSo this is a function that just looks like that,\\nthe slope of this function is given by 1/a.\\nSo the way to interpret this is that if a has any value then let's just keep\\nusing a=2 as an example and you nudge a to the right by .001,\\nyou would expect f(a) to go up by\\n1/a that is by the derivative times the amount that you increase a.\\nSo in fact, if you pull up a calculator,\\nyou find that if a=2,\\nf(a) is about 0.69315 and if you\\nincrease f and if you increase a to 2.001 then f(a) is about 0.69365,\\nthis has gone up by 0.0005.\\nAnd indeed, if you look at the formula for the derivative when a=2,\\nd/da f(a) = 1/2.\\nSo this derivative formula predicts that if you pump up a by .001,\\nyou would expect f(a) to go up by only 1/2 as much and 1/2 of .001\\nis 0.0005 which is exactly what we got.\\nThen when a goes up by .001, going from a=2 to\\na=2.001, f(a) goes up by half as much.\\nSo, the answers are going up by approximately .0005.\\nSo if we draw that little triangle if you will is that if on\\nthe horizontal axis just goes up by .001 on the vertical axis,\\nlog(a) goes up by half of that so .0005.\\nAnd so that 1/a or 1/2 in this case,\\n1a=2 that's just the slope of this line when a=2.\\nSo that's it for derivatives.\\nThere are just two take home messages from this video.\\nFirst is that the derivative of the function just means the slope of\\na function and the slope of a function\\ncan be different at different points on the function.\\nIn our first example where f(a) = 3a those a straight line.\\nThe derivative was the same everywhere,\\nit was three everywhere.\\nFor other functions like f(a) = aÂ² or f(a) = log(a),\\nthe slope of the line varies.\\nSo, the slope or the derivative can be different at different points on the curve.\\nSo that's a first take away.\\nDerivative just means slope of a line.\\nSecond takeaway is that if you want to look up the derivative of a function,\\nyou can flip open your calculus textbook or look up Wikipedia and\\noften get a formula for the slope of these functions at different points.\\nSo that, I hope you have an intuitive understanding of derivatives or slopes of lines.\\nLet's go into the next video.\\nWe'll start to talk about the computation graph and how to\\nuse that to compute derivatives of more complex functions.\"},\n",
       "    {'lesson_number': '07',\n",
       "     'lesson_name': 'computation-graph',\n",
       "     'content': \"You've heard me say\\nthat the computations\\nof a neural network are organized\\nin terms of a forward pass\\nor a forward propagation step,\\nin which we compute the output\\nof the neural network,\\nfollowed by a backward pass\\nor back propagation step,\\nwhich we use to compute gradients\\nor compute derivatives.\\nThe computation graph explains\\nwhy it is organized this way.\\nIn this video, we'll go through\\nan example.\\nIn order to illustrate\\nthe computation graph,\\nlet's use a simpler example\\nthan logistic regression\\nor a full blown neural network.\\nLet's say that we're trying to compute a function, J,\\nwhich is a function\\nof three variables a, b, and c\\nand let's say that function is 3(a+bc).\\nComputing this function actually\\nhas three distinct steps.\\nThe first is you need to compute\\nwhat is bc\\nand let's say we store that\\nin the variable call u.\\nSo u=bc and then you my compute V=a *u.\\nSo let's say this is V.\\nAnd then finally, your output J is 3V.\\nSo this is your final function J\\nthat you're trying to compute.\\nWe can take these three steps\\nand draw them in a computation graph as follows.\\nLet's say, I draw your three variables\\na, b, and c here.\\nSo the first thing we did\\nwas compute u=bc.\\nSo I'm going to put\\na rectangular box around that.\\nAnd so the input to that are b and c.\\nAnd then, you might have V=a+u.\\nSo the inputs to that\\nare V. So the inputs to that are u\\nwith just computed together with a.\\nAnd then finally, we have J=3V.\\nSo as a concrete example, if a=5,\\nb=3 and c=2 then u=bc would be six\\nbecause a+u would be 5+6 is 11,.\\nJ is three times that, so J=33.\\nAnd indeed, hopefully you can verify\\nthat this is three times five\\nplus three times two.\\nAnd if you expand that out,\\nyou actually get 33 as the value of J.\\nSo, the computation graph comes in handy\\nwhen there is some distinguished\\nor some special output variable,\\nsuch as J in this case,\\nthat you want to optimize.\\nAnd in the case\\nof a logistic regression,\\nJ is of course the cost function\\nthat we're trying to minimize.\\nAnd what we're seeing\\nin this little example is that,\\nthrough a left-to-right pass,\\nyou can compute the value of J\\nand what we'll see\\nin the next couple of slides\\nis that in order to compute derivatives\\nthere'll be a right-to-left\\npass like this,\\nkind of going in the opposite direction\\nas the blue arrows.\\nThat would be most natural\\nfor computing the derivatives.\\nSo to recap, the computation graph\\norganizes a computation with this blue arrow,\\nleft-to-right computation.\\nLet's refer to the next video\\nhow you can do the backward red arrow\\nright-to-left computation\\nof the derivatives.\\nLet's go on to the next video.\"},\n",
       "    {'lesson_number': '08',\n",
       "     'lesson_name': 'derivatives-with-a-computation-graph',\n",
       "     'content': \"In the last video,\\nwe worked through an example of using a computation graph to compute a function J.\\nNow, let's take a cleaned up version of that computation graph\\nand show how you can use it to figure out derivative calculations for\\nthat function J.\\nSo here's a computation graph.\\nLet's say you want to compute the derivative of J with respect to v.\\nSo what is that?\\nWell, this says, if we were to take this value of v and\\nchange it a little bit, how would the value of J change?\\nWell, J is defined as 3 times v.\\nAnd right now, v = 11.\\nSo if we're to bump up v by a little bit to 11.001,\\nthen J, which is 3v, so currently 33,\\nwill get bumped up to 33.003.\\nSo here, we've increased v by 0.001.\\nAnd the net result of that is that J goes up 3 times as much.\\nSo the derivative of J with respect to v is equal to 3.\\nBecause the increase in J is 3 times the increase in v.\\nAnd in fact, this is very analogous to the example\\nwe had in the previous video, where we had f(a) = 3a.\\nAnd so we then derived that df/da, which with slightly simplified,\\na slightly sloppy notation, you can write as df/da = 3.\\nSo instead, here we have J = 3v,\\nand so dJ/dv = 3.\\nWith here, J playing the role of f, and\\nv playing the role of a in this previous example that we had from an earlier video.\\nSo indeed, terminology of backpropagation, what we're seeing\\nis that if you want to compute the derivative of this final output variable,\\nwhich usually is a variable you care most about,\\nwith respect to v, then we've done one step of backpropagation.\\nSo we call it one step backwards in this graph.\\nNow let's look at another example.\\nWhat is dJ/da?\\nIn other words, if we bump up the value of a, how does that affect the value of J?\\nWell, let's go through the example, where now a = 5.\\nSo let's bump it up to 5.001.\\nThe net impact of that is that v, which was a + u, so that was previously 11.\\nThis would get increased to 11.001.\\nAnd then we've already seen as above that\\nJ now gets bumped up to 33.003.\\nSo what we're seeing is that if you increase a by 0.001, J increases by 0.003.\\nAnd by increase a, I mean, you have to take this value of 5 and\\njust plug in a new value.\\nThen the change to a will propagate to the right of the computation graph so\\nthat J ends up being 33.003.\\nAnd so the increase to J is 3 times the increase to a.\\nSo that means this derivative is equal to 3.\\nAnd one way to break this down is to say that if you change a,\\nthen that will change v.\\nAnd through changing v, that would change J.\\nAnd so the net change to the value of J when you bump up the value,\\nwhen you nudge the value of a up a little bit, is that,\\nFirst, by changing a, you end up increasing v.\\nWell, how much does v increase?\\nIt is increased by an amount that's determined by dv/da.\\nAnd then the change in v will cause the value of J to also increase.\\nSo in calculus, this is actually called the chain rule that if a affects v,\\naffects J, then the amounts that J changes when you\\nnudge a is the product of how much v changes when you\\nnudge a times how much J changes when you nudge v.\\nSo in calculus, again, this is called the chain rule.\\nAnd what we saw from this calculation is that if you increase a by 0.001,\\nv changes by the same amount.\\nSo dv/da = 1.\\nSo in fact, if you plug in what we have wrapped up previously,\\ndv/dJ = 3 and dv/da = 1.\\nSo the product of these 3 times 1,\\nthat actually gives you the correct value that dJ/da = 3.\\nSo this little illustration shows hows by having computed dJ/dv,\\nthat is, derivative with respect to this variable,\\nit can then help you to compute dJ/da.\\nAnd so that's another step of this backward calculation.\\nI just want to introduce one more new notational convention.\\nWhich is that when you're witting codes to implement backpropagation,\\nthere will usually be some final output variable that you really care about.\\nSo a final output variable that you really care about or that you want to optimize.\\nAnd in this case, this final output variable is J.\\nIt's really the last node in your computation graph.\\nAnd so a lot of computations will be trying to compute the derivative of that\\nfinal output variable.\\nSo d of this final output variable with respect to some other variable.\\nThen we just call that dvar.\\nSo a lot of the computations you have will be to compute the derivative of the final\\noutput variable, J in this case, with various intermediate variables,\\nsuch as a, b, c, u or v.\\nAnd when you implement this in software, what do you call this variable name?\\nOne thing you could do is in Python,\\nyou could give us a very long variable name like dFinalOurputVar/dvar.\\nBut that's a very long variable name.\\nYou could call this, I guess, dJdvar.\\nBut because you're always taking derivatives with respect to dJ, with\\nrespect to this final output variable, I'm going to introduce a new notation.\\nWhere, in code, when you're computing this thing in the code you write,\\nwe're just going to use the variable name dvar in order to represent that quantity.\\nSo dvar in a code you write will represent the derivative of\\nthe final output variable you care about such as J.\\nWell, sometimes, the last l with respect to the various intermediate quantities\\nyou're computing in your code.\\nSo this thing here in your code, you use dv to denote this value.\\nSo dv would be equal to 3.\\nAnd your code, you represent this as da,\\nwhich is we also figured out to be equal to 3.\\nSo we've done backpropagation partially through this computation graph.\\nLet's go through the rest of this example on the next slide.\\nSo let's go to a cleaned up copy of the computation graph.\\nAnd just to recap, what we've done so\\nfar is go backward here and figured out that dv = 3.\\nAnd again, the definition of dv, that's just a variable name,\\nwhere the code is really dJ/dv.\\nWe've figured out that da = 3.\\nAnd again, da is the variable name in your code and that's really the value dJ/da.\\nAnd we hand wave how we've gone backwards on these two edges like so.\\nNow let's keep computing derivatives.\\nNow let's look at the value u.\\nSo what is dJ/du?\\nWell, through a similar calculation as what we did before and\\nthen we start off with u = 6.\\nIf you bump up u to 6.001, then v,\\nwhich is previously 11, goes up to 11.001.\\nAnd so J goes from 33 to 33.003.\\nAnd so the increase in J is 3x, so this is equal.\\nAnd the analysis for u is very similar to the analysis we did for a.\\nThis is actually computed as dJ/dv times dv/du,\\nwhere this we had already figured out was 3.\\nAnd this turns out to be equal to 1.\\nSo we've gone up one more step of backpropagation.\\nWe end up computing that du is also equal to 3.\\nAnd du is, of course, just this dJ/du.\\nNow we just step through one last example in detail.\\nSo what is dJ/db?\\nSo here, imagine if you are allowed to change the value of b.\\nAnd you want to tweak b a little bit in order to minimize or\\nmaximize the value of J.\\nSo what is the derivative or\\nwhat's the slope of this function J when you change the value of b a little bit?\\nIt turns out that using the chain rule for calculus,\\nthis can be written as the product of two things.\\nThis dJ/du times du/db.\\nAnd the reasoning is if you change b a little bit,\\nso b = 3 to, say, 3.001.\\nThe way that it will affect J is it will first affect u.\\nSo how much does it affect u?\\nWell, u is defined as b times c.\\nSo this will go from 6,\\nwhen b = 3, to now 6.002\\nbecause c = 2 in our example here.\\nAnd so this tells us that du/db = 2.\\nBecause when you bump up b by 0.001, u increases twice as much.\\nSo du/db, this is equal to 2.\\nAnd now, we know that u has gone up twice as much as b has gone up.\\nWell, what is dJ/du?\\nWe've already figured out that this is equal to 3.\\nAnd so by multiplying these two out, we find that dJ/db = 6.\\nAnd again, here's the reasoning for the second part of the argument.\\nWhich is we want to know when u goes up by 0.002, how does that affect J?\\nThe fact that dJ/du = 3, that tells us that when\\nu goes up by 0.002, J goes up 3 times as much.\\nSo J should go up by 0.006.\\nSo this comes from the fact that dJ/du = 3.\\nAnd if you check the math in detail,\\nyou will find that if b becomes 3.001,\\nthen u becomes 6.002, v becomes 11.002.\\nSo that's a + u, so that's 5 + u.\\nAnd then J, which is equal to 3 times v,\\nthat ends up being equal to 33.006.\\nAnd so that's how you get that dJ/db = 6.\\nAnd to fill that in, this is if we go backwards, so this is db = 6.\\nAnd db really is the Python code variable name for dJ/db.\\nAnd I won't go through the last example in great detail.\\nBut it turns out that if you also compute out dJ,\\nthis turns out to be dJ/du times du.\\nAnd this turns out to be 9, this turns out to be 3 times 3.\\nI won't go through that example in detail.\\nSo through this last step, it is possible to derive that dc is equal to.\\nSo the key takeaway from this video, from this example, is that when computing\\nderivatives and computing all of these derivatives, the most efficient way to do\\nso is through a right to left computation following the direction of the red arrows.\\nAnd in particular, we'll first compute the derivative with respect to v.\\nAnd then that becomes useful for\\ncomputing the derivative with respect to a and the derivative with respect to u.\\nAnd then the derivative with respect to u, for\\nexample, this term over here and this term over here.\\nThose in turn become useful for computing the derivative with respect to b and\\nthe derivative with respect to c.\\nSo that was the computation graph and how does a forward or left to right\\ncalculation to compute the cost function such as J that you might want to optimize.\\nAnd a backwards or a right to left calculation to compute derivatives.\\nIf you're not familiar with calculus or the chain rule,\\nI know some of those details, but they've gone by really quickly.\\nBut if you didn't follow all the details, don't worry about it.\\nIn the next video,\\nwe'll go over this again in the context of logistic regression.\\nAnd show you exactly what you need to do in order to implement the computations you\\nneed to compute the derivatives of the logistic regression model.\"},\n",
       "    {'lesson_number': '09',\n",
       "     'lesson_name': 'logistic-regression-gradient-descent',\n",
       "     'content': \"Welcome back. In this video,\\nwe'll talk about how to compute derivatives for you\\nto implement gradient descent for logistic regression.\\nThe key takeaways will be what you need to implement.\\nThat is, the key equations you need in order to\\nimplement gradient descent for logistic regression.\\nIn this video, I want to do this computation using the computation graph.\\nI have to admit, using the computation graph is a little bit of\\nan overkill for deriving gradient descent for logistic regression,\\nbut I want to start explaining things this\\nway to get you familiar with these ideas so that,\\nhopefully, it will make a bit more sense when we talk about full-fledged neural networks.\\nTo that, let's dive into gradient descent for logistic regression.\\nTo recap, we had set up logistic regression as follows,\\nyour predictions, Y_hat, is defined as follows,\\nwhere z is that.\\nIf we focus on just one example for now, then the loss,\\nor respect to that one example,\\nis defined as follows,\\nwhere A is the output of logistic regression,\\nand Y is the ground truth label.\\nLet's write this out as a computation graph and for this example,\\nlet's say we have only two features, X1 and X2.\\nIn order to compute Z,\\nwe'll need to input W1,\\nW2, and B, in addition to the feature values X1, X2.\\nThese things, in a computational graph,\\nget used to compute Z, which is W1,\\nX1 + W2 X2 + B,\\nrectangular box around that.\\nThen, we compute Y_hat,\\nor A = Sigma_of_Z,\\nthat's the next step in the computation graph, and then, finally,\\nwe compute L, AY,\\nand I won't copy the formula again.\\nIn logistic regression, what we want to do is to modify the parameters,\\nW and B, in order to reduce this loss.\\nWe've described the forward propagation steps of how you actually\\ncompute the loss on a single training example,\\nnow let's talk about how you can go backwards to compute the derivatives.\\nHere's a cleaned-up version of the diagram.\\nBecause what we want to do is compute derivatives with respect to this loss,\\nthe first thing we want to do when going backwards is to\\ncompute the derivative of this loss with respect to,\\nthe script over there, with respect to this variable A.\\nSo, in the code,\\nyou just use DA to denote this variable.\\nIt turns out that if you are familiar with calculus,\\nyou could show that this ends up being -Y_over_A+1-Y_over_1-A.\\nAnd the way you do that is you take the formula for the loss and,\\nif you're familiar with calculus,\\nyou can compute the derivative with respect to the variable,\\nlowercase A, and you get this formula.\\nBut if you're not familiar with calculus, don't worry about it.\\nWe'll provide the derivative formulas,\\nwhat else you need, throughout this course.\\nIf you are an expert in calculus,\\nI encourage you to look up the formula for the loss from\\ntheir previous slide and try taking derivative with respect to A using calculus,\\nbut if you don't know enough calculus to do that, don't worry about it.\\nNow, having computed this quantity of DA and\\nthe derivative or your final alpha variable with respect to A,\\nyou can then go backwards.\\nIt turns out that you can show DZ which,\\nthis is the part called variable name,\\nthis is going to be the derivative of the loss,\\nwith respect to Z, or for L,\\nyou could really write the loss including A and Y explicitly as parameters or not, right?\\nEither type of notation is equally acceptable.\\nWe can show that this is equal to A-Y.\\nJust a couple of comments only for those of you experts in calculus,\\nif you're not expert in calculus, don't worry about it.\\nBut it turns out that this, DL DZ,\\nthis can be expressed as DL_DA_times_DA_DZ,\\nand it turns out that DA DZ,\\nthis turns out to be A_times_1-A,\\nand DL DA we have previously worked out over here,\\nif you take these two quantities, DL DA,\\nwhich is this term, together with DA DZ,\\nwhich is this term, and just take these two things and multiply them.\\nYou can show that the equation simplifies to A-Y.\\nThat's how you derive it,\\nand that this is really the chain rule that have briefly eluded to the form.\\nFeel free to go through that calculation yourself if you are knowledgeable in calculus,\\nbut if you aren't, all you need to know is that you can compute\\nDZ as A-Y and we've already done that calculus for you.\\nThen, the final step in that computation is to go back to\\ncompute how much you need to change W and B.\\nIn particular, you can show that the derivative with respect to W1 and in quotes,\\ncall this DW1, that this is equal to X1_times_DZ.\\nThen, similarly, DW2, which is how much you want to change W2,\\nis X2_times_DZ and B,\\nexcuse me, DB is equal to DZ.\\nIf you want to do gradient descent with respect to just this one example,\\nwhat you would do is the following;\\nyou would use this formula to compute DZ,\\nand then use these formulas to compute DW1, DW2,\\nand DB, and then you perform these updates.\\nW1 gets updated as W1 minus,\\nlearning rate alpha, times DW1.\\nW2 gets updated similarly,\\nand B gets set as B minus the learning rate times DB.\\nAnd so, this will be one step of grade with respect to a single example.\\nYou see in how to compute derivatives and implement\\ngradient descent for logistic regression with respect to a single training example.\\nBut training logistic regression model,\\nyou have not just one training example given training sets of M training examples.\\nIn the next video,\\nlet's see how you can take these ideas and apply them to learning,\\nnot just from one example,\\nbut from an entire training set.\"},\n",
       "    {'lesson_number': '10',\n",
       "     'lesson_name': 'gradient-descent-on-m-examples',\n",
       "     'content': \"In a previous video, you saw how to compute derivatives and implement\\ngradient descent with respect to just one training example for logistic regression.\\nNow, we want to do it for m training examples.\\nTo get started, let's remind ourselves of the definition of the cost function\\nJ. Cost- function w,b,which you care about is this average,\\none over m sum from i equals one through m of\\nthe loss when you algorithm output a_i on the example y,\\nwhere a_i is the prediction on the ith training example which is sigma of z_i,\\nwhich is equal to sigma of w transpose x_i plus b.\\nSo, what we show in the previous slide is for any single training example,\\nhow to compute the derivatives when you have just one training example.\\nSo dw_1, dw_2 and d_b,\\nwith now the superscript i to denote\\nthe corresponding values you get if you were doing what we did on the previous slide,\\nbut just using the one training example,\\nx_i y_i, excuse me,\\nmissing an i there as well.\\nSo, now you notice the overall cost functions as a sum was really average,\\nbecause the one over m term of the individual losses.\\nSo, it turns out that the derivative,\\nrespect to w_1 of the overall cost function is also going to be\\nthe average of derivatives respect to w_1 of the individual loss terms.\\nBut previously, we have already shown how to compute this term as dw_1_i,\\nwhich we, on the previous slide,\\nshow how to compute this on a single training example.\\nSo, what you need to do is really compute\\nthese derivatives as we showed on the previous training example and average them,\\nand this will give you\\nthe overall gradient that you can use to implement the gradient descent.\\nSo, I know that was a lot of details,\\nbut let's take all of this up and wrap this up into\\na concrete algorithm until when you should\\nimplement logistic regression with gradient descent working.\\nSo, here's what you can do: let's initialize j equals zero,\\ndw_1 equals zero, dw_2 equals zero, d_b equals zero.\\nWhat we're going to do is use a for loop over the training set,\\nand compute the derivative with respect to each training example and then add them up.\\nSo, here's how we do it, for i equals one through m,\\nso m is the number of training examples,\\nwe compute z_i equals w transpose x_i plus b.\\nThe prediction a_i is equal to sigma of z_i,\\nand then let's add up J,\\nJ plus equals y_i log a_i plus one minus y_i log one minus a_i,\\nand then put the negative sign in front of the whole thing,\\nand then as we saw earlier,\\nwe have dz_i, that's equal to a_i minus y_i,\\nand d_w gets plus equals x1_i dz_i,\\ndw_2 plus equals xi_2 dz_i,\\nand I'm doing this calculation assuming that you have just two features,\\nso that n equals to two otherwise,\\nyou do this for dw_1,\\ndw_2, dw_3 and so on,\\nand then db plus equals dz_i,\\nand I guess that's the end of the for loop.\\nThen finally, having done this for all m training examples,\\nyou will still need to divide by m because we're computing averages.\\nSo, dw_1 divide equals m,\\ndw_2 divides equals m,\\ndb divide equals m,\\nin order to compute averages.\\nSo, with all of these calculations,\\nyou've just computed the derivatives of the cost function J with respect\\nto each your parameters w_1, w_2 and b.\\nJust a couple of details about what we're doing,\\nwe're using dw_1 and dw_2 and db as accumulators,\\nso that after this computation,\\ndw_1 is equal to the derivative of\\nyour overall cost function with respect to w_1 and similarly for dw_2 and db.\\nSo, notice that dw_1 and dw_2 do not have a superscript i,\\nbecause we're using them in this code as\\naccumulators to sum over the entire training set.\\nWhereas in contrast, dz_i here,\\nthis was dz with respect to just one single training example.\\nSo, that's why that had a superscript i to refer to the one training example,\\ni that is computerised.\\nSo, having finished all these calculations,\\nto implement one step of gradient descent,\\nyou will implement w_1,\\ngets updated as w_1 minus the learning rate times dw_1,\\nw_2, ends up this as w_2 minus learning rate times dw_2,\\nand b gets updated as b minus the learning rate times db,\\nwhere dw_1, dw_2 and db were as computed.\\nFinally, J here will also be a correct value for your cost function.\\nSo, everything on the slide implements just one single step of gradient descent,\\nand so you have to repeat everything on this slide\\nmultiple times in order to take multiple steps of gradient descent.\\nIn case these details seem too complicated, again,\\ndon't worry too much about it for now,\\nhopefully all this will be clearer when you\\ngo and implement this in the programming assignments.\\nBut it turns out there are two weaknesses\\nwith the calculation as we've implemented it here,\\nwhich is that, to implement logistic regression this way,\\nyou need to write two for loops.\\nThe first for loop is this for loop over the m training examples,\\nand the second for loop is a for loop over all the features over here.\\nSo, in this example,\\nwe just had two features; so,\\nn is equal to two and x equals two,\\nbut maybe we have more features,\\nyou end up writing here dw_1 dw_2,\\nand you similar computations for dw_t,\\nand so on delta dw_n.\\nSo, it seems like you need to have a for loop over the features, over n features.\\nWhen you're implementing deep learning algorithms,\\nyou find that having explicit for loops in\\nyour code makes your algorithm run less efficiency.\\nSo, in the deep learning era,\\nwe would move to a bigger and bigger datasets,\\nand so being able to implement your algorithms without using explicit\\nfor loops is really important and will help you to scale to much bigger datasets.\\nSo, it turns out that there are a set of techniques called vectorization\\ntechniques that allow you to get rid of these explicit for-loops in your code.\\nI think in the pre-deep learning era,\\nthat's before the rise of deep learning,\\nvectorization was a nice to have,\\nso you could sometimes do it to speed up your code and sometimes not.\\nBut in the deep learning era, vectorization,\\nthat is getting rid of for loops,\\nlike this and like this,\\nhas become really important,\\nbecause we're more and more training on very large datasets,\\nand so you really need your code to be very efficient.\\nSo, in the next few videos,\\nwe'll talk about vectorization and how to\\nimplement all this without using even a single for loop.\\nSo, with this, I hope you have a sense of how to\\nimplement logistic regression or gradient descent for logistic regression.\\nThings will be clearer when you implement the programming exercise.\\nBut before actually doing the programming exercise,\\nlet's first talk about vectorization so that you can implement this whole thing,\\nimplement a single iteration of gradient descent without using any for loops.\"},\n",
       "    {'lesson_number': '11',\n",
       "     'lesson_name': 'vectorization',\n",
       "     'content': 'Welcome back. Vectorization is basically\\nthe art of getting rid of explicit for loops in your code.\\nIn the deep learning era, especially in deep learning in practice,\\nyou often find yourself training on relatively large data sets,\\nbecause that\\'s when deep learning algorithms tend to shine.\\nAnd so, it\\'s important that your code very quickly because otherwise,\\nif it\\'s training a big data set,\\nyour code might take a long time to run then you just find\\nyourself waiting a very long time to get the result.\\nSo in the deep learning era,\\nI think the ability to perform vectorization has become a key skill.\\nLet\\'s start with an example.\\nSo, what is Vectorization?\\nIn logistic regression you need to compute Z equals W transpose X plus B,\\nwhere W was this column vector and X is also this vector.\\nMaybe they are very large vectors if you have a lot of features.\\nSo, W and X were both these R and no R, NX dimensional vectors.\\nSo, to compute W transpose X,\\nif you had a non-vectorized implementation,\\nyou would do something like Z equals zero.\\nAnd then for I in range of X.\\nSo, for I equals 1, 2 NX,\\nZ plus equals W I times XI.\\nAnd then maybe you do Z plus equal B at the end.\\nSo, that\\'s a non-vectorized implementation.\\nThen you find that that\\'s going to be really slow.\\nIn contrast, a vectorized implementation would just compute W transpose X directly.\\nIn Python or a numpy,\\nthe command you use for that is Z equals np.W,\\nX, so this computes W transpose X.\\nAnd you can also just add B to that directly.\\nAnd you find that this is much faster.\\nLet\\'s actually illustrate this with a little demo.\\nSo, here\\'s my Jupiter notebook in which I\\'m going to write some Python code.\\nSo, first, let me import the numpy library to import.\\nSend P. And so, for example,\\nI can create A as an array as follows.\\nLet\\'s say print A.\\nNow, having written this chunk of code,\\nif I hit shift enter,\\nthen it executes the code.\\nSo, it created the array A and it prints it out.\\nNow, let\\'s do the Vectorization demo.\\nI\\'m going to import the time libraries,\\nsince we use that,\\nin order to time how long different operations take.\\nCan they create an array A?\\nThose random thought round.\\nThis creates a million dimensional array with random values.\\nb = np.random.rand.\\nAnother million dimensional array.\\nAnd, now, tic=time.time, so this measure the current time,\\nc = np.dot (a, b).\\ntoc = time.time.\\nAnd this print,\\nit is the vectorized version.\\nIt\\'s a vectorize version.\\nAnd so, let\\'s print out.\\nLet\\'s see the last time,\\nso there\\'s toc - tic x 1000,\\nso that we can express this in milliseconds.\\nSo, ms is milliseconds.\\nI\\'m going to hit Shift Enter.\\nSo, that code took about three milliseconds or this time 1.5,\\nmaybe about 1.5 or 3.5 milliseconds at a time.\\nIt varies a little bit as I run it,\\nbut looks like maybe on average it\\'s taking like 1.5 milliseconds,\\nmaybe two milliseconds as I run this.\\nAll right.\\nLet\\'s keep adding to this block of code.\\nThat\\'s not implementing non-vectorize version.\\nLet\\'s see, c = 0,\\nthen tic = time.time.\\nNow, let\\'s implement a for loop.\\nFor I in range of 1 million,\\nI\\'ll pick out the number of zeros right.\\nC += (a,i) x (b,\\ni), and then toc = time.time.\\nFinally, print more than explicit full loop.\\nThe time it takes is this 1000 x toc - tic + \"ms\"\\nto know that we\\'re doing this in milliseconds.\\nLet\\'s do one more thing.\\nLet\\'s just print out the value of C we\\ncompute it to make sure that it\\'s the same value in both cases.\\nI\\'m going to hit shift enter to run this and check that out.\\nIn both cases, the vectorize version\\nand the non-vectorize version computed the same values,\\nas you know, 2.50 to 6.99, so on.\\nThe vectorize version took 1.5 milliseconds.\\nThe explicit for loop and non-vectorize version took about 400, almost 500 milliseconds.\\nThe non-vectorize version took something like 300\\ntimes longer than the vectorize version.\\nWith this example you see that if only you remember to vectorize your code,\\nyour code actually runs over 300 times faster.\\nLet\\'s just run it again.\\nJust run it again.\\nYeah. Vectorize version 1.5 milliseconds seconds and the for loop.\\nSo 481 milliseconds, again,\\nabout 300 times slower to do the explicit for loop.\\nIf the engine x slows down,\\nit\\'s the difference between your code taking maybe one minute to\\nrun versus taking say five hours to run.\\nAnd when you are implementing deep learning algorithms,\\nyou can really get a result back faster.\\nIt will be much faster if you vectorize your code.\\nSome of you might have heard that a lot of\\nscaleable deep learning implementations are done on a GPU or a graphics processing unit.\\nBut all the demos I did just now in the Jupiter notebook where actually on the CPU.\\nAnd it turns out that both GPU and CPU have parallelization instructions.\\nThey\\'re sometimes called SIMD instructions.\\nThis stands for a single instruction multiple data.\\nBut what this basically means is that,\\nif you use built-in functions such as this\\nnp.function or other functions that don\\'t require you explicitly implementing a for loop.\\nIt enables Phyton Pi to take\\nmuch better advantage of parallelism to do your computations much faster.\\nAnd this is true both computations on CPUs and computations on GPUs.\\nIt\\'s just that GPUs are remarkably good at\\nthese SIMD calculations but CPU is actually also not too bad at that.\\nMaybe just not as good as GPUs.\\nYou\\'re seeing how vectorization can significantly speed up your code.\\nThe rule of thumb to remember is whenever possible,\\navoid using explicit for loops.\\nLet\\'s go onto the next video to see some more examples of\\nvectorization and also start to vectorize logistic regression.'},\n",
       "    {'lesson_number': '12',\n",
       "     'lesson_name': 'more-vectorization-examples',\n",
       "     'content': \"In the previous video you saw a few examples of how vectorization,\\nby using built in functions and by avoiding explicit for\\nloops, allows you to speed up your code significantly.\\nLet's look at a few more examples.\\nThe rule of thumb to keep in mind is, when you're programming your neural networks, or\\nwhen you're programming just a regression,\\nwhenever possible avoid explicit for-loops.\\nAnd it's not always possible to never use a for-loop, but when you can\\nuse a built in function or find some other way to compute whatever you need,\\nyou'll often go faster than if you have an explicit for-loop.\\nLet's look at another example.\\nIf ever you want to compute a vector u as the product of the matrix A,\\nand another vector v, then the definition of our matrix\\nmultiply is that your Ui is equal to sum over j,, Aij, Vj.\\nThat's how you define Ui.\\nAnd so the non-vectorized implementation of this\\nwould be to set u equals NP.zeros, it would be n by 1.\\nFor i, and so on.\\nFor j, and so on..\\nAnd then u[i] plus equals a[i][j] times v[j].\\nSo now, this is two for-loops, looping over both i and j.\\nSo, that's a non-vectorized version,\\nthe vectorized implementation which is to say u equals np dot (A,v).\\nAnd the implementation on the right, the vectorized version,\\nnow eliminates two different for-loops, and it's going to be way faster.\\nLet's go through one more example.\\nLet's say you already have a vector, v, in memory and you\\nwant to apply the exponential operation on every element of this vector v.\\nSo you can put u equals the vector, that's e to the v1,\\ne to the v2, and so on, down to e to the vn.\\nSo this would be a non-vectorized implementation,\\nwhich is at first you initialize u to the vector of zeros.\\nAnd then you have a for-loop that computes the elements one at a time.\\nBut it turns out that Python and NumPy have many built-in functions that allow\\nyou to compute these vectors with just a single call to a single function.\\nSo what I would do to implement this is import\\nnumpy as np, and then what you\\njust call u = np.exp(v).\\nAnd so, notice that, whereas previously you had that explicit for-loop,\\nwith just one line of code here, just v as an input vector u as an output vector,\\nyou've gotten rid of the explicit for-loop, and the implementation on\\nthe right will be much faster that the one needing an explicit for-loop.\\nIn fact, the NumPy library has many of the vector value functions.\\nSo np.log (v) will compute the element-wise log,\\nnp.abs computes the absolute value,\\nnp.maximum computes the element-wise maximum\\nto take the max of every element of v with 0.\\nv**2 just takes the element-wise square of each element of v.\\nOne over v takes the element-wise inverse, and so on.\\nSo, whenever you are tempted to write a for-loop take a look, and see if there's\\na way to call a NumPy built-in function to do it without that for-loop.\\nSo, let's take all of these learnings and\\napply it to our logistic regression gradient descent implementation,\\nand see if we can at least get rid of one of the two for-loops we had.\\nSo here's our code for\\ncomputing the derivatives for logistic regression, and we had two for-loops.\\nOne was this one up here, and the second one was this one.\\nSo in our example we had nx equals 2, but\\nif you had more features than just 2 features then you'd\\nneed have a for-loop over dw1, dw2, dw3, and so on.\\nSo its as if there's actually a 4j equals 1, 2, and x.\\ndWj gets updated.\\nSo we'd like to eliminate this second for-loop.\\nThat's what we'll do on this slide.\\nSo the way we'll do so is that instead of explicitly\\ninitializing dw1, dw2, and so on to zeros,\\nwe're going to get rid of this and instead make dw a vector.\\nSo we're going to set dw equals np.zeros, and\\nlet's make this a nx by 1, dimensional vector.\\nThen, here, instead of this for\\nloop over the individual components,\\nwe'll just use this vector value operation,\\ndw plus equals xi times dz(i).\\nAnd then finally, instead of this,\\nwe will just have dw divides equals m.\\nSo now we've gone from having two for-loops to just one for-loop.\\nWe still have this one for-loop that loops over the individual training examples.\\nSo I hope this video gave you a sense of vectorization.\\nAnd by getting rid of one for-loop your code will already run faster.\\nBut it turns out we could do even better.\\nSo the next video will talk about how to vectorize logistic aggression even\\nfurther.\\nAnd you see a pretty surprising result, that without using any for-loops,\\nwithout needing a for-loop over the training examples,\\nyou could write code to process the entire training sets.\\nSo, pretty much all at the same time.\\nSo, let's see that in the next video.\"},\n",
       "    {'lesson_number': '13',\n",
       "     'lesson_name': 'vectorizing-logistic-regression',\n",
       "     'content': \"We have talked about how vectorization lets you speed up your code significantly.\\nIn this video, we'll talk about how you can vectorize\\nthe implementation of logistic regression,\\nso they can process an entire training set,\\nthat is implement a single elevation of grading descent with respect to\\nan entire training set without using even a single explicit for loop.\\nI'm super excited about this technique,\\nand when we talk about neural networks later without\\nusing even a single explicit for loop.\\nLet's get started. Let's first examine the four propagation steps of logistic regression.\\nSo, if you have M training examples,\\nthen to make a prediction on the first example,\\nyou need to compute that,\\ncompute Z. I'm using this familiar formula,\\nthen compute the activations,\\nyou compute y hat in the first example.\\nThen to make a prediction on the second training example,\\nyou need to compute that.\\nThen, to make a prediction on the third example,\\nyou need to compute that, and so on.\\nAnd you might need to do this M times,\\nif you have M training examples.\\nSo, it turns out, that in order to carry out the four propagation step,\\nthat is to compute these predictions on our M training examples,\\nthere is a way to do so,\\nwithout needing an explicit for loop.\\nLet's see how you can do it.\\nFirst, remember that we defined a matrix capital X to be your training inputs,\\nstacked together in different columns like this.\\nSo, this is a matrix,\\nthat is a NX by M matrix.\\nSo, I'm writing this as a Python numpy shape,\\nthis just means that X is a NX by M dimensional matrix.\\nNow, the first thing I want to do is show how you can compute Z1, Z2,\\nZ3 and so on,\\nall in one step,\\nin fact, with one line of code.\\nSo, I'm going to construct a 1\\nby M matrix that's really a row vector while I'm going to compute Z1,\\nZ2, and so on,\\ndown to ZM, all at the same time.\\nIt turns out that this can be expressed as\\nW transpose to capital matrix X plus and then this vector B,\\nB and so on.\\nB, where this thing,\\nthis B, B, B, B,\\nB thing is a 1xM vector or\\n1xM matrix or that is as a M dimensional row vector.\\nSo hopefully there you are with matrix multiplication.\\nYou might see that W transpose X1,\\nX2 and so on to XM,\\nthat W transpose can be a row vector.\\nSo this W transpose will be a row vector like that.\\nAnd so this first term will evaluate to W transpose X1,\\nW transpose X2 and so on, dot, dot, dot,\\nW transpose XM, and then we add this second term B,\\nB, B, and so on,\\nyou end up adding B to each element.\\nSo you end up with another 1xM vector.\\nWell that's the first element,\\nthat's the second element and so on,\\nand that's the nth element.\\nAnd if you refer to the definitions above,\\nthis first element is exactly the definition of Z1.\\nThe second element is exactly the definition of Z2 and so on.\\nSo just as X was once obtained,\\nwhen you took your training examples and\\nstacked them next to each other, stacked them horizontally.\\nI'm going to define capital Z to be this where\\nyou take the lowercase Z's and stack them horizontally.\\nSo when you stack the lower case X's corresponding to a different training examples,\\nhorizontally you get this variable capital X and\\nthe same way when you take these lowercase Z variables,\\nand stack them horizontally,\\nyou get this variable capital Z.\\nAnd it turns out, that in order to implement this,\\nthe non-pie command is capital Z equals NP dot W dot T,\\nthat's W transpose X and then plus B.\\nNow there is a subtlety in Python,\\nwhich is at here B is a real number or if you want to say you know 1x1 matrix,\\nis just a normal real number.\\nBut, when you add this vector to this real number,\\nPython automatically takes this real number B and expands it out to this 1XM row vector.\\nSo in case this operation seems a little bit mysterious,\\nthis is called broadcasting in Python,\\nand you don't have to worry about it for now,\\nwe'll talk about it some more in the next video.\\nBut the takeaway is that with just one line of code, with this line of code,\\nyou can calculate capital Z and capital Z is\\ngoing to be a 1XM matrix that contains all of the lower cases Z's.\\nLowercase Z1 through lower case ZM.\\nSo that was Z, how about these values A.\\nWhat we like to do next,\\nis find a way to compute A1,\\nA2 and so on to AM,\\nall at the same time,\\nand just as stacking lowercase X's resulted in\\ncapital X and stacking horizontally lowercase Z's resulted in capital Z,\\nstacking lower case A,\\nis going to result in a new variable,\\nwhich we are going to define as capital A.\\nAnd in the program assignment,\\nyou see how to implement a vector valued sigmoid function,\\nso that the sigmoid function,\\ninputs this capital Z as a variable and very efficiently outputs capital A.\\nSo you see the details of that in the programming assignment.\\nSo just to recap,\\nwhat we've seen on this slide is that instead of needing to loop over\\nM training examples to compute lowercase Z and lowercase A,\\none of the time, you can implement this one line of code,\\nto compute all these Z's at the same time.\\nAnd then, this one line of code,\\nwith appropriate implementation of\\nlowercase Sigma to compute all the lowercase A's all at the same time.\\nSo this is how you implement\\na vectorize implementation of\\nthe four propagation for all M training examples at the same time.\\nSo to summarize, you've just seen how you can use\\nvectorization to very efficiently compute all of the activations,\\nall the lowercase A's at the same time.\\nNext, it turns out, you can also use vectorization very\\nefficiently to compute the backward propagation,\\nto compute the gradients.\\nLet's see how you can do that, in the next video.\"},\n",
       "    {'lesson_number': '14',\n",
       "     'lesson_name': 'vectorizing-logistic-regressions-gradient-output',\n",
       "     'content': \"In the previous video,\\nyou saw how you can use vectorization to compute their predictions.\\nThe lowercase a's for an entire training set all at the same time.\\nIn this video, you see how you can use vectorization to also\\nperform the gradient computations for all M training samples.\\nAgain, all sort of at the same time.\\nAnd then at the end of this video,\\nwe'll put it all together and show how you can derive\\na very efficient implementation of logistic regression.\\nSo, you may remember that for the gradient computation,\\nwhat we did was we computed dz1 for the first example,\\nwhich could be a1 minus y1 and then dz2 equals\\na2 minus y2 and so on.\\nAnd so on for all M training examples.\\nSo, what we're going to do is define a new variable,\\ndZ is going to be dz1, dz2, dzm.\\nAgain, all the D lowercase z variables stacked horizontally.\\nSo, this would be 1 by m matrix or alternatively a m dimensional row vector.\\nNow recall that from the previous slide,\\nwe'd already figured out how to compute capital A which was this: a1 through\\nam and we had defined capital Y as y1 through ym.\\nAlso you know, stacked horizontally.\\nSo, based on these definitions,\\nmaybe you can see for yourself that dz can be computed as\\njust A minus Y because it's going to be equal to a1 - y1.\\nSo, the first element, a2 - y2,\\nso in the second element and so on.\\nAnd, so this first element a1 - y1 is exactly the definition of dz1.\\nThe second element is exactly the definition of dz2 and so on.\\nSo, with just one line of code,\\nyou can compute all of this at the same time.\\nNow, in the previous implementation,\\nwe've gotten rid of one for loop already but we still had\\nthis second for loop over training examples.\\nSo we initialize dw to zero to a vector of zeroes.\\nBut then we still have to loop over 20 examples where we have\\ndw plus equals x1 times dz1,\\nfor the first training example dw plus equals x2 dz2 and so on.\\nSo we do the M times and then dw divide equals by M and similarly for B, right?\\ndb was initialized as 0 and db plus equals dz1.\\ndb plus equals dz2 down to you know\\ndz(m) and db divide equals M. So that's what we had in the previous implementation.\\nWe'd already got rid of one for loop.\\nSo, at least now dw is a vector and we went separately updating dw1,\\ndw2 and so on.\\nSo, we got rid of that already but we still\\nhad the for loop over the M examples in the training set.\\nSo, let's take these operations and vectorize them.\\nHere's what we can do, for\\nthe vectorized implementation of db, what it's doing is basically summing up,\\nall of these dzs and then dividing by m. So,\\ndb is basically one over m,\\nsum from I equals one through m of dzi and\\nwell all the dzs are in that row vector and so in Python,\\nwhat you do is implement you know,\\n1 over a m times np.\\nsum of dz.\\nSo, you just take this variable and call the np.\\nsum function on it and that would give you db.\\nHow about dw? I'll just write\\nout the correct equations who can verify is the right thing to do.\\nDW turns out to be one over M,\\ntimes the matrix X times dz transpose.\\nAnd, so kind of see why that's the case.\\nThis is equal to one over m then the matrix X's,\\nx1 through xm stacked up in columns like that and dz\\ntranspose is going to be dz1 down to dz(m) like so.\\nAnd so, if you figure out what this matrix times this vector works out to be,\\nit is turns out to be one over m times x1\\ndz1 plus... plus xm dzm.\\nAnd so, this is a n/1 vector and this is what you actually end up with,\\nwith dw because dw was taking these you know,\\nxi dzi and adding them up and so that's what exactly\\nthis matrix vector multiplication is doing and so again,\\nwith one line of code you can compute dw.\\nSo, the vectorized implementation of the derivative calculations is just this,\\nyou use this line to implement db and use\\nthis line to implement dw and notice that without a for loop over the training set,\\nyou can now compute the updates you want to your parameters.\\nSo now, let's put all together into how you would actually implement logistic regression.\\nSo, this is our original,\\nhighly inefficient non vectorize implementation.\\nSo, the first thing we've done in the previous video was get rid of this volume, right?\\nSo, instead of looping over dw1,\\ndw2 and so on,\\nwe have replaced this with a vector value dw which is dw+= xi,\\nwhich is now a vector times dz(i).\\nBut now, we will see that we can also get rid of not\\njust a for loop below but also get rid of this for loop.\\nSo, here is how you do it.\\nSo, using what we have from the previous slides,\\nyou would say, capitalZ,\\nZ equal to w transpose X + B and the code you is write capital Z equals np.\\nw transpose X + B and then a equals sigmoid of capital Z.\\nSo, you have now computed all of this and all of this for all the values of I.\\nNext on the previous slide,\\nwe said you would compute dz equals A - Y.\\nSo, now you computed all of this for all the values of i.\\nThen, finally dw equals 1/m x\\ndz transpose and db equals 1/m of you know, np.\\nsum dz.\\nSo, you've just done forward propagation and back propagation,\\nreally computing the predictions and computing the derivatives on\\nall M training examples without using a for loop.\\nAnd so the gradient descent update then would be you know W\\ngets updated as w minus the learning rate times\\ndw which was just computed above and B is update as B minus the learning rate times db.\\nSometimes is putting colons to that to denote that as is an assignment,\\nbut I guess I haven't been totally consistent with that notation.\\nBut with this, you have just implemented\\na single iteration of gradient descent for logistic regression.\\nNow, I know I said that we should get rid of\\nexplicit for loops whenever you can but if you want to\\nimplement multiple iterations as\\na gradient descent then you still need a for loop over the number of iterations.\\nSo, if you want to have a thousand iterations of gradient descent,\\nyou might still need a for loop over the iteration number.\\nThere is an outermost for loop like that then I\\ndon't think there is any way to get rid of that for loop.\\nBut I do think it's incredibly cool that you can implement\\nat least one iteration of gradient descent without needing to use a for loop.\\nSo, that's it you now have a highly vectorize and\\nhighly efficient implementation of gradient descent for logistic regression.\\nThere is just one more detail that I want to talk about in the next video,\\nwhich is in our description here I briefly alluded to this technique called broadcasting.\\nBroadcasting turns out to be a technique that Python and\\nnumpy allows you to use to make certain parts of your code also much more efficient.\\nSo, let's see some more details of broadcasting in the next video.\"},\n",
       "    {'lesson_number': '15',\n",
       "     'lesson_name': 'broadcasting-in-python',\n",
       "     'content': \"In the previous video, I mentioned that broadcasting\\nis another technique that you can use to make your Python code run faster.\\nIn this video, let's delve into how broadcasting in Python actually works.\\nLet's explore broadcasting with an example.\\nIn this matrix, I've shown the number of calories from carbohydrates,\\nproteins, and fats in 100 grams of four different foods.\\nSo for example, a 100 grams of apples turns out,\\nhas 56 calories from carbs, and much less from proteins and fats.\\nWhereas, in contrast, a 100 grams of beef has 104 calories from protein and\\n135 calories from fat.\\nNow, let's say your goal is to calculate the percentage of calories\\nfrom carbs, proteins and fats for each of the four foods.\\nSo, for example, if you look at this column and\\nadd up the numbers in that column you get that 100 grams of apple\\nhas 56 plus 1.2 plus 1.8 so that's 59 calories.\\nAnd so as a percentage the percentage of\\ncalories from carbohydrates in an apple would\\nbe 56 over 59, that's about 94.9%.\\nSo most of the calories in an apple come from carbs, whereas in contrast,\\nmost of the calories of beef come from protein and fat and so on.\\nSo the calculation you want is really to sum up each of the four columns\\nof this matrix to get the total number of calories in 100 grams of apples,\\nbeef, eggs, and potatoes.\\nAnd then to divide throughout the matrix,\\nso as to get the percentage of calories from carbs, proteins and\\nfats for each of the four foods.\\nSo the question is, can you do this without an explicit for-loop?\\nLet's take a look at how you could do that.\\nWhat I'm going to do is show you how you can set,\\nsay this matrix equal to three by four matrix A.\\nAnd then with one line of Python code we're going to sum down the columns.\\nSo we're going to get four numbers corresponding to the total number\\nof calories in these four different types of foods,\\n100 grams of these four different types of foods.\\nAnd I'm going to use a second line of Python code to divide each of\\nthe four columns by their corresponding sum.\\nIf that verbal description wasn't very clearly,\\nhopefully it will be clearer in a second when we look in the Python code.\\nSo here we are in the Jupiter notebook.\\nI've already written this first piece of code to prepopulate\\nthe matrix A with the numbers we had just now, so we'll hit shift enter and\\njust run that, so there's the matrix A.\\nAnd now here are the two lines of Python code.\\nFirst, we're going to compute tau equals a, that sum.\\nAnd x is equals 0 means to sum vertically.\\nWe'll say more about that in a little bit.\\nAnd then print cal.\\nSo we'll sum vertically.\\nNow 59 is the total number of calories in the apple, 239 was\\nthe total number of calories in the beef and the eggs and potato and so on.\\nAnd then with a compute percentage\\nequals A/cal.reshape 1,4.\\nActually we want percentages, so multiply by 100 here.\\nAnd then let's print percentage.\\nLet's run that.\\nAnd so that command we've taken the matrix A and\\ndivided it by this one by four matrix.\\nAnd this gives us the matrix of percentages.\\nSo as we worked out kind of by hand just now in the apple there\\nwas a first column 94.9% of the calories are from carbs.\\nLet's go back to the slides.\\nSo just to repeat the two lines of code we had,\\nthis is what have written out in the Jupiter notebook.\\nTo add a bit of detail this parameter,\\n(axis = 0), means that you want Python to sum vertically.\\nSo if this is axis 0 this means to sum vertically,\\nwhere as the horizontal axis is axis 1.\\nSo be able to write axis 1 or sum horizontally instead of sum vertically.\\nAnd then this command here,\\nthis is an example of Python broadcasting where you take a matrix A.\\nSo this is a three by four matrix and you divide it by a one by four matrix.\\nAnd technically, after this first line of codes cal, the variable cal,\\nis already a one by four matrix.\\nSo technically you don't need to call reshape here again, so\\nthat's actually a little bit redundant.\\nBut when I'm writing Python codes if I'm not entirely sure what matrix,\\nwhether the dimensions of a matrix I often would just call a reshape command just to\\nmake sure that it's the right column vector or the row vector or\\nwhatever you want it to be.\\nThe reshape command is a constant time.\\nIt's a order one operation that's very cheap to call.\\nSo don't be shy about using the reshape command to make sure that your matrices\\nare the size you need it to be.\\nNow, let's explain in greater detail how this type of operation works, right?\\nWe had a three by four matrix and we divided it by a one by four matrix.\\nSo, how can you divide a three by four matrix by a one by four matrix?\\nOr by one by four vector?\\nLet's go through a few more examples of broadcasting.\\nIf you take a 4 by 1 vector and add it to a number, what\\nPython will do is take this number and auto-expand\\nit into a four by one vector as well, as follows.\\nAnd so the vector [1, 2, 3,\\n4] plus the number 100 ends up with that vector on the right.\\nYou're adding a 100 to every element, and in fact we use this form of\\nbroadcasting where that constant was the parameter b in an earlier video.\\nAnd this type of broadcasting works with both column vectors and row vectors,\\nand in fact we use a similar form of broadcasting earlier with the constant\\nwe're adding to a vector being the parameter b in logistic regression.\\nHere's another example.\\nLet's say you have a two by three matrix and\\nyou add it to this one by n matrix.\\nSo the general case would be if you\\nhave some (m,n) matrix here and\\nyou add it to a (1,n) matrix.\\nWhat Python will do is copy the matrix m,\\ntimes to turn this into m by n matrix, so instead of this one by\\nthree matrix it'll copy it twice in this example to turn it into this.\\nAlso, two by three matrix and we'll add these so\\nyou'll end up with the sum on the right, okay?\\nSo you taken, you added 100 to the first column,\\nadded 200 to second column, added 300 to the third column.\\nAnd this is basically what we did on the previous slide,\\nexcept that we use a division operation instead of an addition operation.\\nSo one last example, whether you have a (m,n) matrix and\\nyou add this to a (m,1) vector, (m,1) matrix.\\nThen just copy this n times horizontally.\\nSo you end up with an (m,n) matrix.\\nSo as you can imagine you copy it horizontally three times.\\nAnd you add those.\\nSo when you add them you end up with this.\\nSo we've added 100 to the first row and added 200 to the second row.\\nHere's the more general principle of broadcasting in Python.\\nIf you have an (m,n) matrix and you add or\\nsubtract or multiply or divide with a (1,n) matrix,\\nthen this will copy it n times into an (m,n) matrix.\\nAnd then apply the addition, subtraction, and\\nmultiplication of division element wise.\\nIf conversely, you were to take the (m,n) matrix and add, subtract, multiply,\\ndivide by an (m,1) matrix, then also this would copy it now n times.\\nAnd turn that into an (m,n) matrix and then apply the operation element wise.\\nJust one of the broadcasting, which is if you have an (m,1) matrix,\\nso that's really a column vector like [1,2,3], and you add,\\nsubtract, multiply or divide by a row number.\\nSo maybe a (1,1) matrix.\\nSo such as that plus 100, then you end up copying\\nthis real number n times until you'll also get another (n,1) matrix.\\nAnd then you perform the operation such as addition on this example element-wise.\\nAnd something similar also works for row vectors.\\nThe fully general version of broadcasting can do even a little bit more than this.\\nIf you're interested you can read the documentation for\\nNumPy, and look at broadcasting in that documentation.\\nThat gives an even slightly more general definition of broadcasting.\\nBut the ones on the slide are the main forms of broadcasting that you end up\\nneeding to use when you implement a neural network.\\nBefore we wrap up, just one last comment, which is for\\nthose of you that are used to programming in either MATLAB or\\nOctave, if you've ever used the MATLAB or Octave function bsxfun\\nin neural network programming bsxfun does something similar, not quite the same.\\nBut it is often used for similar purpose as what we use broadcasting in Python for.\\nBut this is really only for very advanced MATLAB and\\nOctave users, if you've not heard of this, don't worry about it.\\nYou don't need to know it when you're coding up neural networks in Python.\\nSo, that was broadcasting in Python.\\nI hope that when you do the programming homework that broadcasting will allow you\\nto not only make a code run faster,\\nbut also help you get what you want done with fewer lines of code.\\nBefore you dive into the programming excercise, I want to share with you just\\none more set of ideas, which is that there's some tips and\\ntricks that I've found reduces the number of bugs in my Python code and\\nthat I hope will help you too.\\nSo with that, let's talk about that in the next video.\"},\n",
       "    {'lesson_number': '16',\n",
       "     'lesson_name': 'a-note-on-python-numpy-vectors',\n",
       "     'content': \"The ability of python to allow you to use broadcasting operations and\\nmore generally, the great flexibility of the python numpy program language is,\\nI think, both a strength as well as a weakness of the programming language.\\nI think it's a strength because they create expressivity of the language.\\nA great flexibility of the language lets you get a lot done even with just a single\\nline of code.\\nBut there's also weakness because with broadcasting and this great amount of\\nflexibility, sometimes it's possible you can introduce very subtle bugs or\\nvery strange looking bugs, if you're not familiar with all of the intricacies of\\nhow broadcasting and how features like broadcasting work.\\nFor example, if you take a column vector and add it to a row vector, you would\\nexpect it to throw up a dimension mismatch or type error or something.\\nBut you might actually get back a matrix as a sum of a row vector and\\na column vector.\\nSo there is an internal logic to these strange effects of Python.\\nBut if you're not familiar with Python, I've seen some students have very strange,\\nvery hard to find bugs.\\nSo what I want to do in this video is share with you some couple tips and\\ntricks that have been very useful for me to eliminate or\\nsimplify and eliminate all the strange looking bugs in my own code.\\nAnd I hope that with these tips and tricks,\\nyou'll also be able to much more easily write bug-free, python and numpy code.\\nTo illustrate one of the less intuitive effects of Python-Numpy,\\nespecially how you construct vectors in Python-Numpy, let me do a quick demo.\\nLet's set a = np.random.randn(5),\\nso this creates five random Gaussian\\nvariables stored in array a.\\nAnd so let's print(a) and now it turns out that\\nthe shape of a when you do this is this five color structure.\\nAnd so this is called a rank 1 array in Python and\\nit's neither a row vector nor a column vector.\\nAnd this leads it to have some slightly non-intuitive effects.\\nSo for example, if I print a transpose, it ends up looking the same as a.\\nSo a and a transpose end up looking the same.\\nAnd if I print the inner product between a and a transpose, you might think\\na times a transpose is maybe the outer product should give you matrix maybe.\\nBut if I do that, you instead get back a number.\\nSo what I would recommend is that when you're coding new networks,\\nthat you just not use data structures where the shape is 5, or n, rank 1 array.\\nInstead, if you set a to be this, (5,1),\\nthen this commits a to be (5,1) column vector.\\nAnd whereas previously, a and a transpose looked the same,\\nit becomes now a transpose, now a transpose is a row vector.\\nNotice one subtle difference.\\nIn this data structure, there are two square brackets when we print a transpose.\\nWhereas previously, there was one square bracket.\\nSo that's the difference between this is really a 1 by\\n5 matrix versus one of these rank 1 arrays.\\nAnd if you print, say, the product between a and a transpose,\\nthen this gives you the outer product of a vector, right?\\nAnd so, the outer product of a vector gives you a matrix.\\nSo, let's look in greater detail at what we just saw here.\\nThe first command that we ran, just now, was this.\\nAnd this created a data structure with\\na.shape was this funny thing (5,) so\\nthis is called a rank 1 array.\\nAnd this is a very funny data structure.\\nIt doesn't behave consistently as either a row vector nor a column vector,\\nwhich makes some of its effects nonintuitive.\\nSo what I'm going to recommend is that when you're doing your programing\\nexercises, or in fact when you're implementing logistic regression or\\nneural networks that you just do not use these rank 1 arrays.\\nInstead, if every time you create an array,\\nyou commit to making it either a column vector, so\\nthis creates a (5,1) vector, or commit to making it a row vector,\\nthen the behavior of your vectors may be easier to understand.\\nSo in this case, a.shape is going to be equal to 5,1.\\nAnd so this behaves a lot like a, but in fact, this is a column vector.\\nAnd that's why you can think of this as (5,1) matrix, where it's a column vector.\\nAnd here a.shape is going to be 1,5,\\nand this behaves consistently as a row vector.\\nSo when you need a vector, I would say either use this or this, but\\nnot a rank 1 array.\\nOne more thing that I do a lot in my code is if I'm not entirely sure what's\\nthe dimension of one of my vectors, I'll often throw in an assertion statement\\nlike this, to make sure, in this case, that this is a (5,1) vector.\\nSo this is a column vector.\\nThese assertions are really inexpensive to execute, and\\nthey also help to serve as documentation for your code.\\nSo don't hesitate to throw in assertion statements like this whenever you\\nfeel like.\\nAnd then finally, if for some reason you do end up with a rank 1 array,\\nYou can reshape this, a equals a.reshape\\ninto say a (5,1) array or a (1,5) array so\\nthat it behaves more consistently as either column vector or row vector.\\nSo I've sometimes seen students end up with very hard to track\\nbecause those are the nonintuitive effects of rank 1 arrays.\\nBy eliminating rank 1 arrays in my old code, I think my code became simpler.\\nAnd I did not actually find it restrictive in terms of things I could\\nexpress in code.\\nI just never used a rank 1 array.\\nAnd so takeaways are to simplify your code, don't use rank 1 arrays.\\nAlways use either n by one matrices,\\nbasically column vectors, or one by n matrices, or basically row vectors.\\nFeel free to toss a lot of insertion statements, so\\ndouble-check the dimensions of your matrices and arrays.\\nAnd also, don't be shy about calling the reshape operation to make sure that your\\nmatrices or your vectors are the dimension that you need it to be.\\nSo that,\\nI hope that this set of suggestions helps you to eliminate a cause of bugs\\nfrom Python code, and makes the problem exercise easier for you to complete.\"},\n",
       "    {'lesson_number': '17',\n",
       "     'lesson_name': 'quick-tour-of-jupyter-ipython-notebooks',\n",
       "     'content': \"With everything you've learned, you're just about\\nready to tackle your first programming assignment.\\nBefore you do that, let me just give you a quick tour of iPython notebooks in Coursera.\\nHere you see Jupiter iPython notebook that you can get to on Coursera.\\nLet me just quickly show you a few features of this.\\nThe instructions are written right here in the text in the iPython notebook.\\nAnd these long light gray blocks are blocks of code.\\nSo occasionally, you'll see in\\nthese blocks something that'll say this is the START CODE HERE and END CODE HERE.\\nTo do your exercise please make sure\\nto write your code between the START CODE HERE and END CODE HERE.\\nSo, for example, print Hello world.\\nAnd then to execute a code block,\\nyou can hit shift+enter and then execute this code block which,\\nI guess, we just wrote print Hello world.\\nSo that prints it Hello World.\\nTo run a cell,\\nyou can also, to run one of these code blocks of cell,\\nyou can also click cell and then run cell. So that executes this.\\nIt's possible that on your computer,\\nthe keyboard shortcut for Cell,\\nRun Cell might be different than shift+enter.\\nBut on both, my Mac as well as on my PC is shift+enter,\\nso might be the same for you as well.\\nNow when you're reading the instructions,\\nif you accidentally double click on it,\\nyou might end up with this mark down language.\\nIf you end up with this funny looking text,\\nto convert it back to the nice looking text just run this Cell.\\nSo you can go to Cell, Run Cell or I'm going to hit shift+enter and\\nthat basically executes the mark down and turns it back into this nice looking code.\\nJust a couple more tips.\\nWhen you execute code like this,\\nit actually runs on a kernel,\\non a piece of code that runs on the server.\\nIf you're running an excessively large job or if\\nyou leave a computer for a very long time or something goes wrong,\\nyour internet connection or something,\\nthere is a small chance that a kernel on the back end might die,\\nin which case, just click Kernel and then restart Kernel.\\nAnd hopefully, that will reboot the kernel and make it work again.\\nSo that shouldn't happen if you're just running\\nrelatively small jobs and you're just starting up iPython notebook.\\nIf you see an error message that the Kernel has died or something,\\nyou can try Kernel, Restart.\\nFinally, in iPython notebook,\\nlike this, there may be multiple blocks of code.\\nSo even if an earlier block of code doesn't have any create in code,\\nmake sure to execute this block of code because,\\nin this example, it imports numpy as np and so on,\\nand sets up some of the variables that you might need in order to\\nexecute the lower down blocks of code.\\nSo be sure to execute the ones on top even if you aren't asked to write any code in them.\\nAnd finally, when you're done implementing your solutions,\\nthere's this blue submit assignment buttons here on the upper right and we\\nclick that to submit your solutions for grading.\\nI've found that the interactive command shell nature\\nof iPython notebooks to be very useful for learning quickly,\\nimplement a few lines of code,\\nsee an outcome, learn and add very quickly.\\nAnd so I hope that from the exercises in Coursera,\\nJupyter iPython notebooks will help you quickly learn\\nand experiment and see how to implement these algorithms.\\nThere's one more video after this.\\nThis is an optional video that talks about\\nthe cost function for logistic regression. You can watch that or not.\\nEither way is perfectly fine.\\nBut either way, best of luck with the week 2 programming assignments.\\nAnd I also look forward to seeing you at the start of the week three.\"},\n",
       "    {'lesson_number': '18',\n",
       "     'lesson_name': 'explanation-of-logistic-regression-cost-function-optional',\n",
       "     'content': \"In an earlier video, I've written down a form for the cost function for\\nlogistic regression.\\nIn this optional video, I want to give you a quick justification for\\nwhy we like to use that cost function for logistic regression.\\nTo quickly recap, in logistic regression,\\nwe have that the prediction y hat is sigmoid of w transpose x + b,\\nwhere sigmoid is this familiar function.\\nAnd we said that we want to interpret y hat as the p( y = 1 | x).\\nSo we want our algorithm to output y hat as the chance\\nthat y = 1 for a given set of input features x.\\nSo another way to say this is that if y is equal to 1\\nthen the chance of y given x is equal to y hat.\\nAnd conversely if y is equal to 0 then\\nthe chance that y was 0 was 1- y hat, right?\\nSo if y hat was a chance, that y = 1,\\nthen 1- y hat is the chance that y = 0.\\nSo, let me take these last two equations and just copy them to the next slide.\\nSo what I'm going to do is take these two equations which\\nbasically define p(y|x) for the two cases of y = 0 or y = 1.\\nAnd then take these two equations and summarize them into a single equation.\\nAnd just to point out y has to be either 0 or 1 because in binary cost equations,\\ny = 0 or 1 are the only two possible cases, all right.\\nWhen someone take these two equations and summarize them as follows.\\nLet me just write out what it looks like, then we'll explain why it looks like that.\\nSo (1 â€“ y hat) to the power of (1 â€“ y).\\nSo it turns out this one line summarizes the two equations on top.\\nLet me explain why.\\nSo in the first case, suppose y = 1, right?\\nSo if y = 1 then this term ends up being y hat,\\nbecause that's y hat to the power of 1.\\nThis term ends up being 1- y hat to the power of 1- 1, so that's the power of 0.\\nBut, anything to the power of 0 is equal to 1, so that goes away.\\nAnd so, this equation, just as p(y|x) = y hat, when y = 1.\\nSo that's exactly what we wanted.\\nNow how about the second case, what if y = 0?\\nIf y = 0, then this equation above is p(y|x) = y hat to the 0,\\nbut anything to the power of 0 is equal to 1, so\\nthat's just equal to 1 times 1- y hat to the power of 1- y.\\nSo 1- y is 1- 0, so this is just 1.\\nAnd so this is equal to 1 times (1- y hat) = 1- y hat.\\nAnd so here we have that the y = 0, p (y|x) = 1- y hat,\\nwhich is exactly what we wanted above.\\nSo what we've just shown is that this equation\\nis a correct definition for p(ylx).\\nNow, finally, because the log function is a strictly monotonically increasing function,\\nyour maximizing log p(y|x) should give you a similar result as\\noptimizing p(y|x). And if you compute log of p(y|x), thatâ€™s equal to\\nlog of y hat to the power of y, 1 - y hat to the power of 1 - y.\\nAnd so that simplifies to y log y hat\\n+ 1- y times log 1- y hat, right?\\nAnd so this is actually negative of the loss\\nfunction that we had to find previously.\\nAnd there's a negative sign there because usually if you're training a learning\\nalgorithm, you want to make probabilities large\\nwhereas in logistic regression we're expressing this.\\nWe want to minimize the loss function.\\nSo minimizing the loss corresponds to maximizing the log of the probability.\\nSo this is what the loss function on a single example looks like.\\nHow about the cost function,\\nthe overall cost function on the entire training set on m examples?\\nLet's figure that out.\\nSo, the probability of all the labels In the training set.\\nWriting this a little bit informally.\\nIf you assume that the training examples I've drawn independently or drawn IID,\\nidentically independently distributed,\\nthen the probability of the example is the product of probabilities.\\nThe product from i = 1 through m p(y(i) ) given x(i).\\nAnd so if you want to carry out maximum likelihood estimation, right,\\nthen you want to maximize the, find the parameters that maximizes\\nthe chance of your observations and training set.\\nBut maximizing this is the same as maximizing the log, so\\nwe just put logs on both sides.\\nSo log of the probability of the labels in the training set is equal to,\\nlog of a product is the sum of the log.\\nSo that's sum from i=1 through m of log p(y(i)) given x(i).\\nAnd we have previously figured out on the previous\\nslide that this is negative L of y hat i, y i.\\nAnd so in statistics, there's a principle called the principle of maximum likelihood\\nestimation, which just means to choose the parameters that maximizes this thing.\\nOr in other words, that maximizes this thing.\\nNegative sum from i = 1 through m L(y hat ,y) and\\njust move the negative sign outside the summation.\\nSo this justifies the cost we had for\\nlogistic regression which is J(w,b) of this.\\nAnd because we now want to minimize the cost instead of maximizing likelihood,\\nwe've got to rid of the minus sign.\\nAnd then finally for convenience, to make sure that our quantities are better scale,\\nwe just add a 1 over m extra scaling factor there.\\nBut so to summarize, by minimizing this cost function J(w,b) we're really\\ncarrying out maximum likelihood estimation with the logistic regression model.\\nUnder the assumption that our training examples were IID, or\\nidentically independently distributed.\\nSo thank you for watching this video, even though this is optional.\\nI hope this gives you a sense of why we use the cost function we do for\\nlogistic regression.\\nAnd with that, I hope you go on to the programming exercises and\\nthe quiz questions of this week.\\nAnd best of luck with both the quizzes, and the programming exercise.\"},\n",
       "    {'lesson_number': '19',\n",
       "     'lesson_name': 'pieter-abbeel-interview',\n",
       "     'content': \"So, thanks a lot, Pieter,\\nfor joining me today.\\nI think a lot of people know you as\\na well-known machine learning and deep learning and robotics researcher.\\nI'd like to have people hear a bit about your story.\\nHow did you end up doing the work that you do?\\nThat's a good question and actually if you would have asked me as a 14-year-old,\\nwhat I was aspiring to do,\\nit probably would not have been this.\\nIn fact, at the time,\\nI thought being a professional basketball player would be the right way to go.\\nI don't think I was able to achieve it.\\nI feel the machine learning lucked out,\\nthat the basketball thing didn't work out.\\nYes, that didn't work out.\\nIt was a lot of fun playing basketball but it didn't work\\nout to try to make it into a career.\\nSo, what I really liked in school was physics and math.\\nAnd so, from there,\\nit seemed pretty natural to study engineering which\\nis applying physics and math in the real world.\\nAnd actually then, after my undergrad in electrical engineering,\\nI actually wasn't so sure what to do because,\\nliterally, anything engineering seemed interesting to me.\\nUnderstanding how anything works seems interesting.\\nTrying to build anything is interesting.\\nAnd in some sense,\\nartificial intelligence won out because it seemed like it\\ncould somehow help all disciplines in some way.\\nAnd also, it seemed somehow a little more at the core of everything.\\nYou think about how a machine can think,\\nthen maybe that's more the core of everything else than picking any specific discipline.\\nI've been saying AI is the new electricity,\\nsounds like the 14-year-old version of you;\\nhad an earlier version of that even.\\nYou know, in the past few years you've done a lot of work in deep reinforcement learning.\\nWhat's happening? Why is deep reinforcement learning suddenly taking off?\\nBefore I worked in deep reinforcement learning,\\nI worked a lot in reinforcement learning;\\nactually with you and Durant at Stanford, of course.\\nAnd so, we worked on autonomous helicopter flight,\\nthen later at Berkeley with some of my students who worked\\non getting a robot to learn to fold laundry.\\nAnd kind of what characterized the work was a combination\\nof learning that enabled things that would not be possible without learning,\\nbut also a lot of domain expertise in combination with the learning to get this to work.\\nAnd it was very\\ninteresting because you needed domain expertise which\\nwas fun to acquire but, at the same time,\\nwas very time-consuming for every new application you wanted to succeed of;\\nyou needed domain expertise plus machine learning expertise.\\nAnd for me it was in 2012 with\\nthe ImageNet breakthrough results from Geoff Hinton's group in Toronto,\\nAlexNet showing that supervised learning, all of a sudden,\\ncould be done with far less engineering for the domain at hand.\\nThere was very little engineering by vision in AlexNet.\\nIt made me think we really should revisit\\nreinforcement learning under the same kind of viewpoint and see if we can\\nget the diversion of reinforcement learning to work and do\\nequally interesting things as had just happened in the supervised learning.\\nIt sounds like you saw earlier than\\nmost people the potential of deep reinforcement learning.\\nSo now looking in to the future,\\nwhat do you see next?\\nWhat are your predictions for the\\nnext several ways to come in deep reinforcement learning?\\nSo, I think what's interesting about deep reinforcement learning is that,\\nin some sense, there is many more questions than in supervised learning.\\nIn supervised learning, it's about learning an input output mapping.\\nIn reinforcement learning there is the notion of: Where does the data even come from?\\nSo that's the exploration problem.\\nWhen you have data, how do you do credit assignment?\\nHow do you understand what actions you took early on got you the reward later?\\nAnd then, there is issues of safety.\\nWhen you have a system autonomously collecting data,\\nit's actually rather dangerous in most situations.\\nImagine a self-driving car company that says,\\nwe're just going to run deep reinforcement learning.\\nIt's pretty likely that car would get into a lot of\\naccidents before it does anything useful.\\nYou needed negative examples of that, right?\\nYou do need some negative examples somehow, yes;\\nand positive ones, hopefully.\\nSo, I think there is still a lot of challenges in\\ndeep reinforcement learning in terms of\\nworking out some of the specifics of how to get these things to work.\\nSo, the deep part is the representation,\\nbut then the reinforcement learning itself still has a lot of questions.\\nAnd what I feel is that,\\nwith the advances in deep learning,\\nsomehow one part of the puzzle in reinforcement learning has been largely addressed,\\nwhich is the representation part.\\nSo, if there is a pattern we can\\nprobably represent it with a deep network and capture that pattern.\\nAnd how to tease apart the pattern is still a big challenge in reinforcement learning.\\nSo I think big challenges are,\\nhow to get systems to reason over long time horizons.\\nSo right now, a lot of the successes\\nin deep reinforcement learning are a very short horizon.\\nThere are problems where,\\nif you act well over a five second horizon,\\nyou act well over the entire problem.\\nAnd so a five second scale is something very different from a day long scale,\\nor the ability to live a life as a robot or some software agent.\\nSo, I think there's a lot of challenges there.\\nI think safety has a lot of challenges in terms of,\\nhow do you learn safely and also how do\\nyou keep learning once you're already pretty good?\\nSo, to give an example again that\\na lot of people would be familiar with, self-driving cars,\\nfor a self-driving car to be better than a human driver,\\nshould human drivers maybe get into bad accidents every three million miles or something.\\nAnd so, that takes a long time to see the negative data;\\nonce you're as good as a human driver.\\nBut you want your self-driving car to be better than a human driver.\\nAnd so, at that point the data collection becomes really really difficult to get\\nthat interesting data that makes your system improve.\\nSo, it's a lot of challenges related to exploration, that tie into that.\\nBut one of the things I'm actually most excited about right now is seeing\\nif we can actually take a step back and also learn the reinforcement learning algorithm.\\nSo, reinforcement is very complex,\\ncredit assignment is very complex, exploration is very complex.\\nAnd so maybe, just like\\nhow deep learning for supervised learning was able to replace a lot of domain expertise,\\nmaybe we can have programs that are learned,\\nthat are reinforcement learning programs that do all this,\\ninstead of us designing the details.\\nDuring the reward function or during the whole program?\\nSo, this would be learning the entire reinforcement learning program.\\nSo, it would be, imagine,\\nyou have a reinforcement learning program, whatever it is,\\nand you throw it out some problem and then you see how long it takes to learn.\\nAnd then you say, well, that took a while.\\nNow, let another program modify this reinforcement learning program.\\nAfter the modification, see how fast it learns.\\nIf it learns more quickly,\\nthat was a good modification and maybe keep it and improve from there.\\nWell, I see, right. Yes, and pace the direction.\\nI think it has a lot to do with, maybe,\\nthe amount of compute that's becoming available.\\nSo, this would be running reinforcement learning in the inner loop.\\nFor us right now, we run reinforcement learning as the final thing.\\nAnd so, the more compute we get,\\nthe more it becomes possible to maybe run something\\nlike reinforcement learning in the inner loop of a bigger algorithm.\\nStarting from the 14-year-old,\\nyou've worked in AI for some 20 plus years now.\\nSo, tell me a bit about how your understanding of AI has evolved over this time.\\nWhen I started looking at AI,\\nit's very interesting because it really\\ncoincided with coming to Stanford to do my master's degree there,\\nand there were some icons there like John McCarthy who I got to talk with,\\nbut who had a very different approach to,\\nand in the year 2000,\\nfor what most people were doing at the time.\\nAnd also talking with Daphne Koller.\\nAnd I think a lot of my initial thinking of AI was shaped by Daphne's thinking.\\nHer AI class, her probabilistic graphical models class,\\nand kind of really being intrigued by\\nhow simply a distribution of her many random variables and then being able to condition\\non some subsets variables and draw on conclusions about others could\\nactually give you so much if you can somehow make it computationally attractable,\\nwhich was definitely the challenge to make it computable.\\nAnd then from there,\\nwhen I started my Ph.D. And you arrived at Stanford,\\nand I think you give me a really good reality check,\\nthat that's not the right metric to evaluate your work by,\\nand to really try to see the connection from what\\nyou're working on to what impact they can really have,\\nwhat change it can make rather than what's the math that happened to be in your work.\\nRight. That's amazing.\\nI did not realize, I've forgotten that.\\nYes, it's actually one of the things, aside most often that people asking,\\nif you going to cite only one thing that has stuck with you from Andrew's advice,\\nit's making sure you can see the connection to where it's actually going to do something.\\nYou've had and you're continuing to have an amazing career in AI.\\nSo, for some of the people listening to you on video now,\\nif they want to also enter or pursue a career in AI,\\nwhat advice do you have for them?\\nI think it's a really good time to get into artificial intelligence.\\nIf you look at the demand for people, it's so high,\\nthere is so many job opportunities,\\nso many things you can do, researchwise,\\nbuild new companies and so forth.\\nSo, I'd say yes, it's definitely a smart decision in terms of actually getting going.\\nA lot of it, you can self-study,\\nwhether you're in school or not.\\nThere is a lot of online courses, for instance,\\nyour machine learning course,\\nthere is also, for example,\\nAndrej Karpathy's deep learning course which has videos online,\\nwhich is a great way to get started,\\nBerkeley who has a deep reinforcement learning course\\nwhich has all of the lectures online.\\nSo, those are all good places to get started.\\nI think a big part of what's important is to make sure you try things yourself.\\nSo, not just read things or watch videos but try things out.\\nWith frameworks like TensorFlow,\\nChainer, Theano, PyTorch and so forth,\\nI mean whatever is your favorite,\\nit's very easy to get going and get something up and running very quickly.\\nTo get to practice yourself, right?\\nWith implementing and seeing what does and seeing what doesn't work.\\nSo, this past week there was an article in\\nMashable about a 16-year-old in United Kingdom,\\nwho is one of the leaders on Kaggle competitions.\\nAnd it just said,\\nhe just went out and learned things,\\nfound things online, learned everything himself and\\nnever actually took any formal course per se.\\nAnd there is a 16-year-old just being very competitive in Kaggle competition,\\nso it's definitely possible.\\nWe live in good times.\\nIf people want to learn.\\nAbsolutely.\\nOne question I bet you get all sometimes\\nis if someone wants to enter AI machine learning and deep learning,\\nshould they apply for a Ph.D. program or should they get the job with a big company?\\nI think a lot of it has to do with maybe how much mentoring you can get.\\nSo, in a Ph.D. program,\\nyou're such a guaranteed,\\nthe job of the professor,\\nwho is your adviser,\\nis to look out for you.\\nTry to do everything they can to,\\nkind of, shape you,\\nhelp you become stronger at whatever you want to do, for example, AI.\\nAnd so, there is a very clear dedicated person, sometimes you have two advisers.\\nAnd that's literally their job and that's why they are professors,\\nmost of what they like about being professors often is helping\\nshape students to become more capable at things.\\nNow, it doesn't mean it's not possible at companies,\\nand many companies have really good mentors and have people who love\\nto help educate people who come in and strengthen them, and so forth.\\nIt's just, it might not be as much of a guarantee and a given,\\ncompared to actually enrolling in a Ph.D. program or that's the crooks of\\nthe program is that you're going to learn and somebody is there to help you learn.\\nSo it really depends on the company and depends on the Ph.D. program.\\nAbsolutely, yes. But I think it is key that you can learn a lot on your own.\\nBut I think you can learn a lot faster if you have somebody who's more experienced,\\nwho is actually taking it up as\\ntheir responsibility to spend time with you and help accelerate your progress.\\nSo, you've been one of the most visible leaders in deep reinforcement learning.\\nSo, what are the things that\\ndeep reinforcement learning is already working really well at?\\nI think, if you look at some deep reinforcement learning successes,\\nit's very, very intriguing.\\nFor example, learning to play Atari games from pixels,\\nprocessing this pixels which is just numbers that are being\\nprocessed somehow and turned into joystick actions.\\nThen, for example, some of the work we did at Berkeley was,\\nwe have a simulated robot inventing walking and the reward\\nthat it's given is as simple as the further you go north the\\nbetter and the less hard you impact with the ground the better.\\nAnd somehow it decides that walking slash running is the thing to invent whereas,\\nnobody showed it, what walking is or running is.\\nOr robot playing with children's stories and learn to kind of put them together,\\nput a block into matching opening, and so forth.\\nAnd so, I think it's really interesting that in all of these it's possible to learn\\nfrom raw sensory inputs all the way to raw controls,\\nfor example, torques at the motors.\\nBut at the same time.\\nSo it is very interesting that you can have a single algorithm.\\nFor example, you know thrust is impulsive and you can learn,\\ncan have a robot learn to run,\\ncan have a robot learn to stand up,\\ncan have instead of a two legged robot,\\nnow you're swapping a four legged robot.\\nYou run the same reinforcement algorithm and it still learns to run.\\nAnd so, there is no change in the reinforcement algorithm.\\nIt's very, very general. Same for the Atari games.\\nDQN was the same DQN for every one of the games.\\nBut then, when it actually starts hitting\\nthe frontiers of what's not yet possible as well,\\nit's nice it learns from scratch for each one of\\nthese tasks but would be even nicer if it could reuse things it's learned in the past;\\nto learn even more quickly for the next task.\\nAnd that's something that's still on the frontier and not yet possible.\\nIt always starts from scratch, essentially.\\nHow quickly, do you think, you see deep\\nreinforcement learning get deployed in the robots around us,\\nthe robots they're getting deployed in the world today.\\nI think in practice the realistic scenario is one\\nwhere it starts with supervised learning,\\nbehavioral cloning; humans do the work.\\nAnd I think a lot of businesses will be built\\nthat way where it's a human behind the scenes doing a lot of the work.\\nImagine Facebook Messenger assistant.\\nAssistant like that could be built with a human behind\\nthe curtains doing a lot of the work; machine learning,\\nmatches up with what the human does and starts making suggestions to\\nhuman so the humans has a small number of options that we can just click and select.\\nAnd then over time,\\nas it gets pretty good,\\nyou're starting fusing some reinforcement learning where you give it actual objectives,\\nnot just matching the human behind the curtains\\nbut giving objectives of achievement like,\\nmaybe, how fast were these two people able to plan their meeting?\\nOr how fast were they able to book their flight?\\nOr things like that. How long did it take?\\nHow happy were they with it?\\nBut it would probably have to be bootstrap of a lot of\\nbehavioral cloning of humans showing how this could be done.\\nSo it sounds behavioral cloning just supervise learning to\\nmimic whatever the person is doing and then gradually later on,\\nthe reinforcement learning to have it think about longer time horizons?\\nIs that a fair summary?\\nI'd say so, yes.\\nJust because straight up reinforcement learning from scratch is really fun to watch.\\nIt's super intriguing and very few things more fun to watch\\nthan a reinforcement learning robot starting from nothing and inventing things.\\nBut it's just time consuming and it's not always safe.\\nThank you very much. That was fascinating.\\nI'm really glad we had the chance to chat.\\nWell, Andrew thank you for having me. Very much appreciate it.\"}],\n",
       "   'generated_content': Code(topic='Neural Network Basics: Logistic Regression and Gradient Descent', cell=[CellFormat(cell_type='markdown', cell_no=1, cell_content='# Chapter 2: Neural Network Basics'), CellFormat(cell_type='markdown', cell_no=2, cell_content='Welcome to the second chapter of this course, where we will dive into the fundamental concepts of neural network programming. This chapter focuses on understanding logistic regression, a key algorithm for binary classification, and how to implement it efficiently using gradient descent and vectorization techniques.'), CellFormat(cell_type='markdown', cell_no=3, cell_content='## 2.1 Binary Classification'), CellFormat(cell_type='markdown', cell_no=4, cell_content=\"Binary classification is a problem where the goal is to classify an input into one of two categories, typically labeled as 0 or 1. A common example is image recognition, such as determining if an image contains a 'cat' (output 1) or 'not-cat' (output 0).\"), CellFormat(cell_type='markdown', cell_no=5, cell_content='### Image Representation'), CellFormat(cell_type='markdown', cell_no=6, cell_content=\"To feed an image into a machine learning model, it must be represented numerically. Images are typically stored as three separate matrices for Red, Green, and Blue (RGB) color channels. For a 64x64 pixel image, you would have three 64x64 matrices. To convert this into a single feature vector \\\\( x \\\\), we 'unroll' all these pixel intensity values into a long column vector. If an image is 64x64 pixels with 3 color channels, the dimension of \\\\( x \\\\) will be \\\\( 64 \\\\times 64 \\\\times 3 = 12288 \\\\). We denote this dimension as \\\\( n_x \\\\).\"), CellFormat(cell_type='markdown', cell_no=7, cell_content='### Notation for Training Examples'), CellFormat(cell_type='markdown', cell_no=8, cell_content=\"Throughout this course, we'll use consistent notation:\"), CellFormat(cell_type='markdown', cell_no=9, cell_content='- A single training example: \\\\( (x^{(i)}, y^{(i)}) \\\\), where \\\\( x^{(i)} \\\\) is an \\\\( n_x \\\\)-dimensional feature vector and \\\\( y^{(i)} \\\\) is the label (0 or 1).\\n- The entire training set: \\\\( m \\\\) training examples, \\\\( \\\\{(x^{(1)}, y^{(1)}), \\\\dots, (x^{(m)}, y^{(m)})\\\\} \\\\).\\n- \\\\( m \\\\) denotes the number of training examples.\\n- \\\\( X \\\\): A matrix formed by stacking all \\\\( m \\\\) training input vectors \\\\( x^{(i)} \\\\) as columns.\\n  - \\\\( X = [x^{(1)} | x^{(2)} | \\\\dots | x^{(m)}] \\\\)\\n  - Shape of \\\\( X \\\\): \\\\( (n_x, m) \\\\).\\n- \\\\( Y \\\\): A matrix formed by stacking all \\\\( m \\\\) training output labels \\\\( y^{(i)} \\\\) as columns.\\n  - \\\\( Y = [y^{(1)} | y^{(2)} | \\\\dots | y^{(m)}] \\\\)\\n  - Shape of \\\\( Y \\\\): \\\\( (1, m) \\\\).\\n\\nThis convention of stacking training examples in columns makes vectorization much easier in implementation.'), CellFormat(cell_type='markdown', cell_no=10, cell_content='## 2.2 Logistic Regression Model'), CellFormat(cell_type='markdown', cell_no=11, cell_content='Logistic regression is an algorithm specifically designed for binary classification problems where the output labels are either 0 or 1. The goal is to output a prediction \\\\( \\\\hat{y} \\\\) which estimates the probability that \\\\( y=1 \\\\) given the input \\\\( x \\\\).'), CellFormat(cell_type='markdown', cell_no=12, cell_content='### Parameters and Output'), CellFormat(cell_type='markdown', cell_no=13, cell_content='The parameters of logistic regression are:\\n- \\\\( w \\\\): An \\\\( n_x \\\\)-dimensional vector (weights).\\n- \\\\( b \\\\): A real number (bias).\\n\\nThe output \\\\( \\\\hat{y} \\\\) is generated using the sigmoid function:\\n\\n1. Compute \\\\( z = w^T x + b \\\\)\\n2. Compute \\\\( \\\\hat{y} = \\\\sigma(z) = \\\\frac{1}{1 + e^{-z}} \\\\)\\n\\nThe sigmoid function maps any real number \\\\( z \\\\) to a value between 0 and 1, which is ideal for representing probabilities.'), CellFormat(cell_type='markdown', cell_no=14, cell_content='### Sigmoid Function Visualization'), CellFormat(cell_type='code', cell_no=15, cell_content=\"import numpy as np\\nimport matplotlib.pyplot as plt\\n\\ndef sigmoid(z):\\n    return 1 / (1 + np.exp(-z))\\n\\nz = np.linspace(-10, 10, 100)\\nsigma_z = sigmoid(z)\\n\\nplt.figure(figsize=(8, 4))\\nplt.plot(z, sigma_z)\\nplt.xlabel('z')\\nplt.ylabel('sigmoid(z)')\\nplt.title('Sigmoid Function')\\nplt.grid(True)\\nplt.axvline(0, color='gray', linestyle='--')\\nplt.axhline(0.5, color='gray', linestyle='--')\\nplt.axhline(0, color='gray', linestyle=':')\\nplt.axhline(1, color='gray', linestyle=':')\\nplt.text(0.5, 0.55, '(0, 0.5)', ha='left', va='bottom')\\nplt.show()\"), CellFormat(cell_type='markdown', cell_no=16, cell_content='## 2.3 Logistic Regression Cost Function'), CellFormat(cell_type='markdown', cell_no=17, cell_content='To train the parameters \\\\( w \\\\) and \\\\( b \\\\), we need a function that measures how well our model is performing. This is achieved through a **Loss Function** and a **Cost Function**.'), CellFormat(cell_type='markdown', cell_no=18, cell_content='### Loss Function for a Single Training Example'), CellFormat(cell_type='markdown', cell_no=19, cell_content='For a single training example \\\\( (x^{(i)}, y^{(i)}) \\\\), with prediction \\\\( \\\\hat{y}^{(i)} \\\\), the loss function \\\\( L(\\\\hat{y}^{(i)}, y^{(i)}) \\\\) is defined as:\\n\\n$$L(\\\\hat{y}^{(i)}, y^{(i)}) = -\\\\left(y^{(i)} \\\\log(\\\\hat{y}^{(i)}) + (1 - y^{(i)}) \\\\log(1 - \\\\hat{y}^{(i)})\\\\right)$$\\n\\nThis specific loss function is chosen because it leads to a convex optimization problem, making it easier for gradient descent to find the global optimum. Intuitively:\\n- If \\\\( y^{(i)} = 1 \\\\), we want \\\\( \\\\hat{y}^{(i)} \\\\) to be as close to 1 as possible.\\n- If \\\\( y^{(i)} = 0 \\\\), we want \\\\( \\\\hat{y}^{(i)} \\\\) to be as close to 0 as possible.'), CellFormat(cell_type='markdown', cell_no=20, cell_content='### Cost Function for the Entire Training Set'), CellFormat(cell_type='markdown', cell_no=21, cell_content='The **Cost Function** \\\\( J(w, b) \\\\) measures the overall performance of our parameters \\\\( w \\\\) and \\\\( b \\\\) on the entire training set. It is the average of the loss functions over all \\\\( m \\\\) training examples:\\n\\n$$J(w, b) = \\\\frac{1}{m} \\\\sum_{i=1}^{m} L(\\\\hat{y}^{(i)}, y^{(i)})$$\\n\\nOur goal in training is to find the parameters \\\\( w \\\\) and \\\\( b \\\\) that minimize this cost function \\\\( J(w, b) \\\\).'), CellFormat(cell_type='markdown', cell_no=22, cell_content='## 2.4 Gradient Descent'), CellFormat(cell_type='markdown', cell_no=23, cell_content='Gradient descent is an iterative optimization algorithm used to find the parameters \\\\( w \\\\) and \\\\( b \\\\) that minimize the cost function \\\\( J(w, b) \\\\). Since \\\\( J(w, b) \\\\) for logistic regression is a convex function, gradient descent will converge to the global minimum.'), CellFormat(cell_type='markdown', cell_no=24, cell_content='### Algorithm Steps'), CellFormat(cell_type='markdown', cell_no=25, cell_content='1. **Initialize** \\\\( w \\\\) and \\\\( b \\\\) to some initial values (often zeros for logistic regression).\\n2. **Repeatedly update** \\\\( w \\\\) and \\\\( b \\\\) in the direction opposite to the gradient of the cost function:\\n   $$w := w - \\\\alpha \\\\frac{\\\\partial J(w,b)}{\\\\partial w}$$\\n   $$b := b - \\\\alpha \\\\frac{\\\\partial J(w,b)}{\\\\partial b}$$\\n   Where:\\n   - \\\\( \\\\alpha \\\\) is the **learning rate**, controlling the size of the step.\\n   - \\\\( \\\\frac{\\\\partial J}{\\\\partial w} \\\\) and \\\\( \\\\frac{\\\\partial J}{\\\\partial b} \\\\) are the **partial derivatives** (gradients) of the cost function with respect to \\\\( w \\\\) and \\\\( b \\\\) respectively.'), CellFormat(cell_type='markdown', cell_no=26, cell_content='### Intuition Behind Derivatives'), CellFormat(cell_type='markdown', cell_no=27, cell_content=\"A derivative (or partial derivative in multi-variable functions) intuitively represents the *slope* of a function at a particular point. By moving in the direction opposite to the slope, we move 'downhill' on the cost function surface towards its minimum.\\n\\nFor example, if \\\\( f(a) = 3a \\\\), the derivative \\\\( \\\\frac{df(a)}{da} = 3 \\\\). This means if we increase \\\\( a \\\\) by a tiny amount (e.g., 0.001), \\\\( f(a) \\\\) increases by \\\\( 3 \\\\times 0.001 \\\\). For \\\\( f(a) = a^2 \\\\), the derivative \\\\( \\\\frac{df(a)}{da} = 2a \\\\), indicating that the slope changes depending on the value of \\\\( a \\\\).\"), CellFormat(cell_type='markdown', cell_no=28, cell_content='## 2.5 Computation Graph and Derivatives'), CellFormat(cell_type='markdown', cell_no=29, cell_content='Neural network computations are often organized into a **forward pass** (computing output) and a **backward pass** (computing gradients). A computation graph visually represents these steps, making the derivative calculations (backpropagation) more systematic.'), CellFormat(cell_type='markdown', cell_no=30, cell_content='### Example: \\\\( J = 3(a + bc) \\\\)'), CellFormat(cell_type='markdown', cell_no=31, cell_content=\"Let's break down the computation of \\\\( J \\\\) into intermediate steps:\\n1. \\\\( u = bc \\\\)\\n2. \\\\( v = a + u \\\\)\\n3. \\\\( J = 3v \\\\)\\n\\n### Forward Pass (Left-to-Right)\"), CellFormat(cell_type='markdown', cell_no=32, cell_content='If \\\\( a=5, b=3, c=2 \\\\):\\n- \\\\( u = 3 \\\\times 2 = 6 \\\\)\\n- \\\\( v = 5 + 6 = 11 \\\\)\\n- \\\\( J = 3 \\\\times 11 = 33 \\\\)'), CellFormat(cell_type='markdown', cell_no=33, cell_content='### Backward Pass (Right-to-Left for Gradients)'), CellFormat(cell_type='markdown', cell_no=34, cell_content='Using the chain rule, we compute derivatives by going backward through the graph:\\n- \\\\( \\\\frac{dJ}{dv} = 3 \\\\) (from \\\\( J = 3v \\\\))\\n- \\\\( \\\\frac{dJ}{du} = \\\\frac{dJ}{dv} \\\\times \\\\frac{dv}{du} \\\\)\\n  - \\\\( v = a + u \\\\) implies \\\\( \\\\frac{dv}{du} = 1 \\\\)\\n  - So, \\\\( \\\\frac{dJ}{du} = 3 \\\\times 1 = 3 \\\\)\\n- \\\\( \\\\frac{dJ}{da} = \\\\frac{dJ}{dv} \\\\times \\\\frac{dv}{da} \\\\)\\n  - \\\\( v = a + u \\\\) implies \\\\( \\\\frac{dv}{da} = 1 \\\\)\\n  - So, \\\\( \\\\frac{dJ}{da} = 3 \\\\times 1 = 3 \\\\)\\n- \\\\( \\\\frac{dJ}{db} = \\\\frac{dJ}{du} \\\\times \\\\frac{du}{db} \\\\)\\n  - \\\\( u = bc \\\\) implies \\\\( \\\\frac{du}{db} = c \\\\)\\n  - So, \\\\( \\\\frac{dJ}{db} = 3 \\\\times c = 3 \\\\times 2 = 6 \\\\)\\n- \\\\( \\\\frac{dJ}{dc} = \\\\frac{dJ}{du} \\\\times \\\\frac{du}{dc} \\\\)\\n  - \\\\( u = bc \\\\) implies \\\\( \\\\frac{du}{dc} = b \\\\)\\n  - So, \\\\( \\\\frac{dJ}{dc} = 3 \\\\times b = 3 \\\\times 3 = 9 \\\\)'), CellFormat(cell_type='markdown', cell_no=35, cell_content='In code, we often denote \\\\( \\\\frac{dJ}{d_{var}} \\\\) as `d_var` (e.g., `dv` for \\\\( \\\\frac{dJ}{dv} \\\\)).'), CellFormat(cell_type='markdown', cell_no=36, cell_content='### Applying to Logistic Regression (Single Example)'), CellFormat(cell_type='markdown', cell_no=37, cell_content='For a single training example \\\\( (x, y) \\\\):\\n\\n**Forward Propagation:**\\n1. \\\\( z = w^T x + b \\\\)\\n2. \\\\( a = \\\\sigma(z) \\\\)\\n3. \\\\( L(a, y) = -\\\\left(y \\\\log(a) + (1 - y) \\\\log(1 - a)\\\\right) \\\\)\\n\\n**Backward Propagation:**\\n1. \\\\( da = \\\\frac{\\\\partial L}{\\\\partial a} = -\\\\frac{y}{a} + \\\\frac{1-y}{1-a} \\\\)\\n2. \\\\( dz = \\\\frac{\\\\partial L}{\\\\partial z} = a - y \\\\)\\n   (This derivative is derived from \\\\( \\\\frac{\\\\partial L}{\\\\partial a} \\\\times \\\\frac{\\\\partial a}{\\\\partial z} \\\\), where \\\\( \\\\frac{\\\\partial a}{\\\\partial z} = a(1-a) \\\\))\\n3. \\\\( dw = \\\\frac{\\\\partial L}{\\\\partial w} = x \\\\cdot dz \\\\)\\n4. \\\\( db = \\\\frac{\\\\partial L}{\\\\partial b} = dz \\\\)\\n\\n**Parameter Update (for a single example):**\\n$$w := w - \\\\alpha \\\\cdot dw$$\\n$$b := b - \\\\alpha \\\\cdot db$$'), CellFormat(cell_type='markdown', cell_no=38, cell_content='## 2.6 Gradient Descent on M Examples (Non-Vectorized)'), CellFormat(cell_type='markdown', cell_no=39, cell_content='To perform gradient descent on the entire training set of \\\\( m \\\\) examples, we need to compute the average of the gradients for each example.'), CellFormat(cell_type='markdown', cell_no=40, cell_content='### Algorithm Outline (for one iteration):'), CellFormat(cell_type='code', cell_no=41, cell_content=\"W, B = initialize_parameters()\\nalpha = learning_rate\\n\\nJ_total = 0\\ndw_total = np.zeros_like(W)\\ndb_total = 0\\n\\nfor i in range(m): # Loop over each training example\\n    # Forward Propagation for example i\\n    z_i = np.dot(W.T, X[:, i]) + B\\n    a_i = sigmoid(z_i)\\n    \\n    # Calculate loss and accumulate\\n    J_total += -(Y[0, i] * np.log(a_i) + (1 - Y[0, i]) * np.log(1 - a_i))\\n    \\n    # Backward Propagation for example i\\n    dz_i = a_i - Y[0, i]\\n    dw_total += X[:, i].reshape(W.shape) * dz_i # Assuming X[:, i] is n_x dimensional\\n    db_total += dz_i\\n\\n# Average the cost and gradients\\nJ = J_total / m\\ndW = dw_total / m\\ndB = db_total / m\\n\\n# Update parameters\\nW = W - alpha * dW\\nB = B - alpha * dB\\n\\n# This entire block is repeated for 'num_iterations' times\"), CellFormat(cell_type='markdown', cell_no=42, cell_content='This implementation involves explicit `for` loops over `m` training examples and potentially another `for` loop over `n_x` features (if `W` is updated element-wise). For large datasets, this approach is highly inefficient.'), CellFormat(cell_type='markdown', cell_no=43, cell_content='## 2.7 Vectorization'), CellFormat(cell_type='markdown', cell_no=44, cell_content='Vectorization is the process of replacing explicit `for` loops with optimized array operations, typically provided by libraries like NumPy. This significantly speeds up computation by leveraging parallelism in modern CPUs and GPUs (SIMD instructions).'), CellFormat(cell_type='markdown', cell_no=45, cell_content='### Example: Dot Product'), CellFormat(cell_type='code', cell_no=46, cell_content='import time\\n\\n# Non-vectorized version\\nW_nv = np.random.rand(1000000)\\nX_nv = np.random.rand(1000000)\\n\\ntic = time.time()\\nZ_nv = 0\\nfor i in range(1000000):\\n    Z_nv += W_nv[i] * X_nv[i]\\nZ_nv += B # Assuming B is a scalar\\ntoc = time.time()\\nprint(f\"Non-vectorized: {1000*(toc-tic):.3f} ms\")\\n\\n# Vectorized version\\nW_v = np.random.rand(1000000)\\nX_v = np.random.rand(1000000)\\nB = np.random.rand() # Scalar bias\\n\\ntic = time.time()\\nZ_v = np.dot(W_v, X_v) + B\\ntoc = time.time()\\nprint(f\"Vectorized: {1000*(toc-tic):.3f} ms\")'), CellFormat(cell_type='markdown', cell_no=47, cell_content='As seen from the demo, vectorized operations can be orders of magnitude faster. The rule of thumb in deep learning is: **avoid explicit for-loops whenever possible.**'), CellFormat(cell_type='markdown', cell_no=48, cell_content='### More Vectorization Examples'), CellFormat(cell_type='markdown', cell_no=49, cell_content='NumPy provides many functions that operate element-wise on vectors or matrices without explicit loops:\\n- `np.exp(v)`: Element-wise exponential.\\n- `np.log(v)`: Element-wise natural logarithm.\\n- `np.abs(v)`: Element-wise absolute value.\\n- `np.maximum(v, 0)`: Element-wise maximum.\\n- `v**2`: Element-wise square.\\n- `1/v`: Element-wise inverse.\\n\\nMatrix multiplications like `np.dot(A, v)` (or `A @ v` in Python 3.5+) are also highly optimized.'), CellFormat(cell_type='markdown', cell_no=50, cell_content='## 2.8 Vectorizing Logistic Regression'), CellFormat(cell_type='markdown', cell_no=51, cell_content='We can vectorize both the forward and backward propagation steps across all `m` training examples, eliminating all explicit `for` loops (except for the outer loop for gradient descent iterations).'), CellFormat(cell_type='markdown', cell_no=52, cell_content='### Vectorized Forward Propagation (for all `m` examples)'), CellFormat(cell_type='markdown', cell_no=53, cell_content='Recall \\\\( X \\\\) has shape \\\\( (n_x, m) \\\\) and \\\\( W \\\\) has shape \\\\( (n_x, 1) \\\\).'), CellFormat(cell_type='markdown', cell_no=54, cell_content=\"1. Compute \\\\( Z = W^T X + b \\\\)\\n   - \\\\( W^T \\\\) will be \\\\( (1, n_x) \\\\).\\n   - \\\\( W^T X \\\\) will be \\\\( (1, m) \\\\), containing \\\\( z^{(1)}, z^{(2)}, \\\\dots, z^{(m)} \\\\).\\n   - Adding scalar \\\\( b \\\\) to a \\\\( (1,m) \\\\) vector: Python's **broadcasting** feature automatically expands \\\\( b \\\\) into a \\\\( (1,m) \\\\) vector of \\\\( b \\\\)'s and adds element-wise.\\n   - In NumPy: `Z = np.dot(W.T, X) + b` (where `Z` is `(1, m)` matrix).\\n\\n2. Compute \\\\( A = \\\\sigma(Z) \\\\)\\n   - Apply the sigmoid function element-wise to the matrix \\\\( Z \\\\).\\n   - In NumPy: `A = sigmoid(Z)` (where `A` is `(1, m)` matrix, containing \\\\( a^{(1)}, a^{(2)}, \\\\dots, a^{(m)} \\\\)).\"), CellFormat(cell_type='markdown', cell_no=55, cell_content='### Vectorized Backward Propagation (for all `m` examples)'), CellFormat(cell_type='markdown', cell_no=56, cell_content='Recall \\\\( A \\\\) and \\\\( Y \\\\) both have shape \\\\( (1, m) \\\\).'), CellFormat(cell_type='markdown', cell_no=57, cell_content='1. Compute \\\\( dZ = A - Y \\\\)\\n   - This is an element-wise subtraction of \\\\( (1, m) \\\\) matrices.\\n   - In NumPy: `dZ = A - Y` (where `dZ` is `(1, m)` matrix, containing \\\\( dz^{(1)}, dz^{(2)}, \\\\dots, dz^{(m)} \\\\)).\\n\\n2. Compute \\\\( dW = \\\\frac{1}{m} X dZ^T \\\\)\\n   - \\\\( X \\\\) is \\\\( (n_x, m) \\\\).\\n   - \\\\( dZ^T \\\\) is \\\\( (m, 1) \\\\).\\n   - The product \\\\( X dZ^T \\\\) is \\\\( (n_x, 1) \\\\), representing the sum \\\\( \\\\sum_{i=1}^{m} x^{(i)} dz^{(i)} \\\\).\\n   - In NumPy: `dW = (1/m) * np.dot(X, dZ.T)` (where `dW` is `(n_x, 1)` vector).\\n\\n3. Compute \\\\( db = \\\\frac{1}{m} \\\\sum_{i=1}^{m} dz^{(i)} \\\\)\\n   - This is the average of all elements in \\\\( dZ \\\\).\\n   - In NumPy: `db = (1/m) * np.sum(dZ)` (where `db` is a scalar).'), CellFormat(cell_type='markdown', cell_no=58, cell_content='### Full Vectorized Logistic Regression (One Iteration)'), CellFormat(cell_type='code', cell_no=59, cell_content=\"import numpy as np\\n\\ndef sigmoid(z):\\n    return 1 / (1 + np.exp(-z))\\n\\n# Assume W, b, X, Y, alpha, m are already defined:\\n# W: (n_x, 1) array\\n# b: scalar\\n# X: (n_x, m) array\\n# Y: (1, m) array\\n# alpha: scalar learning rate\\n# m: number of examples\\n\\n# --- Forward Propagation ---\\nZ = np.dot(W.T, X) + b  # (1, n_x) @ (n_x, m) + scalar -> (1, m)\\nA = sigmoid(Z)          # (1, m) array\\n\\n# --- Compute Cost ---\\n# Cost = -(1/m) * np.sum(Y * np.log(A) + (1 - Y) * np.log(1 - A))\\n# Note: This is an intermediate step, not part of the core gradient descent updates\\n\\n# --- Backward Propagation ---\\ndZ = A - Y                                 # (1, m) array\\ndW = (1/m) * np.dot(X, dZ.T)               # (n_x, m) @ (m, 1) -> (n_x, 1) array\\ndb = (1/m) * np.sum(dZ)                    # Scalar\\n\\n# --- Update Parameters ---\\nW = W - alpha * dW\\nb = b - alpha * db\\n\\n# This entire block is repeated for 'num_iterations' times in an outer loop.\"), CellFormat(cell_type='markdown', cell_no=60, cell_content='## 2.9 Broadcasting in Python'), CellFormat(cell_type='markdown', cell_no=61, cell_content=\"Broadcasting is a powerful feature in NumPy that allows operations between arrays of different shapes, by automatically 'stretching' the smaller array to match the larger array's shape, under certain conditions.\"), CellFormat(cell_type='markdown', cell_no=62, cell_content='### Broadcasting Rules'), CellFormat(cell_type='markdown', cell_no=63, cell_content=\"- **Scalar to Vector/Matrix**: A scalar (e.g., `b`) can be added to a vector or matrix. The scalar is effectively copied to match the shape of the larger array.\\n  - Example: `np.array([1, 2, 3, 4]) + 100` results in `[101, 102, 103, 104]`.\\n- **` (m, n) ` matrix op ` (1, n) ` row vector**: The row vector is copied `m` times vertically to match the ` (m, n) ` shape.\\n- **` (m, n) ` matrix op ` (m, 1) ` column vector**: The column vector is copied `n` times horizontally to match the ` (m, n) ` shape.\\n\\nBroadcasting works for element-wise operations like addition, subtraction, multiplication, and division. It's crucial for writing concise and efficient vectorized code.\"), CellFormat(cell_type='markdown', cell_no=64, cell_content='## 2.10 A Note on Python/NumPy Vectors'), CellFormat(cell_type='markdown', cell_no=65, cell_content=\"While NumPy's flexibility is powerful, it can lead to subtle bugs, especially with **rank 1 arrays** (e.g., `np.random.randn(5)` results in a shape of `(5,)`). These arrays don't consistently behave as either row or column vectors.\"), CellFormat(cell_type='markdown', cell_no=66, cell_content='### Best Practices to Avoid Bugs'), CellFormat(cell_type='markdown', cell_no=67, cell_content='1. **Avoid Rank 1 Arrays**: Always explicitly define vectors as either column vectors `(n, 1)` or row vectors `(1, n)`.\\n   - Column vector: `a = np.random.randn(5, 1)` (shape `(5, 1)`)\\n   - Row vector: `a = np.random.randn(1, 5)` (shape `(1, 5)`)\\n2. **Use Assertion Statements**: Include `assert(a.shape == (5, 1))` to double-check dimensions in your code. This helps catch dimension mismatches early.\\n3. **Reshape if Necessary**: If you encounter a rank 1 array, use `.reshape()` to convert it to the desired `(n, 1)` or `(1, n)` shape (e.g., `a = a.reshape(5, 1)`).\\n\\nAdhering to these practices will make your code more robust and easier to debug.')])},\n",
       "  {'chapter_number': '3',\n",
       "   'chapter_name': 'Shallow_neural_networks',\n",
       "   'lessons': [{'lesson_number': '01',\n",
       "     'lesson_name': 'neural-networks-overview',\n",
       "     'content': \"Welcome back. In this week,\\nyou learned to implement a neural network.\\nBefore diving into the technical details,\\nI want in this video,\\nto give you a quick overview of what you'll be seeing in this week's videos.\\nSo, if you don't follow all the details in this video,\\ndon't worry about it, we'll delve into the technical details in the next few videos.\\nBut for now, let's give a quick overview of how you implement a neural network.\\nLast week, we had talked about logistic regression,\\nand we saw how this model corresponds to the following computation draft,\\nwhere you then put the features x and parameters\\nw and b that allows you to compute z which is then used to computes a,\\nand we were using a interchangeably with\\nthis output y hat and then you can compute the loss function,\\nL. A neural network looks like this.\\nAs I'd already previously alluded,\\nyou can form a neural network by stacking together a lot of little sigmoid units.\\nWhereas previously, this node corresponds to two steps to calculations.\\nThe first is compute the z-value,\\nsecond is it computes this a value.\\nIn this neural network,\\nthis stack of notes will correspond to a z-like calculation like this,\\nas well as, an a-like calculation like that.\\nThen, that node will correspond to another z and another a like calculation.\\nSo the notation which we will introduce later will look like this.\\nFirst, we'll inputs the features, x,\\ntogether with some parameters w and b,\\nand this will allow you to compute z one.\\nSo, new notation that we'll introduce is that we'll use\\nsuperscript square bracket one to refer to\\nquantities associated with this stack of nodes, it's called a layer.\\nThen later, we'll use superscript square bracket\\ntwo to refer to quantities associated with that node.\\nThat's called another layer of the neural network.\\nThe superscript square brackets,\\nlike we have here,\\nare not to be confused with\\nthe superscript round brackets which we use to refer to individual training examples.\\nSo, whereas x superscript round bracket I refer to the ith training example,\\nsuperscript square bracket one and two refer to these different layers;\\nlayer one and layer two in this neural network.\\nBut so going on, after computing z_1 similar to logistic regression,\\nthere'll be a computation to compute a_1,\\nand that's just sigmoid of z_1,\\nand then you compute z_2 using another linear equation and then compute a_2.\\nA_2 is the final output\\nof the neural network and will also be used interchangeably with y-hat.\\nSo, I know that was a lot of details but the key intuition to\\ntake away is that whereas for logistic regression,\\nwe had this z followed by a calculation.\\nIn this neural network,\\nhere we just do it multiple times,\\nas a z followed by a calculation,\\nand a z followed by a calculation,\\nand then you finally compute the loss at the end.\\nYou remember that for logistic regression,\\nwe had this backward calculation in order\\nto compute derivatives or as you're computing your d a,\\nd z and so on.\\nSo, in the same way,\\na neural network will end up doing a backward calculation that looks like\\nthis in which you end up computing da_2,\\ndz_2, that allows you to compute dw_2,\\ndb_2, and so on.\\nThis right to left backward calculation that is denoting with the red arrows.\\nSo, that gives you a quick overview of what a neural network looks like.\\nIt's basically taken logistic regression and repeating it twice.\\nI know there was a lot of new notation laws,\\nnew details, don't worry about saving them,\\nfollow everything, we'll go into the details most probably in the next few videos.\\nSo, let's go on to the next video.\\nWe'll start to talk about the neural network representation.\"},\n",
       "    {'lesson_number': '02',\n",
       "     'lesson_name': 'neural-network-representation',\n",
       "     'content': \"You see me draw a few pictures of neural networks.\\nIn this video, we'll talk about exactly what those pictures means.\\nIn other words,\\nexactly what those neural networks that we've been drawing represent.\\nAnd we'll start with focusing on the case of neural networks with\\nwhat was called a single hidden layer.\\nHere's a picture of a neural network.\\nLet's give different parts of these pictures some names.\\nWe have the input features, x1, x2, x3 stacked up vertically.\\nAnd this is called the input layer of the neural network.\\nSo maybe not surprisingly, this contains the inputs to the neural network.\\nThen there's another layer of circles.\\nAnd this is called a hidden layer of the neural network.\\nI'll come back in a second to say what the word hidden means.\\nBut the final layer here is formed by, in this case, just one node.\\nAnd this single-node layer is called the output layer, and is responsible for\\ngenerating the predicted value y hat.\\nIn a neural network that you train with supervised learning,\\nthe training set contains values of the inputs x as well as the target outputs y.\\nSo the term hidden layer refers to the fact that in the training set,\\nthe true values for these nodes in the middle are not observed.\\nThat is, you don't see what they should be in the training set.\\nYou see what the inputs are.\\nYou see what the output should be.\\nBut the things in the hidden layer are not seen in the training set.\\nSo that kind of explains the name hidden layer; just because you\\ndon't see it in the training set.\\nLet's introduce a bit more notation.\\nWhereas previously, we were using the vector X to denote the input features and\\nalternative notation for\\nthe values of the input features will be A superscript square bracket 0.\\nAnd the term A also stands for activations, and\\nit refers to the values that different layers\\nof the neural network are passing on to the subsequent layers.\\nSo the input layer passes on the value x to the hidden layer, so\\nwe're going to call that activations of the input layer A super script 0.\\nThe next layer, the hidden layer, will in turn generate some set of activations,\\nwhich I'm going to write as A superscript square bracket 1.\\nSo in particular, this first unit or this first node,\\nwe generate a value A superscript square bracket 1 subscript 1.\\nThis second node we generate a value.\\nNow we have a subscript 2 and so on.\\nAnd so, A superscript square bracket 1,\\nthis is a four dimensional vector you want in Python\\nbecause the 4x1 matrix, or a 4 column vector, which looks like this.\\nAnd it's four dimensional, because in this case we have four nodes, or\\nfour units, or four hidden units in this hidden layer.\\nAnd then finally, the open layer regenerates some value A2,\\nwhich is just a real number.\\nAnd so y hat is going to take on the value of A2.\\nSo this is analogous to how in logistic regression we have y hat equals a and\\nin logistic regression which we only had that one output layer, so\\nwe don't use the superscript square brackets.\\nBut with our neural network, we now going to use the superscript square\\nbracket to explicitly indicate which layer it came from.\\nOne funny thing about notational conventions in neural networks\\nis that this network that you've seen here is called a two layer neural network.\\nAnd the reason is that when we count layers in neural networks,\\nwe don't count the input layer.\\nSo the hidden layer is layer one and the output layer is layer two.\\nIn our notational convention, we're calling the input layer layer zero, so\\ntechnically maybe there are three layers in this neural network.\\nBecause there's the input layer, the hidden layer, and the output layer.\\nBut in conventional usage, if you read research papers and elsewhere in\\nthe course, you see people refer to this particular neural network as a two layer\\nneural network, because we don't count the input layer as an official layer.\\nFinally, something that we'll get to later is that the hidden layer and\\nthe output layers will have parameters associated with them.\\nSo the hidden layer will have associated with it parameters w and b.\\nAnd I'm going to write superscripts square bracket 1 to indicate that these\\nare parameters associated with layer one with the hidden layer.\\nWe'll see later that w will be a 4 by 3 matrix and\\nb will be a 4 by 1 vector in this example.\\nWhere the first coordinate four comes from the fact that we have\\nfour nodes of our hidden units and a layer, and\\nthree comes from the fact that we have three input features.\\nWe'll talk later about the dimensions of these matrices.\\nAnd it might make more sense at that time.\\nBut in some of the output layers has associated with it also, parameters w\\nsuperscript square bracket 2 and b superscript square bracket 2.\\nAnd it turns out the dimensions of these are 1 by 4 and 1 by 1.\\nAnd these 1 by 4 is because the hidden layer has four hidden units,\\nthe output layer has just one unit.\\nBut we will go over the dimension of these matrices and vectors in a later video.\\nSo you've just seen what a two layered neural network looks like.\\nThat is a neural network with one hidden layer.\\nIn the next video,\\nlet's go deeper into exactly what this neural network is computing.\\nThat is how this neural network inputs x and\\ngoes all the way to computing its output y hat.\"},\n",
       "    {'lesson_number': '03',\n",
       "     'lesson_name': 'computing-a-neural-networks-output',\n",
       "     'content': \"In the last video, you saw what a single hidden layer neural network looks like.\\nIn this video, let's go through the details of\\nexactly how this neural network computes these outputs.\\nWhat you see is that it is like logistic regression,\\nbut repeated a lot of times.\\nLet's take a look. So, this is how a two-layer neural network looks.\\nLet's go more deeply into exactly what this neural network computes.\\nNow, we've said before that logistic regression,\\nthe circle in logistic regression,\\nreally represents two steps of computation rows.\\nYou compute z as follows, and a second,\\nyou compute the activation as a sigmoid function of z.\\nSo, a neural network just does this a lot more times.\\nLet's start by focusing on just one of the nodes in the hidden layer.\\nLet's look at the first node in the hidden layer.\\nSo, I've grayed out the other nodes for now.\\nSo, similar to logistic regression on the left,\\nthis nodes in the hidden layer does two steps of computation.\\nThe first step and think of as the left half of this node,\\nit computes z equals w transpose x plus b,\\nand the notation we'll use is,\\nthese are all quantities associated with the first hidden layer.\\nSo, that's why we have a bunch of square brackets there.\\nThis is the first node in the hidden layer.\\nSo, that's why we have the subscript one over there.\\nSo first, it does that,\\nand then the second step,\\nis it computes a_[1]_1 equals sigmoid of z_[1]_1, like so.\\nSo, for both z and a,\\nthe notational convention is that a, l, i,\\nthe l here in superscript square brackets,\\nrefers to the layer number,\\nand the i subscript here,\\nrefers to the nodes in that layer.\\nSo, the node we'll be looking at is layer one,\\nthat is a hidden layer node one.\\nSo, that's why the superscripts and subscripts were both one, one.\\nSo, that little circle,\\nthat first node in the neural network,\\nrepresents carrying out these two steps of computation.\\nNow, let's look at the second node in the neural network,\\nor the second node in the hidden layer of the neural network.\\nSimilar to the logistic regression unit on the left,\\nthis little circle represents two steps of computation.\\nThe first step is it computes z.\\nThis is still layer one,\\nbut now as a second node equals w transpose x,\\nplus b_[1]_2, and then a_[1] two equals sigmoid of z_[1]_2.\\nAgain, feel free to pause the video if you want,\\nbut you can double-check that the superscript and\\nsubscript notation is consistent with what we have written here above in purple.\\nSo, we've talked through the first two hidden units in a neural network,\\nhaving units three and four also represents some computations.\\nSo now, let me take this pair of equations,\\nand this pair of equations,\\nand let's copy them to the next slide.\\nSo, here's our neural network,\\nand here's the first,\\nand here's the second equations that we've worked out\\npreviously for the first and the second hidden units.\\nIf you then go through and write out the corresponding equations\\nfor the third and fourth hidden units, you get the following.\\nSo, let me show this notation is clear,\\nthis is the vector w_[1]_1,\\nthis is a vector transpose times x.\\nSo, that's what the superscript T there represents.\\nIt's a vector transpose.\\nNow, as you might have guessed,\\nif you're actually implementing a neural network,\\ndoing this with a for loop, seems really inefficient.\\nSo, what we're going to do,\\nis take these four equations and vectorize.\\nSo, we're going to start by showing how to compute z as a vector,\\nit turns out you could do it as follows.\\nLet me take these w's and stack them into a matrix,\\nthen you have w_[1]_1 transpose,\\nso that's a row vector,\\nor this column vector transpose gives you a row vector, then w_[1]_2,\\ntranspose, w_[1]_3 transpose, w_[1]_4 transpose.\\nSo, by stacking those four w vectors together,\\nyou end up with a matrix.\\nSo, another way to think of this is that we have four logistic regression units there,\\nand each of the logistic regression units,\\nhas a corresponding parameter vector,\\nw. By stacking those four vectors together,\\nyou end up with this four by three matrix.\\nSo, if you then take this matrix and multiply it by your input features x1,\\nx2, x3, you end up with by how matrix multiplication works.\\nYou end up with w_[1]_1 transpose x,\\nw_2_[1] transpose x, w_3_[1] transpose x, w_4_[1] transpose x.\\nThen, let's not figure the b's.\\nSo, we now add to this a vector b_[1]_1 one, b_[1]_2, b_[1]_3, b_[1]_4.\\nSo, that's basically this,\\nthen this is b_[1]_1, b_[1]_2, b_[1]_3, b_[1]_4.\\nSo, you see that each of the four rows of\\nthis outcome correspond exactly to each of these four rows,\\neach of these four quantities that we had above.\\nSo, in other words, we've just shown that this thing is therefore equal to\\nz_[1]_1, z_[1]_2, z_[1]_3, z_[1]_4, as defined here.\\nMaybe not surprisingly, we're going to call this whole thing, the vector z_[1],\\nwhich is taken by stacking up these individuals of z's into a column vector.\\nWhen we're vectorizing, one of the rules of thumb that might help you navigate this,\\nis that while we have different nodes in the layer,\\nwe'll stack them vertically.\\nSo, that's why we have z_[1]_1 through z_[1]_4,\\nthose corresponded to four different nodes in the hidden layer,\\nand so we stacked these four numbers vertically to form the vector z[1].\\nTo use one more piece of notation,\\nthis four by three matrix here which we obtained by stacking the lowercase w_[1]_1,\\nw_[1]_2, and so on, we're going to call this matrix W capital [1].\\nSimilarly, this vector, we're going to call b superscript [1] square bracket.\\nSo, this is a four by one vector.\\nSo now, we've computed z using this vector matrix notation,\\nthe last thing we need to do is also compute these values of a.\\nSo, prior won't surprise you to see that we're going to define a_[1],\\nas just stacking together,\\nthose activation values, a [1],\\n1 through a [1], 4.\\nSo, just take these four values and stack them together in a vector called a[1].\\nThis is going to be a sigmoid of z[1],\\nwhere this now has been implementation of\\nthe sigmoid function that takes in the four elements of z,\\nand applies the sigmoid function element-wise to it.\\nSo, just a recap,\\nwe figured out that z_[1] is equal to w_[1] times the vector x plus the vector b_[1],\\nand a_[1] is sigmoid times z_[1].\\nLet's just copy this to the next slide.\\nWhat we see is that for the first layer of the neural network given an input x,\\nwe have that z_[1] is equal to w_[1] times x plus b_[1],\\nand a_[1] is sigmoid of z_[1].\\nThe dimensions of this are four by one equals,\\nthis was a four by three matrix times a three by one vector plus a four by one vector b,\\nand this is four by one same dimension as end.\\nRemember, that we said x is equal to a_[0].\\nJust say y hat is also equal to a two.\\nIf you want, you can actually take this x and replace it with a_[0],\\nsince a_[0] is if you want as an alias for the vector of input features, x.\\nNow, through a similar derivation,\\nyou can figure out that the representation for the next layer can\\nalso be written similarly where what the output layer does is,\\nit has associated with it,\\nso the parameters w_[2] and b_[2].\\nSo, w_[2] in this case is going to be a one by four matrix,\\nand b_[2] is just a real number as one by on.\\nSo, z_[2] is going to be a real number we'll write as a one by one matrix.\\nIs going to be a one by four thing times a was four by one,\\nplus b_[2] as one by one,\\nso this gives you just a real number.\\nIf you think of this last upper unit as just being\\nanalogous to logistic regression which have parameters w and b,\\nw really plays an analogous role to w_[2] transpose,\\nor w_[2] is really W transpose and b is equal to b_[2].\\nI said we want to cover up the left of this network and ignore all that for now,\\nthen this last upper unit is a lot like logistic regression,\\nexcept that instead of writing the parameters as w and b,\\nwe're writing them as w_[2] and b_[2],\\nwith dimensions one by four and one by one.\\nSo, just a recap.\\nFor logistic regression, to implement the output or to implement prediction,\\nyou compute z equals w transpose x plus b,\\nand a or y hat equals a,\\nequals sigmoid of z.\\nWhen you have a neural network with one hidden layer,\\nwhat you need to implement,\\nis to computer this output is just these four equations.\\nYou can think of this as a vectorized implementation of computing\\nthe output of first these for logistic regression units in the hidden layer,\\nthat's what this does, and\\nthen this logistic regression in the output layer which is what this does.\\nI hope this description made sense,\\nbut the takeaway is to compute the output of this neural network,\\nall you need is those four lines of code.\\nSo now, you've seen how given a single input feature,\\nvector a, you can with four lines of code,\\ncompute the output of this neural network.\\nSimilar to what we did for logistic regression,\\nwe'll also want to vectorize across multiple training examples.\\nWe'll see that by stacking up training examples in different columns in the matrix,\\nwith just slight modification to this, you also,\\nsimilar to what you saw in this regression,\\nbe able to compute the output of this neural network,\\nnot just a one example at a time,\\nprolong your, say your entire training set at a time.\\nSo, let's see the details of that in the next video.\"},\n",
       "    {'lesson_number': '04',\n",
       "     'lesson_name': 'vectorizing-across-multiple-examples',\n",
       "     'content': \"In the last video, you saw how to compute the prediction on a neural network,\\ngiven a single training example.\\nIn this video, you see how to vectorize across multiple training examples.\\nAnd the outcome will be quite similar to what you saw for logistic regression.\\nWhereby stacking up different training examples in different columns of\\nthe matrix, you'd be able to take the equations you had from the previous video.\\nAnd with very little modification, change them to make the neural network compute\\nthe outputs on all the examples on pretty much all at the same time.\\nSo let's see the details on how to do that.\\nThese were the four equations we have from the previous video of how you compute z1,\\na1, z2 and a2.\\nAnd they tell you how, given an input feature back to x,\\nyou can use them to generate a2 =y hat for a single training example.\\nNow if you have m training examples, you need to repeat this process for\\nsay, the first training example.\\nx superscript (1) to compute\\ny hat 1 does a prediction on your first training example.\\nThen x(2) use that to generate prediction y hat (2).\\nAnd so on down to x(m) to generate a prediction y hat (m).\\nAnd so in all these activation function notation as well,\\nI'm going to write this as a[2](1).\\nAnd this is a[2](2),\\nand a(2)(m), so\\nthis notation a[2](i).\\nThe round bracket i refers to training example i,\\nand the square bracket 2 refers to layer 2, okay.\\nSo that's how the square bracket and the round bracket indices work.\\nAnd so to suggest that if you have an unvectorized implementation and\\nwant to compute the predictions of all your training examples,\\nyou need to do for i = 1 to m.\\nThen basically implement these four equations, right?\\nYou need to make a z[1](i)\\n= W(1) x(i) + b[1],\\na[1](i) = sigma of z[1](1).\\nz[2](i) = w[2]a[1](i)\\n+ b[2] andZ2i equals w2a1i plus b2 and\\na[2](i) = sigma point of z[2](i).\\nSo it's basically these four equations on top by adding the superscript round\\nbracket i to all the variables that depend on the training example.\\nSo adding this superscript round bracket i to x is z and a,\\nif you want to compute all the outputs on your m training examples examples.\\nWhat we like to do is vectorize this whole computation, so as to get rid of this for.\\nAnd by the way, in case it seems like I'm getting a lot of nitty gritty\\nlinear algebra, it turns out that being able to implement this\\ncorrectly is important in the deep learning era.\\nAnd we actually chose notation very carefully for this course and\\nmake this vectorization steps as easy as possible.\\nSo I hope that going through this nitty gritty will actually help you to\\nmore quickly get correct implementations of these algorithms working.\\nAlright, so let me just copy this whole block of code to the next slide and\\nthen we'll see how to vectorize this.\\nSo here's what we have from the previous slide with the for\\nloop going over our m training examples.\\nSo recall that we defined the matrix x to be equal\\nto our training examples stacked up in these columns like so.\\nSo take the training examples and stack them in columns.\\nSo this becomes a n, or\\nmaybe nx by m diminish the matrix.\\nI'm just going to give away the punch line and tell you what you need to implement in\\norder to have a vectorized implementation of this for loop.\\nIt turns out what you need to do is compute\\nZ[1] = W[1] X + b[1],\\nA[1]= sig point of z[1].\\nThen Z[2] = w[2]\\nA[1] + b[2] and\\nthen A[2] = sig point of Z[2].\\nSo if you want the analogy is that we went from lower case vector xs\\nto just capital case X matrix by stacking up the lower case xs in different columns.\\nIf you do the same thing for the zs, so for example,\\nif you take z[1](i), z[1](2), and so\\non, and these are all column vectors, up to z[1](m), right.\\nSo that's this first quantity that all m of them, and stack them in columns.\\nThen just gives you the matrix z[1].\\nAnd similarly you look at say this quantity and\\ntake a[1](1), a[1](2) and so on and\\na[1](m), and stacked them up in columns.\\nThen this, just as we went from lower case x to capital case X, and\\nlower case z to capital case Z.\\nThis goes from the lower case a, which are vectors to this capital A[1],\\nthat's over there and similarly, for z[2] and a[2].\\nRight they're also obtained by taking these vectors and\\nstacking them horizontally.\\nAnd taking these vectors and stacking them horizontally,\\nin order to get Z[2], and E[2].\\nOne of the property of this notation that might help\\nyou to think about it is that this matrixes say Z and A,\\nhorizontally we're going to index across training examples.\\nSo that's why the horizontal index corresponds to different training example,\\nwhen you sweep from left to right you're scanning through the training cells.\\nAnd vertically this vertical index corresponds to different nodes in\\nthe neural network.\\nSo for example, this node, this value at the top most,\\ntop left most corner of the mean corresponds to the activation\\nof the first heading unit on the first training example.\\nOne value down corresponds to the activation in the second hidden unit on\\nthe first training example,\\nthen the third heading unit on the first training sample and so on.\\nSo as you scan down this is your indexing to the hidden units number.\\nWhereas if you move horizontally, then you're going from the first hidden unit.\\nAnd the first training example to now the first hidden unit and\\nthe second training sample, the third training example.\\nAnd so on until this node here corresponds to the activation of the first\\nhidden unit on the final train example and the nth training example.\\nOkay so the horizontally the matrix A goes over different training examples.\\nAnd vertically the different indices in the matrix\\nA corresponds to different hidden units.\\nAnd a similar intuition holds true for the matrix Z as well as for\\nX where horizontally corresponds to different training examples.\\nAnd vertically it corresponds to different input features which\\nare really different than those of the input layer of the neural network.\\nSo of these equations, you now know how to implement in your network\\nwith vectorization, that is vectorization across multiple examples.\\nIn the next video I want to show you a bit more justification about why\\nthis is a correct implementation of this type of vectorization.\\nIt turns out the justification would be similar to what you had seen in logistic regression.\\nLet's go on to the next video.\"},\n",
       "    {'lesson_number': '05',\n",
       "     'lesson_name': 'explanation-for-vectorized-implementation',\n",
       "     'content': \"In the previous video,\\nwe saw how with your training examples stacked up horizontally in the matrix x,\\nyou can derive a vectorized implementation for propagation through your neural network.\\nLet's give a bit more justification for why the equations we wrote\\ndown is a correct implementation of vectorizing across multiple examples.\\nSo let's go through part of the forward propagation calculation for the few examples.\\nLet's say that for the first training example,\\nyou end up computing\\nthis x1 plus b1 and then for the second training example,\\nyou end up computing this x2 plus b1 and\\nthen for the third training example,\\nyou end up computing this 3 plus b1.\\nSo, just to simplify the explanation on this slide, I'm going to ignore b.\\nSo let's just say, to simplify this justification a little bit that b is equal to zero.\\nBut the argument we're going to lay out will work with\\njust a little bit of a change even when b is non-zero.\\nIt does just simplify the description on the slide a bit.\\nNow, w1 is going to be some matrix, right?\\nSo I have some number of rows in this matrix.\\nSo if you look at this calculation x1,\\nwhat you have is\\nthat w1 times x1 gives you some column vector which you must draw like this.\\nAnd similarly, if you look at this vector x2,\\nyou have that w1 times\\nx2 gives some other column vector, right?\\nAnd that's gives you this z12.\\nAnd finally, if you look at x3,\\nyou have w1 times x3,\\ngives you some third column vector, that's this z13.\\nSo now, if you consider the training set capital X,\\nwhich we form by stacking together all of our training examples.\\nSo the matrix capital X is formed by taking the vector x1 and\\nstacking it vertically with x2 and then also x3.\\nThis is if we have only three training examples.\\nIf you have more, you know, they'll keep stacking horizontally like that.\\nBut if you now take this matrix x and multiply it by w then you end up with,\\nif you think about how matrix multiplication works,\\nyou end up with the first column being\\nthese same values that I had drawn up there in purple.\\nThe second column will be those same four values.\\nAnd the third column will be those orange values,\\nwhat they turn out to be.\\nBut of course this is just equal to z11 expressed as\\na column vector followed by z12 expressed as a column vector followed by z13,\\nalso expressed as a column vector.\\nAnd this is if you have three training examples.\\nYou get more examples then there'd be more columns.\\nAnd so, this is just our matrix capital Z1.\\nSo I hope this gives a justification for why we had\\npreviously w1 times xi equals\\nz1i when we're looking at single training example at the time.\\nWhen you took the different training examples and stacked them up in different columns,\\nthen the corresponding result is that you end up\\nwith the z's also stacked at the columns.\\nAnd I won't show but you can convince yourself if you want that with Python broadcasting,\\nif you add back in,\\nthese values of b to the values are still correct.\\nAnd what actually ends up happening is you end up with Python broadcasting,\\nyou end up having bi individually to each of the columns of this matrix.\\nSo on this slide, I've only justified that z1 equals\\nw1x plus b1 is\\na correct vectorization of\\nthe first step of the four steps we have in the previous slide,\\nbut it turns out that a similar analysis allows you to\\nshow that the other steps also work on using\\na very similar logic where if you stack the inputs in columns then after the equation,\\nyou get the corresponding outputs also stacked up in columns.\\nFinally, let's just recap everything we talked about in this video.\\nIf this is your neural network,\\nwe said that this is what you need to do if you were to implement for propagation,\\none training example at a time going from i equals 1 through m. And then we said,\\nlet's stack up the training examples in columns like so and for each of these values z1,\\na1, z2, a2, let's stack up the corresponding columns as follows.\\nSo this is an example for a1 but this is true for z1,\\na1, z2, and a2.\\nThen what we show on the previous slide was that\\nthis line allows you to vectorize this across all m examples at the same time.\\nAnd it turns out with the similar reasoning,\\nyou can show that all of the other lines are\\ncorrect vectorizations of all four of these lines of code.\\nAnd just as a reminder,\\nbecause x is also equal to a0 because remember that\\nthe input feature vector x was equal to a0, so xi equals a0i.\\nThen there's actually a certain symmetry to\\nthese equations where this first equation can also be\\nwritten z1 = w1 a0 + b1.\\nAnd so, you see that this pair of equations and this pair of\\nequations actually look very similar but just of all of the indices advance by one.\\nSo this kind of shows that the different layers of a neural network are\\nroughly doing the same thing or just doing the same computation over and over.\\nAnd here we have two-layer neural network where we go to\\na much deeper neural network in next week's videos.\\nYou see that even deeper neural networks are basically taking\\nthese two steps and just doing them even more times than you're seeing here.\\nSo that's how you can vectorize your neural network across multiple training examples.\\nNext, we've so far been using the sigmoid functions throughout our neural networks.\\nIt turns out that's actually not the best choice.\\nIn the next video, let's dive a little bit\\nfurther into how you can use different, what's called,\\nactivation functions of which the sigmoid function is just one possible choice.\"},\n",
       "    {'lesson_number': '06',\n",
       "     'lesson_name': 'why-do-you-need-non-linear-activation-functions',\n",
       "     'content': \"Why does a neural network need a non-linear activation function?\\nTurns out that your neural network to compute interesting functions,\\nyou do need to pick a non-linear activation function, let's see one.\\nSo, here's the four prop equations for the neural network.\\nWhy don't we just get rid of this?\\nGet rid of the function g?\\nAnd set a1 equals z1.\\nOr alternatively, you can say that g of z is equal to z, all right?\\nSometimes this is called the linear activation function.\\nMaybe a better name for it would be the identity activation function\\nbecause it just outputs whatever was input.\\nFor the purpose of this, what if a(2) was just equal z(2)?\\nIt turns out if you do this, then this model is just computing y or\\ny-hat as a linear function of your input features,\\nx, to take the first two equations.\\nIf you have that a(1)\\n= Z(1) = W(1)x + b, and\\nthen a(2) = z (2) =\\nW(2)a(1) + b.\\nThen if you take this definition of a1 and\\nplug it in there, you find that a2 =\\nw2(w1x + b1), move that up a bit.\\nRight? So this is a1 + b2, and so\\nthis simplifies to:\\n(W2w1)x +\\n(w2b1 + b2).\\nSo this is just,\\nlet's call this w prime b prime.\\nSo this is just equal to w' x + b'.\\nIf you were to use linear activation functions or\\nwe can also call them identity activation functions,\\nthen the neural network is just outputting a linear function of the input.\\nAnd we'll talk about deep networks later, neural networks with many, many layers,\\nmany hidden layers. And it turns out that\\nif you use a linear activation function or alternatively,\\nif you don't have an activation function, then no matter how many layers your neural\\nnetwork has, all it's doing is just computing a linear activation function.\\nSo you might as well not have any hidden layers.\\nSome of the cases that are briefly mentioned, it turns out that if you have\\na linear activation function here and a sigmoid function here, then this model is\\nno more expressive than standard logistic regression without any hidden layer.\\nSo I won't bother to prove that, but you could try to do so if you want.\\nBut the take home is that a linear hidden layer is more or less useless\\nbecause the composition of two linear functions is itself a linear function.\\nSo unless you throw a non-linear item in there, then you're not computing more\\ninteresting functions even as you go deeper in the network.\\nThere is just one place where you might use a linear activation function.\\ng(x) = z.\\nAnd that's if you are doing machine learning on the regression problem.\\nSo if y is a real number.\\nSo for example, if you're trying to predict housing prices.\\nSo y is not 0, 1, but is a real number, anywhere from - I don't know -\\n$0 is the price of house up to however expensive, right, houses get, I guess.\\nMaybe houses can be potentially millions of dollars, so\\nhowever much houses cost in your data set.\\nBut if y takes on these real values,\\nthen it might be okay to have a linear activation function here so\\nthat your output y hat is also\\na real number going anywhere from minus infinity to plus infinity.\\nBut then the hidden units should not use the activation functions.\\nThey could use ReLU or tanh or Leaky ReLU or maybe something else.\\nSo the one place you might use a linear activation function\\nis usually in the output layer.\\nBut other than that, using a linear activation function in the hidden layer\\nexcept for some very special circumstances relating to compression that we're\\ngoing to talk about using the linear activation function is extremely rare.\\nAnd, of course, if we're actually predicting housing prices,\\nas you saw in the week one video, because housing prices are all non-negative,\\nPerhaps even then you can use a value activation function so\\nthat your output y-hats are all greater than or equal to 0.\\nSo I hope that gives you a sense of why having a non-linear activation\\nfunction is a critical part of neural networks.\\nNext we're going to start to talk about gradient descent and\\nto do that to set up for our discussion for gradient descent,\\nin the next video I want to show you how to estimate-how to compute-the slope or\\nthe derivatives of individual activation functions.\\nSo let's go on to the next video.\"},\n",
       "    {'lesson_number': '07',\n",
       "     'lesson_name': 'gradient-descent-for-neural-networks',\n",
       "     'content': \"All right. I think this'll be an exciting video.\\nIn this video, you'll see how to implement\\ngradient descent for your neural network with one hidden layer.\\nIn this video, I'm going to just give you the equations you need to\\nimplement in order to get back-propagation or to get gradient descent working,\\nand then in the video after this one,\\nI'll give some more intuition about why\\nthese particular equations are the accurate equations,\\nare the correct equations for computing the gradients you need for your neural network.\\nSo, your neural network,\\nwith a single hidden layer for now,\\nwill have parameters W1,\\nB1, W2, and B2.\\nSo, as a reminder,\\nif you have NX or alternatively N0 input features,\\nand N1 hidden units,\\nand N2 output units in our examples.\\nSo far I've only had N2 equals one,\\nthen the matrix W1 will be N1 by N0.\\nB1 will be an N1 dimensional vector,\\nso we can write that as N1 by one-dimensional matrix,\\nreally a column vector.\\nThe dimensions of W2 will be N2 by N1,\\nand the dimension of B2 will be N2 by one.\\nRight, so far we've only seen examples where N2 is equal to one,\\nwhere you have just one single hidden unit.\\nSo, you also have a cost function for a neural network.\\nFor now, I'm just going to assume that you're doing binary classification.\\nSo, in that case,\\nthe cost of your parameters as follows is going to be one\\nover M of the average of that loss function.\\nSo, L here is the loss when your neural network predicts Y hat, right.\\nThis is really A2 when the gradient label is equal to Y.\\nIf you're doing binary classification,\\nthe loss function can be exactly what you use for logistic regression earlier.\\nSo, to train the parameters of your algorithm,\\nyou need to perform gradient descent.\\nWhen training a neural network,\\nit is important to initialize the parameters randomly rather than to all zeros.\\nWe'll see later why that's the case,\\nbut after initializing the parameter to something,\\neach loop or gradient descents with computed predictions.\\nSo, you basically compute your Y hat I,\\nfor I equals one through M, say.\\nThen, you need to compute the derivative.\\nSo, you need to compute DW1,\\nand that's the derivative of the cost function with respect to the parameter W1,\\nyou can compute another variable,\\nshall I call DB1,\\nwhich is the derivative or the slope of your cost function with\\nrespect to the variable B1 and so on.\\nSimilarly for the other parameters W2 and B2.\\nThen finally, the gradient descent update would be to update W1 as W1 minus Alpha.\\nThe learning rate times D, W1.\\nB1 gets updated as B1 minus the learning rate,\\ntimes DB1, and similarly for W2 and B2.\\nSometimes, I use colon equals and sometimes equals,\\nas either notation works fine.\\nSo, this would be one iteration of gradient descent,\\nand then you repeat this some number of\\ntimes until your parameters look like they're converging.\\nSo, in previous videos,\\nwe talked about how to compute the predictions,\\nhow to compute the outputs,\\nand we saw how to do that in a vectorized way as well.\\nSo, the key is to know how to compute these partial derivative terms,\\nthe DW1, DB1 as well as the derivatives DW2 and DB2.\\nSo, what I'd like to do is just give you\\nthe equations you need in order to compute these derivatives.\\nI'll defer to the next video, which is an optional video, to go\\ngreater into Jeff about how we came up with those formulas.\\nSo, let me just summarize again the equations for propagation.\\nSo, you have Z1 equals W1X plus B1,\\nand then A1 equals the activation function in that layer applied element wise as Z1,\\nand then Z2 equals W2,\\nA1 plus V2, and then finally,\\njust as all vectorized across your training set, right?\\nA2 is equal to G2 of Z2.\\nAgain, for now, if we assume we're doing binary classification,\\nthen this activation function really should be the sigmoid function,\\nsame just for that end neural.\\nSo, that's the forward propagation or the left to\\nright for computation for your neural network.\\nNext, let's compute the derivatives.\\nSo, this is the back propagation step.\\nThen I compute DZ2 equals A2 minus the gradient of Y,\\nand just as a reminder,\\nall this is vectorized across examples.\\nSo, the matrix Y is this one by\\nM matrix that lists all of your M examples stacked horizontally.\\nThen it turns out DW2 is equal to this,\\nand in fact, these first three equations are\\nvery similar to gradient descents for logistic regression.\\nX is equals one,\\ncomma, keep dims equals true.\\nJust a little detail this np.sum is\\na Python NumPy command for summing across one-dimension of a matrix.\\nIn this case, summing horizontally,\\nand what keepdims does is, it prevents Python from\\noutputting one of those funny rank one arrays, right?\\nWhere the dimensions was your N comma.\\nSo, by having keepdims equals true,\\nthis ensures that Python outputs for DB a vector that is N by one.\\nIn fact, technically this will be I guess N2 by one.\\nIn this case, it's just a one by one number,\\nso maybe it doesn't matter.\\nBut later on, we'll see when it really matters.\\nSo, so far what we've done is very similar to logistic regression.\\nBut now as you continue to run back propagation,\\nyou will compute this,\\nDZ2 times G1 prime of Z1.\\nSo, this quantity G1 prime is\\nthe derivative of whether it was the activation function you use for the hidden layer,\\nand for the output layer,\\nI assume that you are doing binary classification with the sigmoid function.\\nSo, that's already baked into that formula for DZ2,\\nand his times is element-wise product.\\nSo, this here is going to be an N1 by M matrix, and this here,\\nthis element-wise derivative thing is also going to be an N1 by N matrix,\\nand so this times there is an element-wise product of two matrices.\\nThen finally, DW1 is equal to that,\\nand DB1 is equal to this,\\nand p.sum DZ1 axis\\nequals one, keepdims equals true.\\nSo, whereas previously the keepdims maybe matter less if N2 is equal to one.\\nResult is just a one by one thing, is just a real number.\\nHere, DB1 will be a N1 by one vector,\\nand so you want Python, you want Np.sons.\\nI'll put something of this dimension rather than a funny rank one array\\nof that dimension which could end up messing up some of your data calculations.\\nThe other way would be to not have to keep the parameters,\\nbut to explicitly reshape the output of NP.sum into this dimension,\\nwhich you would like DB to have.\\nSo, that was forward propagation in I guess four equations,\\nand back-propagation in I guess six equations.\\nI know I just wrote down these equations,\\nbut in the next optional video,\\nlet's go over some intuitions for how\\nthe six equations for the back propagation algorithm were derived.\\nPlease feel free to watch that or not.\\nBut either way, if you implement these algorithms,\\nyou will have a correct implementation of forward prop and back prop.\\nYou'll be able to compute the derivatives you need in order to apply gradient descent,\\nto learn the parameters of your neural network.\\nIt is possible to implement this algorithm and\\nget it to work without deeply understanding the calculus.\\nA lot of successful deep learning practitioners do so.\\nBut, if you want,\\nyou can also watch the next video,\\njust to get a bit more intuition of what the derivation of these equations.\"},\n",
       "    {'lesson_number': '08',\n",
       "     'lesson_name': 'backpropagation-intuition-optional',\n",
       "     'content': \"In the last video, you saw\\nthe equations for back-propagation.\\nIn this video, let's go over some intuition using\\nthe computation graph for\\nhow those equations were derived.\\nThis video is completely optional\\nso feel free to watch it or not.\\nYou should be able to do the whole works either way.\\nRecall that when we talked about logistic regression,\\nwe had this forward pass where we compute z, then A,\\nand then A loss and the to take derivatives we\\nhad this backward pass where we can\\nfirst compute da and then go on to compute dz,\\nand then go on to compute dw and db.\\nThe definition for the loss was L of a comma y equals\\nnegative y log A minus 1 minus y times log 1 minus A.\\nIf you're familiar with\\ncalculus and you take the derivative of\\nthis with respect to A\\nthat will give you the formula for da.\\nSo da is equal to that.\\nIf you actually figure out the calculus,\\nyou can show that this is\\nnegative y over A plus 1 minus y over one minus A.\\nJust kind of derived that from\\ncalculus by taking derivatives of this.\\nIt turns out when you take\\nanother step backwards to compute dz,\\nwe then worked out that dz is equal to A\\nminus y. I didn't explain why previously,\\nbut it turns out that from the chain rule of calculus,\\ndz is equal to da times g prime of z.\\nWhere here g of z equals sigmoid of\\nz as our activation function\\nfor this output unit in logistic regression.\\nJust remember, this is still logistic regression,\\nwill have X_1, X_2,\\nX_3, and then just one sigmoid unit,\\nand then that gives us a, gives us y hat.\\nHere the activation function was sigmoid function.\\nAs an aside, only for those of you familiar\\nwith the chain rule of calculus.\\nThe reason for this is\\nbecause a is equal to sigmoid of z,\\nand so partial of L with respect\\nto z is equal to partial of\\nL with respect to a times da, dz.\\nSince A is equal to sigmoid of z.\\nThis is equal to d,\\ndz g of z,\\nwhich is equal to g prime of z.\\nThat's why this expression,\\nwhich is dz in our code is equal to this expression,\\nwhich is da in our code times g prime of z\\nand so this just that.\\nThat last derivation would have made sense only if you're\\nfamiliar with calculus and\\nspecifically the chain rule from calculus.\\nBut if not, don't worry about it,\\nI'll try to explain the intuition wherever it's needed.\\nThen finally, having computed dz for logistic regression,\\nwe will compute dw,\\nwhich it turned out was dz times x and\\ndb which is just\\ndz where you have a single training example.\\nThat was logistic regression.\\nWhat we're going to do when computing\\nback-propagation for a neural network\\nis a calculation a lot like this,\\nbut only we'll do it twice.\\nBecause now we have not x going to an output unit,\\nbut x going to a hidden layer\\nand then going to an output unit.\\nInstead of this computation\\nbeing one step as we have here,\\nwe'll have two steps here\\nin this neural network with two layers.\\nIn this two-layer neural network,\\nthat is with the input layer,\\nhidden layer, and an output layer.\\nRemember the steps of a computation.\\nFirst, you compute z_1\\nusing this equation and then compute a_1,\\nand then you compute z_2.\\nNotice z_2 also depends on the parameters W_2 and b_2,\\nand then based on z_2 you compute a_2.\\nThen finally, that gives you the loss.\\nWhat back-propagation does, is it will go backward to\\ncompute da_2 and then dz_2,\\nthen go back to compute dW_2 and db_2.\\nGo back to compute da_1,\\ndz_1, and so on.\\nWe don't need to take\\nderivatives with respect to the input x,\\nsince input x for supervised learning because\\nWe're not trying to optimize x,\\nso we won't bother to take derivatives,\\nat least for supervised learning with respect to\\nx. I'm going to skip explicitly computing da.\\nIf you want, you can actually compute da^2,\\nand then use that to compute dz^2.\\nBut in practice, you could collapse both of\\nthese steps into one step.\\nYou end up that dz^2 is equal to a^2 minus y,\\nsame as before, and you have also going to\\nwrite dw^2 and db^2 down here below.\\nYou have that dw^2 is equal to dz^2 times a^1 transpose,\\nand db^2 equals dz^2.\\nThis step is quite similar for logistic regression,\\nwhere we had that dw was equal to dz times x,\\nexcept that now, a^1 plays the role of x,\\nand there's an extra transpose there.\\nBecause the relationship between\\nthe capital matrix W\\nand our individual parameters w was,\\nthere's a transpose there,\\nbecause w is equal to a row vector.\\nIn the case of logistic regression\\nwith the single output,\\ndw^2 is like that, whereas w here was a column vector.\\nThat's why there's an extra transpose for a^1,\\nwhereas we didn't for x here for logistic regression.\\nThis completes half of backpropagation.\\nThen again, you can compute da^1,\\nif you wish although in practice,\\nthe computation for da^1,\\nand dz^1 are usually collapsed into one step.\\nWhat you'd actually implement is that dz^1 is equal to\\nw^2 transpose times dz^2 and then,\\ntimes an element-wise product of g^1 prime of z^1.\\nJust to do a check on the dimensions.\\nIf you have a neural network that looks like this,\\noutputs y if so.\\nIf you have n^0 and x equals n^0,\\nand for features, n^1 hidden units,\\nand n^2 so far,\\nand n^2 in our case,\\njust one output unit,\\nthen the matrix w^2 is n^2 by n^1 dimensional,\\nz^2, and therefore, dz^2 are going\\nto be n^2 by one-dimensional.\\nThere's really going to be a one by one\\nwhen we're doing binary classification,\\nand z^1, and therefore also dz^1\\nare going to be n^1 by one-dimensional.\\nNote that for any variable,\\nfoo and dfoo always have the same dimensions.\\nThat's why, w and dw always have the same dimension.\\nSimilarly, for b and db,\\nand z and dz, and so on.\\nTo make sure that the dimensions of these all match up,\\nwe have that dz^1 is equal to w^2 transpose, times dz^2.\\nThen, this is an element-wise product times\\ng^1 prime of z^1.\\nMashing the dimensions from above,\\nthis is going to be n^1 by 1,\\nis equal to w^2 transpose,\\nwe transpose of this.\\nIt is just going to be, n^1 by n^2-dimensional,\\ndz^2 is going to be n^2 by one-dimensional.\\nThen, this is same dimension as z^.\\nThis is also, n^1 by\\none-dimensional, so element-wise product.\\nThe dimensions do make sense.\\nN^1 by one-dimensional vector can be\\nobtained by n^1 by n^2 dimensional matrix,\\ntimes n^2 by n^1,\\nbecause the product of these two things gives you\\nan n^1 by one-dimensional matrix.\\nThis becomes the element-wise product of 2,\\nn^1 by one-dimensional vectors,\\nso the dimensions do match up.\\nOne tip when implementing backprop,\\nif you just make sure that the dimensions of\\nyour matrices match up, if you think through,\\nwhat are the dimensions of\\nyour various matrices including w^1,\\nw^2, z^1, z^2, a^1,\\na^2, and so on,\\nand just make sure that\\nthe dimensions of these matrix operations may match up,\\nsometimes that will already\\neliminate quite a lot of bugs in backprop.\\nThis gives us dz^1. Then finally,\\njust to wrap up, dw^1 and db^1,\\nwe should write them here, I guess.\\nBut since I'm running out of space,\\nI'll write them on the right of the slide,\\ndw^1 and db^1 are given by the following formulas.\\nThis is going to equal to dz^1 times x transpose,\\nand this is going to be equal to dz.\\nYou might notice a similarity between\\nthese equations and these equations,\\nwhich is really no coincidence,\\nbecause x plays the role of a^0.\\nX transpose is a^0 transpose.\\nThose equations are actually very similar.\\nThat gives a sense for how backpropagation is derived.\\nWe have six key equations here for dz_2,\\ndw_2, db_2, dz_1, dw_1, and db_1.\\nLet me just take these six equations and\\ncopy them over to the next slide.\\nHere they are. So far we've derived\\nthat propagation for training\\non a single training example at a time.\\nBut it should come as no surprise that\\nrather than working on a single example at a time,\\nwe would like to vectorize\\nacross different training examples.\\nYou remember that for\\na propagation when we're\\noperating on one example at a time,\\nwe had equations like this,\\nas well as say a^1 equals g^1 plus z^1.\\nIn order to vectorize, we took say,\\nthe z's and stack them up in columns like this,\\nz^1m, and call this capital Z.\\nThen we found that by stacking things up in columns\\nand defining the capital uppercase version of these,\\nwe then just had z^1 equals to the w^1x plus\\nb and a^1 equals g^1 of z^1.\\nWe defined the notation very carefully\\nin this course to make sure that\\nstacking examples into different columns\\nof a matrix makes all this workout.\\nIt turns out that if you go through the math carefully,\\nthe same trick also works for backpropagation.\\nThe vectorized equations are as follows.\\nFirst, if you take this dzs for\\ndifferent training examples and stack\\nthem as different columns of a matrix,\\nsame for this, same for this.\\nThen this is the vectorized implementation.\\nHere's how you can compute dW^2.\\nThere is this extra 1 over n\\nbecause the cost function J is\\nthis 1 over m of the sum from I\\nequals 1 through m of the losses.\\nWhen computing derivatives, we\\nhave that extra 1 over m term,\\njust as we did when we were\\ncomputing the weight updates for logistic regression.\\nThat's the update you get for db^2,\\nagain, some of the dz's.\\nThen, we have 1 over m. Dz^1 is computed as follows.\\nOnce again, this is an element-wise product only,\\nwhereas previously, we saw on\\nthe previous slide that this was\\nan n1 by one-dimensional vector.\\nNo w, this is n1 by m dimensional matrix.\\nBoth of these are also n1 by m dimensional.\\nThat's why that asterisk is the element-wise product.\\nFinally, the remaining two updates\\nperhaps shouldn't look too surprising.\\nI hope that gives you some intuition for how\\nthe backpropagation algorithm is derived.\\nIn all of machine learning,\\nI think the derivation of the backpropagation algorithm\\nis actually one of the most\\ncomplicated pieces of math I've seen.\\nIt requires knowing both linear algebra as well as\\nthe derivative of matrices to really\\nderive it from scratch from first principles.\\nIf you are an expert in matrix calculus,\\nusing this process, you\\nmight want to derive the algorithm yourself.\\nBut I think that there actually plenty of\\ndeep learning practitioners that have seen\\nthe derivation at about the level you've\\nseen in this video and are already\\nable to have all the right intuitions and be able\\nto implement this algorithm very effectively.\\nIf you are an expert in calculus\\ndo see if you can derive the whole thing from scratch.\\nIt is one of the hardest pieces of math on\\nthe very hardest derivations\\nthat I've seen in all of machine learning.\\nBut either way, if you implement this,\\nthis will work and I think you have\\nenough intuitions to tune in and get it to work.\\nThere's just one last detail,\\nmy share of you before you implement your neural network,\\nwhich is how to\\ninitialize the weights of your neural network.\\nIt turns out that\\ninitializing your parameters not to zero,\\nbut randomly turns out to be\\nvery important for training your neural network.\\nIn the next video, you'll see why.\"},\n",
       "    {'lesson_number': '09',\n",
       "     'lesson_name': 'random-initialization',\n",
       "     'content': \"When you change your neural network,\\nit's important to initialize the weights randomly.\\nFor logistic regression, it was okay to initialize the weights to zero.\\nBut for a neural network of initialize the weights to parameters to all zero and\\nthen applied gradient descent, it won't work.\\nLet's see why.\\nSo you have here two input features, so\\nn0=2, and two hidden units, so n1=2.\\nAnd so the matrix associated with the hidden layer,\\nw 1, is going to be two-by-two.\\nLet's say that you initialize it to all 0s, so 0 0 0 0, two-by-two matrix.\\nAnd let's say B1 is also equal to 0 0.\\nIt turns out initializing the bias terms b to 0 is actually okay,\\nbut initializing w to all 0s is a problem.\\nSo the problem with this formalization is that for\\nany example you give it, you'll have that a1,1 and\\na1,2, will be equal, right?\\nSo this activation and this activation will be the same,\\nbecause both of these hidden units are computing exactly the same function.\\nAnd then, when you compute backpropagation,\\nit turns out that dz11 and\\ndz12 will also be the same colored by symmetry, right?\\nBoth of these hidden units will initialize the same way.\\nTechnically, for what I'm saying,\\nI'm assuming that the outgoing weights or also identical.\\nSo that's w2 is equal to 0 0.\\nBut if you initialize the neural network this way,\\nthen this hidden unit and this hidden unit are completely identical.\\nSometimes you say they're completely symmetric,\\nwhich just means that they're completing exactly the same function.\\nAnd by kind of a proof by induction,\\nit turns out that after every single iteration of training your two hidden\\nunits are still computing exactly the same function.\\nSince plots will show that dw will be a matrix that looks like this.\\nWhere every row takes on the same value.\\nSo we perform a weight update.\\nSo when you perform a weight update, w1 gets updated as w1- alpha times dw.\\nYou find that w1, after every iteration,\\nwill have the first row equal to the second row.\\nSo it's possible to construct a proof by induction that if you\\ninitialize all the ways, all the values of w to 0,\\nthen because both hidden units start off computing the same function.\\nAnd both hidden the units have the same influence on the output unit,\\nthen after one iteration, that same statement is still true,\\nthe two hidden units are still symmetric.\\nAnd therefore, by induction, after two iterations, three iterations and so on,\\nno matter how long you train your neural network,\\nboth hidden units are still computing exactly the same function.\\nAnd so in this case, there's really no point to having more than one hidden unit.\\nBecause they are all computing the same thing.\\nAnd of course, for larger neural networks, let's say of three features and\\nmaybe a very large number of hidden units,\\na similar argument works to show that with a neural network like this.\\nLet me draw all the edges, if you initialize the weights to zero,\\nthen all of your hidden units are symmetric.\\nAnd no matter how long you're upgrading the center,\\nall continue to compute exactly the same function.\\nSo that's not helpful, because you want the different\\nhidden units to compute different functions.\\nThe solution to this is to initialize your parameters randomly.\\nSo here's what you do.\\nYou can set w1 = np.random.randn.\\nThis generates a gaussian random variable (2,2).\\nAnd then usually, you multiply this by very small number, such as 0.01.\\nSo you initialize it to very small random values.\\nAnd then b, it turns out that b does not have the symmetry problem,\\nwhat's called the symmetry breaking problem.\\nSo it's okay to initialize b to just zeros.\\nBecause so long as w is initialized randomly,\\nyou start off with the different hidden units computing different things.\\nAnd so you no longer have this symmetry breaking problem.\\nAnd then similarly, for w2, you're going to initialize that randomly.\\nAnd b2, you can initialize that to 0.\\nSo you might be wondering, where did this constant come from and why is it 0.01?\\nWhy not put the number 100 or 1000?\\nTurns out that we usually prefer to initialize\\nthe weights to very small random values.\\nBecause if you are using a tanh or sigmoid activation function, or\\nthe other sigmoid, even just at the output layer.\\nIf the weights are too large,\\nthen when you compute the activation values,\\nremember that z[1]=w1 x + b.\\nAnd then a1 is the activation function applied to z1.\\nSo if w is very big, z will be very, or at least some\\nvalues of z will be either very large or very small.\\nAnd so in that case, you're more likely to end up at these fat parts of the tanh\\nfunction or the sigmoid function, where the slope or the gradient is very small.\\nMeaning that gradient descent will be very slow.\\nSo learning was very slow.\\nSo just a recap, if w is too large, you're more likely to end up\\neven at the very start of training, with very large values of z.\\nWhich causes your tanh or your sigmoid activation function to be saturated,\\nthus slowing down learning.\\nIf you don't have any sigmoid or\\ntanh activation functions throughout your neural network, this is less of an issue.\\nBut if you're doing binary classification, and your output unit is a sigmoid\\nfunction, then you just don't want the initial parameters to be too large.\\nSo that's why multiplying by 0.01 would be something reasonable to try, or\\nany other small number.\\nAnd same for w2, right?\\nThis can be random.random.\\nI guess this would be 1 by 2 in this example, times 0.01.\\nMissing an s there.\\nSo finally, it turns out that sometimes they can be better constants than 0.01.\\nWhen you're training a neural network with just one hidden layer,\\nit is a relatively shallow neural network, without too many hidden layers.\\nSet it to 0.01 will probably work okay.\\nBut when you're training a very very deep neural network,\\nthen you might want to pick a different constant than 0.01.\\nAnd in next week's material, we'll talk a little bit about how and\\nwhen you might want to choose a different constant than 0.01.\\nBut either way, it will usually end up being a relatively small number.\\nSo that's it for this week's videos.\\nYou now know how to set up a neural network of a hidden layer,\\ninitialize the parameters, make predictions using.\\nAs well as compute derivatives and implement gradient descent,\\nusing backprop.\\nSo that, you should be able to do the quizzes,\\nas well as this week's programming exercises.\\nBest of luck with that.\\nI hope you have fun with the problem exercise, and\\nlook forward to seeing you in the week four materials.\"},\n",
       "    {'lesson_number': '10',\n",
       "     'lesson_name': 'ian-goodfellow-interview',\n",
       "     'content': \"Hi, Ian. Thanks a lot for joining us today.\\nThank you for inviting me,\\nAndrew. I am glad to be here.\\nToday, you are one of the world's most visible deep learning researchers.\\nLet us share a bit about your personal story.\\nSo, how do you end up doing this work that you now do?\\nYeah. That sounds great.\\nI guess I first became interested in machine learning right before I met you, actually.\\nI had been working on neuroscience and my undergraduate adviser,\\nJerry Cain, at Stanford encouraged me to take your Intro to AI class.\\nOh, I didn't know that. Okay.\\nSo I had always thought that AI was a good idea,\\nbut that in practice, the main, I think,\\nidea that was happening was like game AI,\\nwhere people have a lot of hard-coded rules for\\nnon-player characters in games to say\\ndifferent scripted lines at different points in time.\\nAnd then, when I took your Intro to AI class and you covered topics like\\nlinear regression and the variance decomposition of the error of linear regression,\\nI started to realize that this is a real science and I could actually\\nhave a scientific career in AI rather than neuroscience.\\nI see. Great. And then what happened?\\nWell, I came back and I was the TA to your course later.\\nOh, I see. Right. Like a TA.\\nSo a really big turning point for me was while I was TA-ing that course,\\none of the students,\\nmy friend Ethan Dreifuss,\\ngot interested in Geoff Hinton's deep belief net paper.\\nI see.\\nAnd the two of us ended up building one of the first GPU CUDA-based machines at\\nStanford in order to run Watson machines in our spare time over winter break.\\nI see.\\nAnd at that point, I started to have\\na very strong intuition that deep learning was the way to go in the future,\\nthat a lot of the other algorithms that I was working with,\\nlike support vector machines,\\ndidn't seem to have the right asymptotics,\\nthat you add more training data and they get slower,\\nor for the same amount of training data,\\nit's hard to make them perform a lot better by changing other settings.\\nAt that point, I started to focus on deep learning as much as possible.\\nAnd I remember Richard Reyna's very old GPU paper\\nacknowledges you for having done a lot of early work.\\nYeah. Yeah. That was written using some of the machines that we built.\\nYeah.\\nThe first machine I built was just something that Ethan and I built at\\nEthan's mom's house with our own money,\\nand then later, we used lab money to build the first two or three for the Stanford lab.\\nWow that's great. I never knew that story. That's great.\\nAnd then, today, one of\\nthe things that's really taken\\nthe deep learning world by storm is your invention of GANs.\\nSo how did you come up with that?\\nI've been studying generative models for a long time,\\nso GANs are a way of doing\\ngenerative modeling where you have a lot of training data and you'd like\\nto learn to produce more examples that resemble the trading data, but they're imaginary.\\nThey've never been seen exactly in that form before.\\nThere were several other ways of doing generative models that had been\\npopular for several years before I had the idea for GANs.\\nAnd after I'd been working on all those other methods throughout most of my Ph.D.,\\nI knew a lot about the advantages and disadvantages of all the other frameworks like\\nBoltzmann machines and sparse coding\\nand all the other approaches that have been really popular for years.\\nI was looking for something that avoid all these disadvantages at the same time.\\nAnd then finally, when I was arguing about generative models with my friends in a bar,\\nsomething clicked into place,\\nand I started telling them, You need to do,\\nthis, this, and this and I swear it will work.\\nAnd my friends didn't believe me that it would work.\\nI was supposed to be writing the deep learning textbook at the time,\\nI see.\\nBut I believed strongly enough that it would work that I\\nwent home and coded it up the same night and it worked.\\nSo it take you one evening to implement the first version of GANs?\\nI implemented it around midnight\\nafter going home from the bar where my friend had his going-away party.\\nI see.\\nAnd the first version of it worked,\\nwhich is very, very fortunate.\\nI didn't have to search for hyperparameters or anything.\\nThere was a story, I read it somewhere,\\nwhere you had a near-death experience and that reaffirmed your commitment to AI.\\nTell me that one.\\nSo, yeah. I wasn't actually near death but I briefly thought that I was.\\nI had a very bad headache and some of\\nthe doctors thought that I might have a brain hemorrhage.\\nAnd during the time that I was waiting for\\nmy MRI results to find out whether I had a brain hemorrhage or not,\\nI realized that most of the thoughts I was having were about making\\nsure that other people would eventually\\ntry out the research ideas that I had at the time.\\nI see. I see.\\nIn retrospect, they're all pretty silly research ideas.\\nI see.\\nBut at that point,\\nI realized that this was actually one of my highest priorities in life,\\nwas carrying out my machine learning research work.\\nI see. Yeah. That's great,\\nthat when you thought you might be dying soon,\\nyou're just thinking how to get the research done.\\nYeah.\\nYeah. That's commitment.\\nYeah.\\nYeah. Yeah. So today, you're still at the center of a lot of the activities with GANs,\\nwith Generative Adversarial Networks.\\nSo tell me how you see the future of GANs.\\nRight now, GANs are used for a lot of different things, like semi-supervised learning,\\ngenerating training data for other models and even simulating scientific experiments.\\nIn principle, all of these things could be done by other kinds of generative models.\\nSo I think that GANs are at an important crossroads right now.\\nRight now, they work well some of the time,\\nbut it can be more of an art than a science to really bring that performance out of them.\\nIt's more or less how people felt about deep learning in general 10 years ago.\\nAnd back then, we were using\\ndeep belief networks with Boltzmann machines as the building blocks,\\nand they were very, very finicky.\\nOver time, we switched to things like rectified linear units and batch normalization,\\nand deep learning became a lot more reliable.\\nIf we can make GANs become as reliable as deep learning has become,\\nthen I think we'll keep seeing GANs used in\\nall the places they're used today with much greater success.\\nIf we aren't able to figure out how to stabilize GANs,\\nthen I think their main contribution to the history of deep learning is\\nthat they will have shown people how to\\ndo all these tasks that involve generative modeling,\\nand eventually, we'll replace them with other forms of generative models.\\nSo I spend maybe about 40 percent of my time right now working on stabilizing GANs.\\nI see. Cool. Okay. Oh, and so just as a lot of people\\nthat joined deep learning about 10 years ago, such as yourself,\\nwound up being pioneers,\\nmaybe the people that join GANs today,\\nif it works out, could end up the early pioneers.\\nYeah. A lot of people already are early pioneers of GANs,\\nand I think if you wanted to give any kind of history of GANs so far,\\nyou'd really need to mention other groups like Indico\\nand Facebook and Berkeley for all the different things that they've done.\\nSo in addition to all your research,\\nyou also coauthored a book on deep learning. How is that going?\\nThat's right, with Yoshua Bengio and Aaron Courville,\\nwho are my Ph.D. co-advisers.\\nWe wrote the first textbook on the modern version of deep learning,\\nand that has been very popular,\\nboth in the English edition and the Chinese edition.\\nWe've sold about, I think around 70,000 copies total between those two languages.\\nAnd I've had a lot of feedback from students who said that they've learned a lot from it.\\nOne thing that we did a little bit differently than some other books is we start with\\na very focused introduction to the kind of math that you need to do in deep learning.\\nI think one thing that I got from your courses at Stanford is\\nthat linear algebra and probability are very important,\\nthat people get excited about the machine learning algorithms,\\nbut if you want to be a really excellent practitioner,\\nyou've got to master the basic math that underlies the whole approach in the first place.\\nSo we make sure to give\\na very focused presentation of the math basics at the start of the book.\\nThat way, you don't need to go ahead and learn all that linear algebra,\\nthat you can get\\na very quick crash course in the pieces of\\nlinear algebra that are the most useful for deep learning.\\nSo even someone whose math is a little shaky or haven't seen the math for\\na few years will be able to start from the beginning of your book and\\nget that background and get into deep learning?\\nAll of the facts that you would need to know are there.\\nIt would definitely take some focused effort to practice making use of them.\\nYeah. Yeah. Great.\\nIf someone's really afraid of math,\\nit might be a bit of a painful experience.\\nBut if you're ready for the learning experience and you believe you can master it,\\nI think all the tools that you need are there.\\nAs someone that worked in deep learning for a long time,\\nI'd be curious, if you look back over the years.\\nTell me a bit about how you're thinking\\nof AI and deep learning has evolved over the years.\\nTen years ago, I felt like, as a community,\\nthe biggest challenge in machine learning was just how\\nto get it working for AI-related tasks at all.\\nWe had really good tools that we could use for simpler tasks,\\nwhere we wanted to recognize patterns in how to extract features,\\nwhere a human designer could do a lot of\\nthe work by creating those features and then hand it off to the computer.\\nNow, that was really good for different things\\nlike predicting which ads a user would click\\non or different kinds of basic scientific analysis.\\nBut we really struggled to do anything involving millions of pixels in an image or\\na raw audio wave form where\\nthe system had to build all of its understanding from scratch.\\nWe finally got over the hurdle really thoroughly maybe five years ago.\\nAnd now, we're at a point where there are\\nso many different paths open that someone who wants to get involved in AI,\\nmaybe the hardest problem they face is choosing which path they want to go down.\\nDo you want to make reinforcement learning work as well as supervised learning works?\\nDo you want to make unsupervised learning work as well as supervised learning works?\\nDo you want to make sure that machine learning algorithms are fair\\nand don't reflect biases that we'd prefer to avoid?\\nDo you want to make sure that the societal issues surrounding AI work out well,\\nthat we're able to make sure that AI benefits everyone\\nrather than causing social upheaval and trouble with loss of jobs?\\nI think right now,\\nthere's just really an amazing amount of different things that can be done,\\nboth to prevent downsides from AI but also to make sure\\nthat we leverage all of the upsides that it offers us.\\nAnd so today, there are a lot of people wanting to get into AI.\\nSo, what advice would you have for someone like that?\\nI think a lot of people that want to get into AI start thinking that\\nthey absolutely need to get a Ph.D. or some other kind of credential like that.\\nI don't think that's actually a requirement anymore.\\nOne way that you could get a lot of attention is to write good code and put it on GitHub.\\nIf you have an interesting project that solves\\na problem that someone working at the top level wanted to solve,\\nonce they find your GitHub repository,\\nthey'll come find you and ask you to come work there.\\nA lot of the people that I've hired or\\nrecruited at OpenAI last year or at Google this year,\\nI first became interested in working with them because of\\nsomething that I saw that they released in an open-source forum on the Internet.\\nWriting papers and putting them on Archive can also be good.\\nA lot of the time,\\nit's harder to reach the point where you have something polished enough to really be\\na new academic contribution to the scientific literature,\\nbut you can often get to the point of having a useful software product much earlier.\\nSo read your book,\\npractice the materials and post on GitHub and maybe on Archive.\\nI think if you learned by reading the book,\\nit's really important to also work on a project at the same time,\\nto either choose some way of\\napplying machine learning to an area that you are already interested in.\\nLike if you're a field biologist and you want to get into deep learning,\\nmaybe you could use it to identify birds,\\nor if you don't have an idea for how you'd like to use machine learning in your own life,\\nyou could pick something like making a Street View house numbers classifier,\\nwhere all the data sets are set up to make it very straightforward for you.\\nAnd that way, you get to exercise all of\\nthe basic skills while you read the book or while\\nyou watch Coursera videos that explain the concepts to you.\\nSo over the last couple of years,\\nI've also seen you do one more work on adversarial examples.\\nTell us a bit about that.\\nYeah. I think adversarial examples are\\nthe beginning of a new field that I call machine learning security.\\nIn the past, we've seen computer security issues\\nwhere attackers could fool a computer into running the wrong code.\\nThat's called application-level security.\\nAnd there's been attacks where people can fool a computer into believing that\\nmessages on a network come from somebody that is not actually who they say they are.\\nThat's called network-level security.\\nNow, we're starting to see that you can also fool\\nmachine-learning algorithms into doing things they shouldn't,\\neven if the program running the machine-learning algorithm is running the correct code,\\neven if the program running\\nthe machine-learning algorithm knows\\nwho all the messages on the network really came from.\\nAnd I think, it's important to build security\\ninto a new technology near the start of its development.\\nWe found that it's very hard to build a working system first and then add security later.\\nSo I am really excited about the idea that if\\nwe dive in and start anticipating security problems with machine learning now,\\nwe can make sure that these algorithms are secure from\\nthe start instead of trying to patch it in retroactively years later.\\nThank you. That was great.\\nThere's a lot about your story that I thought was fascinating and that,\\ndespite having known you for years,\\nI didn't actually know, so thank you for sharing all that.\\nOh, very welcome. Thank you for inviting me. It was a great shot.\\nOkay. Thank you.\\nVery welcome.\"}],\n",
       "   'generated_content': Code(topic='Chapter 3: Shallow Neural Networks', cell=[CellFormat(cell_type='markdown', cell_no=1, cell_content='# Chapter 3: Shallow Neural Networks'), CellFormat(cell_type='markdown', cell_no=2, cell_content='## Lesson 01: Neural Networks Overview'), CellFormat(cell_type='markdown', cell_no=3, cell_content=\"Welcome back. In this week, you learned to implement a neural network. Before diving into the technical details, I want in this video, to give you a quick overview of what you'll be seeing in this week's videos. So, if you don't follow all the details in this video, don't worry about it, we'll delve into the technical details in the next few videos. But for now, let's give a quick overview of how you implement a neural network.\"), CellFormat(cell_type='markdown', cell_no=4, cell_content=\"Last week, we had talked about logistic regression, and we saw how this model corresponds to the following computation draft, where you then put the features x and parameters w and b that allows you to compute z which is then used to computes a, and we were using a interchangeably with this output y hat and then you can compute the loss function, L. A neural network looks like this. As I'd already previously alluded, you can form a neural network by stacking together a lot of little sigmoid units. Whereas previously, this node corresponds to two steps to calculations. The first is compute the z-value, second is it computes this a value. In this neural network, this stack of notes will correspond to a z-like calculation like this, as well as, an a-like calculation like that. Then, that node will correspond to another z and another a like calculation.\"), CellFormat(cell_type='markdown', cell_no=5, cell_content=\"So the notation which we will introduce later will look like this. First, we'll inputs the features, x, together with some parameters w and b, and this will allow you to compute z one. So, new notation that we'll introduce is that we'll use superscript square bracket one to refer to quantities associated with this stack of nodes, it's called a layer. Then later, we'll use superscript square bracket two to refer to quantities associated with that node. That's called another layer of the neural network. The superscript square brackets, like we have here, are not to be confused with the superscript round brackets which we use to refer to individual training examples. So, whereas x superscript round bracket I refer to the ith training example, superscript square bracket one and two refer to these different layers; layer one and layer two in this neural network. But so going on, after computing z_1 similar to logistic regression, there'll be a computation to compute a_1, and that's just sigmoid of z_1, and then you compute z_2 using another linear equation and then compute a_2. A_2 is the final output of the neural network and will also be used interchangeably with y-hat.\"), CellFormat(cell_type='markdown', cell_no=6, cell_content=\"So, I know that was a lot of details but the key intuition to take away is that whereas for logistic regression, we had this z followed by a calculation. In this neural network, here we just do it multiple times, as a z followed by a calculation, and a z followed by a calculation, and then you finally compute the loss at the end. You remember that for logistic regression, we had this backward calculation in order to compute derivatives or as you're computing your d a, d z and so on. So, in the same way, a neural network will end up doing a backward calculation that looks like this in which you end up computing da_2, dz_2, that allows you to compute dw_2, db_2, and so on. This right to left backward calculation that is denoting with the red arrows. So, that gives you a quick overview of what a neural network looks like. It's basically taken logistic regression and repeating it twice. I know there was a lot of new notation laws, new details, don't worry about saving them, follow everything, we'll go into the details most probably in the next few videos. So, let's go on to the next video. We'll start to talk about the neural network representation.\"), CellFormat(cell_type='markdown', cell_no=7, cell_content='## Lesson 02: Neural Network Representation'), CellFormat(cell_type='markdown', cell_no=8, cell_content=\"You see me draw a few pictures of neural networks. In this video, we'll talk about exactly what those pictures means. In other words, exactly what those neural networks that we've been drawing represent. And we'll start with focusing on the case of neural networks with what was called a single hidden layer.\"), CellFormat(cell_type='markdown', cell_no=9, cell_content=\"Here's a picture of a neural network. Let's give different parts of these pictures some names. We have the input features, x1, x2, x3 stacked up vertically. And this is called the input layer of the neural network. So maybe not surprisingly, this contains the inputs to the neural network. Then there's another layer of circles. And this is called a hidden layer of the neural network. I'll come back in a second to say what the word hidden means. But the final layer here is formed by, in this case, just one node. And this single-node layer is called the output layer, and is responsible for generating the predicted value y hat. In a neural network that you train with supervised learning, the training set contains values of the inputs x as well as the target outputs y. So the term hidden layer refers to the fact that in the training set, the true values for these nodes in the middle are not observed. That is, you don't see what they should be in the training set. You see what the inputs are. You see what the output should be. But the things in the hidden layer are not seen in the training set. So that kind of explains the name hidden layer; just because you don't see it in the training set.\"), CellFormat(cell_type='markdown', cell_no=10, cell_content=\"Let's introduce a bit more notation. Whereas previously, we were using the vector X to denote the input features and alternative notation for the values of the input features will be A superscript square bracket 0. And the term A also stands for activations, and it refers to the values that different layers of the neural network are passing on to the subsequent layers. So the input layer passes on the value x to the hidden layer, so we're going to call that activations of the input layer A super script 0. The next layer, the hidden layer, will in turn generate some set of activations, which I'm going to write as A superscript square bracket 1. So in particular, this first unit or this first node, we generate a value A superscript square bracket 1 subscript 1. This second node we generate a value. Now we have a subscript 2 and so on. And so, A superscript square bracket 1, this is a four dimensional vector you want in Python because the 4x1 matrix, or a 4 column vector, which looks like this. And it's four dimensional, because in this case we have four nodes, or four units, or four hidden units in this hidden layer. And then finally, the open layer regenerates some value A2, which is just a real number. And so y hat is going to take on the value of A2. So this is analogous to how in logistic regression we have y hat equals a and in logistic regression which we only had that one output layer, so we don't use the superscript square brackets. But with our neural network, we now going to use the superscript square bracket to explicitly indicate which layer it came from.\"), CellFormat(cell_type='markdown', cell_no=11, cell_content=\"One funny thing about notational conventions in neural networks is that this network that you've seen here is called a two layer neural network. And the reason is that when we count layers in neural networks, we don't count the input layer. So the hidden layer is layer one and the output layer is layer two. In our notational convention, we're calling the input layer layer zero, so technically maybe there are three layers in this neural network. Because there's the input layer, the hidden layer, and the output layer. But in conventional usage, if you read research papers and elsewhere in the course, you see people refer to this particular neural network as a two layer neural network, because we don't count the input layer as an official layer.\"), CellFormat(cell_type='markdown', cell_no=12, cell_content=\"Finally, something that we'll get to later is that the hidden layer and the output layers will have parameters associated with them. So the hidden layer will have associated with it parameters w and b. And I'm going to write superscripts square bracket 1 to indicate that these are parameters associated with layer one with the hidden layer. We'll see later that w will be a 4 by 3 matrix and b will be a 4 by 1 vector in this example. Where the first coordinate four comes from the fact that we have four nodes of our hidden units and a layer, and three comes from the fact that we have three input features. We'll talk later about the dimensions of these matrices. And it might make more sense at that time. But in some of the output layers has associated with it also, parameters w superscript square bracket 2 and b superscript square bracket 2. And it turns out the dimensions of these are 1 by 4 and 1 by 1. And these 1 by 4 is because the hidden layer has four hidden units, the output layer has just one unit. But we will go over the dimension of these matrices and vectors in a later video. So you've just seen what a two layered neural network looks like. That is a neural network with one hidden layer. In the next video, let's go deeper into exactly what this neural network is computing. That is how this neural network inputs x and goes all the way to computing its output y hat.\"), CellFormat(cell_type='markdown', cell_no=13, cell_content=\"## Lesson 03: Computing a Neural Network's Output\"), CellFormat(cell_type='markdown', cell_no=14, cell_content=\"In the last video, you saw what a single hidden layer neural network looks like. In this video, let's go through the details of exactly how this neural network computes these outputs. What you see is that it is like logistic regression, but repeated a lot of times. Let's take a look. So, this is how a two-layer neural network looks. Let's go more deeply into exactly what this neural network computes.\"), CellFormat(cell_type='markdown', cell_no=15, cell_content=\"Now, we've said before that logistic regression, the circle in logistic regression, really represents two steps of computation rows. You compute z as follows, and a second, you compute the activation as a sigmoid function of z. So, a neural network just does this a lot more times. Let's start by focusing on just one of the nodes in the hidden layer. Let's look at the first node in the hidden layer. So, I've grayed out the other nodes for now. So, similar to logistic regression on the left, this nodes in the hidden layer does two steps of computation. The first step and think of as the left half of this node, it computes z equals w transpose x plus b, and the notation we'll use is, these are all quantities associated with the first hidden layer. So, that's why we have a bunch of square brackets there. This is the first node in the hidden layer. So, that's why we have the subscript one over there. So first, it does that, and then the second step, is it computes a_[1]_1 equals sigmoid of z_[1]_1, like so.\"), CellFormat(cell_type='markdown', cell_no=16, cell_content=\"So, for both z and a, the notational convention is that a, l, i, the l here in superscript square brackets, refers to the layer number, and the i subscript here, refers to the nodes in that layer. So, the node we'll be looking at is layer one, that is a hidden layer node one. So, that's why the superscripts and subscripts were both one, one. So, that little circle, that first node in the neural network, represents carrying out these two steps of computation.\"), CellFormat(cell_type='markdown', cell_no=17, cell_content=\"Now, let's look at the second node in the neural network, or the second node in the hidden layer of the neural network. Similar to the logistic regression unit on the left, this little circle represents two steps of computation. The first step is it computes z. This is still layer one, but now as a second node equals w transpose x, plus b_[1]_2, and then a_[1] two equals sigmoid of z_[1]_2. Again, feel free to pause the video if you want, but you can double-check that the superscript and subscript notation is consistent with what we have written here above in purple. So, we've talked through the first two hidden units in a neural network, having units three and four also represents some computations. So now, let me take this pair of equations, and this pair of equations, and let's copy them to the next slide. So, here's our neural network, and here's the first, and here's the second equations that we've worked out previously for the first and the second hidden units. If you then go through and write out the corresponding equations for the third and fourth hidden units, you get the following. So, let me show this notation is clear, this is the vector w_[1]_1, this is a vector transpose times x. So, that's what the superscript T there represents. It's a vector transpose.\"), CellFormat(cell_type='markdown', cell_no=18, cell_content=\"Now, as you might have guessed, if you're actually implementing a neural network, doing this with a for loop, seems really inefficient. So, what we're going to do, is take these four equations and vectorize. So, we're going to start by showing how to compute z as a vector, it turns out you could do it as follows. Let me take these w's and stack them into a matrix, then you have w_[1]_1 transpose, so that's a row vector, or this column vector transpose gives you a row vector, then w_[1]_2, transpose, w_[1]_3 transpose, w_[1]_4 transpose. So, by stacking those four w vectors together, you end up with a matrix. So, another way to think of this is that we have four logistic regression units there, and each of the logistic regression units, has a corresponding parameter vector, w. By stacking those four vectors together, you end up with this four by three matrix. So, if you then take this matrix and multiply it by your input features x1, x2, x3, you end up with by how matrix multiplication works. You end up with w_[1]_1 transpose x, w_2_[1] transpose x, w_3_[1] transpose x, w_4_[1] transpose x. Then, let's not figure the b's. So, we now add to this a vector b_[1]_1 one, b_[1]_2, b_[1]_3, b_[1]_4. So, that's basically this, then this is b_[1]_1, b_[1]_2, b_[1]_3, b_[1]_4. So, you see that each of the four rows of this outcome correspond exactly to each of these four rows, each of these four quantities that we had above. So, in other words, we've just shown that this thing is therefore equal to z_[1]_1, z_[1]_2, z_[1]_3, z_[1]_4, as defined here. Maybe not surprisingly, we're going to call this whole thing, the vector z_[1], which is taken by stacking up these individuals of z's into a column vector. When we're vectorizing, one of the rules of thumb that might help you navigate this, is that while we have different nodes in the layer, we'll stack them vertically. So, that's why we have z_[1]_1 through z_[1]_4, those corresponded to four different nodes in the hidden layer, and so we stacked these four numbers vertically to form the vector z[1]. To use one more piece of notation, this four by three matrix here which we obtained by stacking the lowercase w_[1]_1, w_[1]_2, and so on, we're going to call this matrix W capital [1]. Similarly, this vector, we're going to call b superscript [1] square bracket. So, this is a four by one vector.\"), CellFormat(cell_type='markdown', cell_no=19, cell_content=\"So now, we've computed z using this vector matrix notation, the last thing we need to do is also compute these values of a. So, prior won't surprise you to see that we're going to define a_[1], as just stacking together, those activation values, a [1], 1 through a [1], 4. So, just take these four values and stack them together in a vector called a[1]. This is going to be a sigmoid of z[1], where this now has been implementation of the sigmoid function that takes in the four elements of z, and applies the sigmoid function element-wise to it. So, just a recap, we figured out that z_[1] is equal to w_[1] times the vector x plus the vector b_[1], and a_[1] is sigmoid times z_[1]. Let's just copy this to the next slide. What we see is that for the first layer of the neural network given an input x, we have that z_[1] is equal to w_[1] times x plus b_[1], and a_[1] is sigmoid of z_[1]. The dimensions of this are four by one equals, this was a four by three matrix times a three by one vector plus a four by one vector b, and this is four by one same dimension as end. Remember, that we said x is equal to a_[0]. Just say y hat is also equal to a two. If you want, you can actually take this x and replace it with a_[0], since a_[0] is if you want as an alias for the vector of input features, x.\"), CellFormat(cell_type='markdown', cell_no=20, cell_content=\"Now, through a similar derivation, you can figure out that the representation for the next layer can also be written similarly where what the output layer does is, it has associated with it, so the parameters w_[2] and b_[2]. So, w_[2] in this case is going to be a one by four matrix, and b_[2] is just a real number as one by on. So, z_[2] is going to be a real number we'll write as a one by one matrix. Is going to be a one by four thing times a was four by one, plus b_[2] as one by one, so this gives you just a real number. If you think of this last upper unit as just being analogous to logistic regression which have parameters w and b, w really plays an analogous role to w_[2] transpose, or w_[2] is really W transpose and b is equal to b_[2]. I said we want to cover up the left of this network and ignore all that for now, then this last upper unit is a lot like logistic regression, except that instead of writing the parameters as w and b, we're writing them as w_[2] and b_[2], with dimensions one by four and one by one. So, just a recap. For logistic regression, to implement the output or to implement prediction, you compute z equals w transpose x plus b, and a or y hat equals a, equals sigmoid of z. When you have a neural network with one hidden layer, what you need to implement, is to computer this output is just these four equations. You can think of this as a vectorized implementation of computing the output of first these for logistic regression units in the hidden layer, that's what this does, and then this logistic regression in the output layer which is what this does. I hope this description made sense, but the takeaway is to compute the output of this neural network, all you need is those four lines of code. So now, you've seen how given a single input feature, vector a, you can with four lines of code, compute the output of this neural network. Similar to what we did for logistic regression, we'll also want to vectorize across multiple training examples. We'll see that by stacking up training examples in different columns in the matrix, with just slight modification to this, you also, similar to what you saw in this regression, be able to compute the output of this neural network, not just a one example at a time, prolong your, say your entire training set at a time. So, let's see the details of that in the next video.\"), CellFormat(cell_type='markdown', cell_no=21, cell_content='## Lesson 04: Vectorizing Across Multiple Examples'), CellFormat(cell_type='markdown', cell_no=22, cell_content=\"In the last video, you saw how to compute the prediction on a neural network, given a single training example. In this video, you see how to vectorize across multiple training examples. And the outcome will be quite similar to what you saw for logistic regression. Whereby stacking up different training examples in different columns of the matrix, you'd be able to take the equations you had from the previous video. And with very little modification, change them to make the neural network compute the outputs on all the examples on pretty much all at the same time. So let's see the details on how to do that.\"), CellFormat(cell_type='markdown', cell_no=23, cell_content=\"These were the four equations we have from the previous video of how you compute z1, a1, z2 and a2. And they tell you how, given an input feature back to x, you can use them to generate a2 =y hat for a single training example. Now if you have m training examples, you need to repeat this process for say, the first training example. x superscript (1) to compute y hat 1 does a prediction on your first training example. Then x(2) use that to generate prediction y hat (2). And so on down to x(m) to generate a prediction y hat (m). And so in all these activation function notation as well, I'm going to write this as a[2](1). And this is a[2](2), and a(2)(m), so this notation a[2](i). The round bracket i refers to training example i, and the square bracket 2 refers to layer 2, okay. So that's how the square bracket and the round bracket indices work. And so to suggest that if you have an unvectorized implementation and want to compute the predictions of all your training examples, you need to do for i = 1 to m. Then basically implement these four equations, right? You need to make a z[1](i) = W(1) x(i) + b[1], a[1](i) = sigma of z[1](1). z[2](i) = w[2]a[1](i) + b[2] andZ2i equals w2a1i plus b2 and a[2](i) = sigma point of z[2](i). So it's basically these four equations on top by adding the superscript round bracket i to all the variables that depend on the training example. So adding this superscript round bracket i to x is z and a, if you want to compute all the outputs on your m training examples examples. What we like to do is vectorize this whole computation, so as to get rid of this for. And by the way, in case it seems like I'm getting a lot of nitty gritty linear algebra, it turns out that being able to implement this correctly is important in the deep learning era. And we actually chose notation very carefully for this course and make this vectorization steps as easy as possible. So I hope that going through this nitty gritty will actually help you to more quickly get correct implementations of these algorithms working. Alright, so let me just copy this whole block of code to the next slide and then we'll see how to vectorize this.\"), CellFormat(cell_type='markdown', cell_no=24, cell_content=\"So here's what we have from the previous slide with the for loop going over our m training examples. So recall that we defined the matrix x to be equal to our training examples stacked up in these columns like so. So take the training examples and stack them in columns. So this becomes a n, or maybe nx by m diminish the matrix. I'm just going to give away the punch line and tell you what you need to implement in order to have a vectorized implementation of this for loop. It turns out what you need to do is compute Z[1] = W[1] X + b[1], A[1]= sig point of z[1]. Then Z[2] = w[2] A[1] + b[2] and then A[2] = sig point of Z[2]. So if you want the analogy is that we went from lower case vector xs to just capital case X matrix by stacking up the lower case xs in different columns. If you do the same thing for the zs, so for example, if you take z[1](i), z[1](2), and so on, and these are all column vectors, up to z[1](m), right. So that's this first quantity that all m of them, and stack them in columns. Then just gives you the matrix z[1]. And similarly you look at say this quantity and take a[1](1), a[1](2) and so on and a[1](m), and stacked them up in columns. Then this, just as we went from lower case x to capital case X, and lower case z to capital case Z. This goes from the lower case a, which are vectors to this capital A[1], that's over there and similarly, for z[2] and a[2]. Right they're also obtained by taking these vectors and stacking them horizontally. And taking these vectors and stacking them horizontally, in order to get Z[2], and E[2]. One of the property of this notation that might help you to think about it is that this matrixes say Z and A, horizontally we're going to index across training examples. So that's why the horizontal index corresponds to different training example, when you sweep from left to right you're scanning through the training cells. And vertically this vertical index corresponds to different nodes in the neural network. So for example, this node, this value at the top most, top left most corner of the mean corresponds to the activation of the first heading unit on the first training example. One value down corresponds to the activation in the second hidden unit on the first training example, then the third heading unit on the first training sample and so on. So as you scan down this is your indexing to the hidden units number. Whereas if you move horizontally, then you're going from the first hidden unit. And the first training example to now the first hidden unit and the second training sample, the third training example. And so on until this node here corresponds to the activation of the first hidden unit on the final train example and the nth training example. Okay so the horizontally the matrix A goes over different training examples. And vertically the different indices in the matrix A corresponds to different hidden units. And a similar intuition holds true for the matrix Z as well as for X where horizontally corresponds to different training examples. And vertically it corresponds to different input features which are really different than those of the input layer of the neural network. So of these equations, you now know how to implement in your network with vectorization, that is vectorization across multiple examples. In the next video I want to show you a bit more justification about why this is a correct implementation of this type of vectorization. It turns out the justification would be similar to what you had seen in logistic regression. Let's go on to the next video.\"), CellFormat(cell_type='markdown', cell_no=25, cell_content='## Lesson 05: Explanation for Vectorized Implementation'), CellFormat(cell_type='markdown', cell_no=26, cell_content=\"In the previous video, we saw how with your training examples stacked up horizontally in the matrix x, you can derive a vectorized implementation for propagation through your neural network. Let's give a bit more justification for why the equations we wrote down is a correct implementation of vectorizing across multiple examples.\"), CellFormat(cell_type='markdown', cell_no=27, cell_content=\"So let's go through part of the forward propagation calculation for the few examples. Let's say that for the first training example, you end up computing this x1 plus b1 and then for the second training example, you end up computing this x2 plus b1 and then for the third training example, you end up computing this 3 plus b1. So, just to simplify the explanation on this slide, I'm going to ignore b. So let's just say, to simplify this justification a little bit that b is equal to zero. But the argument we're going to lay out will work with just a little bit of a change even when b is non-zero. It does just simplify the description on the slide a bit. Now, w1 is going to be some matrix, right? So I have some number of rows in this matrix. So if you look at this calculation x1, what you have is that w1 times x1 gives you some column vector which you must draw like this. And similarly, if you look at this vector x2, you have that w1 times x2 gives some other column vector, right? And that's gives you this z12. And finally, if you look at x3, you have w1 times x3, gives you some third column vector, that's this z13. So now, if you consider the training set capital X, which we form by stacking together all of our training examples. So the matrix capital X is formed by taking the vector x1 and stacking it vertically with x2 and then also x3. This is if we have only three training examples. If you have more, you know, they'll keep stacking horizontally like that. But if you now take this matrix x and multiply it by w then you end up with, if you think about how matrix multiplication works, you end up with the first column being these same values that I had drawn up there in purple. The second column will be those same four values. And the third column will be those orange values, what they turn out to be. But of course this is just equal to z11 expressed as a column vector followed by z12 expressed as a column vector followed by z13, also expressed as a column vector. And this is if you have three training examples. You get more examples then there'd be more columns. And so, this is just our matrix capital Z1. So I hope this gives a justification for why we had previously w1 times xi equals z1i when we're looking at single training example at the time. When you took the different training examples and stacked them up in different columns, then the corresponding result is that you end up with the z's also stacked at the columns. And I won't show but you can convince yourself if you want that with Python broadcasting, if you add back in, these values of b to the values are still correct. And what actually ends up happening is you end up with Python broadcasting, you end up having bi individually to each of the columns of this matrix. So on this slide, I've only justified that z1 equals w1x plus b1 is a correct vectorization of the first step of the four steps we have in the previous slide, but it turns out that a similar analysis allows you to show that the other steps also work on using a very similar logic where if you stack the inputs in columns then after the equation, you get the corresponding outputs also stacked up in columns.\"), CellFormat(cell_type='markdown', cell_no=28, cell_content=\"Finally, let's just recap everything we talked about in this video. If this is your neural network, we said that this is what you need to do if you were to implement for propagation, one training example at a time going from i equals 1 through m. And then we said, let's stack up the training examples in columns like so and for each of these values z1, a1, z2, a2, let's stack up the corresponding columns as follows. So this is an example for a1 but this is true for z1, a1, z2, and a2. Then what we show on the previous slide was that this line allows you to vectorize this across all m examples at the same time. And it turns out with the similar reasoning, you can show that all of the other lines are correct vectorizations of all four of these lines of code. And just as a reminder, because x is also equal to a0 because remember that the input feature vector x was equal to a0, so xi equals a0i. Then there's actually a certain symmetry to these equations where this first equation can also be written z1 = w1 a0 + b1. And so, you see that this pair of equations and this pair of equations actually look very similar but just of all of the indices advance by one. So this kind of shows that the different layers of a neural network are roughly doing the same thing or just doing the same computation over and over. And here we have two-layer neural network where we go to a much deeper neural network in next week's videos. You see that even deeper neural networks are basically taking these two steps and just doing them even more times than you're seeing here. So that's how you can vectorize your neural network across multiple training examples. Next, we've so far been using the sigmoid functions throughout our neural networks. It turns out that's actually not the best choice. In the next video, let's dive a little bit further into how you can use different, what's called, activation functions of which the sigmoid function is just one possible choice.\"), CellFormat(cell_type='markdown', cell_no=29, cell_content='## Lesson 06: Why Do You Need Non-Linear Activation Functions?'), CellFormat(cell_type='markdown', cell_no=30, cell_content=\"Why does a neural network need a non-linear activation function? Turns out that your neural network to compute interesting functions, you do need to pick a non-linear activation function, let's see one. So, here's the four prop equations for the neural network. Why don't we just get rid of this? Get rid of the function g? And set a1 equals z1. Or alternatively, you can say that g of z is equal to z, all right? Sometimes this is called the linear activation function. Maybe a better name for it would be the identity activation function because it just outputs whatever was input. For the purpose of this, what if a(2) was just equal z(2)? It turns out if you do this, then this model is just computing y or y-hat as a linear function of your input features, x, to take the first two equations.\"), CellFormat(cell_type='markdown', cell_no=31, cell_content=\"If you have that a(1) = Z(1) = W(1)x + b, and then a(2) = z (2) = W(2)a(1) + b. Then if you take this definition of a1 and plug it in there, you find that a2 = w2(w1x + b1), move that up a bit. Right? So this is a1 + b2, and so this simplifies to: (W2w1)x + (w2b1 + b2). So this is just, let's call this w prime b prime. So this is just equal to w' x + b'. If you were to use linear activation functions or we can also call them identity activation functions, then the neural network is just outputting a linear function of the input. And we'll talk about deep networks later, neural networks with many, many layers, many hidden layers. And it turns out that if you use a linear activation function or alternatively, if you don't have an activation function, then no matter how many layers your neural network has, all it's doing is just computing a linear activation function. So you might as well not have any hidden layers. Some of the cases that are briefly mentioned, it turns out that if you have a linear activation function here and a sigmoid function here, then this model is no more expressive than standard logistic regression without any hidden layer. So I won't bother to prove that, but you could try to do so if you want. But the take home is that a linear hidden layer is more or less useless because the composition of two linear functions is itself a linear function. So unless you throw a non-linear item in there, then you're not computing more interesting functions even as you go deeper in the network.\"), CellFormat(cell_type='markdown', cell_no=32, cell_content=\"There is just one place where you might use a linear activation function. g(x) = z. And that's if you are doing machine learning on the regression problem. So if y is a real number. So for example, if you're trying to predict housing prices. So y is not 0, 1, but is a real number, anywhere from - I don't know - $0 is the price of house up to however expensive, right, houses get, I guess. Maybe houses can be potentially millions of dollars, so however much houses cost in your data set. But if y takes on these real values, then it might be okay to have a linear activation function here so that your output y hat is also a real number going anywhere from minus infinity to plus infinity. But then the hidden units should not use the activation functions. They could use ReLU or tanh or Leaky ReLU or maybe something else. So the one place you might use a linear activation function is usually in the output layer. But other than that, using a linear activation function in the hidden layer except for some very special circumstances relating to compression that we're going to talk about using the linear activation function is extremely rare. And, of course, if we're actually predicting housing prices, as you saw in the week one video, because housing prices are all non-negative, Perhaps even then you can use a value activation function so that your output y-hats are all greater than or equal to 0. So I hope that gives you a sense of why having a non-linear activation function is a critical part of neural networks. Next we're going to start to talk about gradient descent and to do that to set up for our discussion for gradient descent, in the next video I want to show you how to estimate-how to compute-the slope or the derivatives of individual activation functions. So let's go on to the next video.\"), CellFormat(cell_type='markdown', cell_no=33, cell_content='## Lesson 07: Gradient Descent for Neural Networks'), CellFormat(cell_type='markdown', cell_no=34, cell_content=\"All right. I think this'll be an exciting video. In this video, you'll see how to implement gradient descent for your neural network with one hidden layer. In this video, I'm going to just give you the equations you need to implement in order to get back-propagation or to get gradient descent working, and then in the video after this one, I'll give some more intuition about why these particular equations are the accurate equations, are the correct equations for computing the gradients you need for your neural network.\"), CellFormat(cell_type='markdown', cell_no=35, cell_content=\"So, your neural network, with a single hidden layer for now, will have parameters W1, B1, W2, and B2. So, as a reminder, if you have NX or alternatively N0 input features, and N1 hidden units, and N2 output units in our examples. So far I've only had N2 equals one, then the matrix W1 will be N1 by N0. B1 will be an N1 dimensional vector, so we can write that as N1 by one-dimensional matrix, really a column vector. The dimensions of W2 will be N2 by N1, and the dimension of B2 will be N2 by one. Right, so far we've only seen examples where N2 is equal to one, where you have just one single hidden unit. So, you also have a cost function for a neural network. For now, I'm just going to assume that you're doing binary classification. So, in that case, the cost of your parameters as follows is going to be one over M of the average of that loss function. So, L here is the loss when your neural network predicts Y hat, right. This is really A2 when the gradient label is equal to Y. If you're doing binary classification, the loss function can be exactly what you use for logistic regression earlier. So, to train the parameters of your algorithm, you need to perform gradient descent. When training a neural network, it is important to initialize the parameters randomly rather than to all zeros. We'll see later why that's the case, but after initializing the parameter to something, each loop or gradient descents with computed predictions. So, you basically compute your Y hat I, for I equals one through M, say. Then, you need to compute the derivative. So, you need to compute DW1, and that's the derivative of the cost function with respect to the parameter W1, you can compute another variable, shall I call DB1, which is the derivative or the slope of your cost function with respect to the variable B1 and so on. Similarly for the other parameters W2 and B2. Then finally, the gradient descent update would be to update W1 as W1 minus Alpha. The learning rate times D, W1. B1 gets updated as B1 minus the learning rate, times DB1, and similarly for W2 and B2. Sometimes, I use colon equals and sometimes equals, as either notation works fine. So, this would be one iteration of gradient descent, and then you repeat this some number of times until your parameters look like they're converging.\"), CellFormat(cell_type='markdown', cell_no=36, cell_content=\"So, in previous videos, we talked about how to compute the predictions, how to compute the outputs, and we saw how to do that in a vectorized way as well. So, the key is to know how to compute these partial derivative terms, the DW1, DB1 as well as the derivatives DW2 and DB2. So, what I'd like to do is just give you the equations you need in order to compute these derivatives. I'll defer to the next video, which is an optional video, to go greater into Jeff about how we came up with those formulas.\"), CellFormat(cell_type='markdown', cell_no=37, cell_content=\"So, let me just summarize again the equations for propagation. So, you have Z1 equals W1X plus B1, and then A1 equals the activation function in that layer applied element wise as Z1, and then Z2 equals W2, A1 plus V2, and then finally, just as all vectorized across your training set, right? A2 is equal to G2 of Z2. Again, for now, if we assume we're doing binary classification, then this activation function really should be the sigmoid function, same just for that end neural. So, that's the forward propagation or the left to right for computation for your neural network.\"), CellFormat(cell_type='markdown', cell_no=38, cell_content=\"Next, let's compute the derivatives. So, this is the back propagation step. Then I compute DZ2 equals A2 minus the gradient of Y, and just as a reminder, all this is vectorized across examples. So, the matrix Y is this one by M matrix that lists all of your M examples stacked horizontally. Then it turns out DW2 is equal to this, and in fact, these first three equations are very similar to gradient descents for logistic regression. X is equals one, comma, keep dims equals true. Just a little detail this np.sum is a Python NumPy command for summing across one-dimension of a matrix. In this case, summing horizontally, and what keepdims does is, it prevents Python from outputting one of those funny rank one arrays, right? Where the dimensions was your N comma. So, by having keepdims equals true, this ensures that Python outputs for DB a vector that is N by one. In fact, technically this will be I guess N2 by one. In this case, it's just a one by one number, so maybe it doesn't matter. But later on, we'll see when it really matters. So, so far what we've done is very similar to logistic regression. But now as you continue to run back propagation, you will compute this, DZ2 times G1 prime of Z1. So, this quantity G1 prime is the derivative of whether it was the activation function you use for the hidden layer, and for the output layer, I assume that you are doing binary classification with the sigmoid function. So, that's already baked into that formula for DZ2, and his times is element-wise product. So, this here is going to be an N1 by M matrix, and this here, this element-wise derivative thing is also going to be an N1 by N matrix, and so this times there is an element-wise product of two matrices. Then finally, DW1 is equal to that, and DB1 is equal to this, and p.sum DZ1 axis equals one, keepdims equals true. So, whereas previously the keepdims maybe matter less if N2 is equal to one. Result is just a one by one thing, is just a real number. Here, DB1 will be a N1 by one vector, and so you want Python, you want Np.sons. I'll put something of this dimension rather than a funny rank one array of that dimension which could end up messing up some of your data calculations. The other way would be to not have to keep the parameters, but to explicitly reshape the output of NP.sum into this dimension, which you would like DB to have. So, that was forward propagation in I guess four equations, and back-propagation in I guess six equations. I know I just wrote down these equations, but in the next optional video, let's go over some intuitions for how the six equations for the back propagation algorithm were derived. Please feel free to watch that or not. But either way, if you implement these algorithms, you will have a correct implementation of forward prop and back prop. You'll be able to compute the derivatives you need in order to apply gradient descent, to learn the parameters of your neural network. It is possible to implement this algorithm and get it to work without deeply understanding the calculus. A lot of successful deep learning practitioners do so. But, if you want, you can also watch the next video, just to get a bit more intuition of what the derivation of these equations.\"), CellFormat(cell_type='markdown', cell_no=39, cell_content='## Lesson 08: Backpropagation Intuition (Optional)'), CellFormat(cell_type='markdown', cell_no=40, cell_content=\"In the last video, you saw the equations for back-propagation. In this video, let's go over some intuition using the computation graph for how those equations were derived. This video is completely optional so feel free to watch it or not. You should be able to do the whole works either way. Recall that when we talked about logistic regression, we had this forward pass where we compute z, then A, and then A loss and the to take derivatives we had this backward pass where we can first compute da and then go on to compute dz, and then go on to compute dw and db. The definition for the loss was L of a comma y equals negative y log A minus 1 minus y times log 1 minus A. If you're familiar with calculus and you take the derivative of this with respect to A that will give you the formula for da. So da is equal to that. If you actually figure out the calculus, you can show that this is negative y over A plus 1 minus y over one minus A. Just kind of derived that from calculus by taking derivatives of this. It turns out when you take another step backwards to compute dz, we then worked out that dz is equal to A minus y. I didn't explain why previously, but it turns out that from the chain rule of calculus, dz is equal to da times g prime of z. Where here g of z equals sigmoid of z as our activation function for this output unit in logistic regression. Just remember, this is still logistic regression, will have X_1, X_2, X_3, and then just one sigmoid unit, and then that gives us a, gives us y hat. Here the activation function was sigmoid function. As an aside, only for those of you familiar with the chain rule of calculus. The reason for this is because a is equal to sigmoid of z, and so partial of L with respect to z is equal to partial of L with respect to a times da, dz. Since A is equal to sigmoid of z. This is equal to d, dz g of z, which is equal to g prime of z. That's why this expression, which is dz in our code is equal to this expression, which is da in our code times g prime of z and so this just that. That last derivation would have made sense only if you're familiar with calculus and specifically the chain rule from calculus. But if not, don't worry about it, I'll try to explain the intuition wherever it's needed. Then finally, having computed dz for logistic regression, we will compute dw, which it turned out was dz times x and db which is just dz where you have a single training example. That was logistic regression.\"), CellFormat(cell_type='markdown', cell_no=41, cell_content=\"What we're going to do when computing back-propagation for a neural network is a calculation a lot like this, but only we'll do it twice. Because now we have not x going to an output unit, but x going to a hidden layer and then going to an output unit. Instead of this computation being one step as we have here, we'll have two steps here in this neural network with two layers. In this two-layer neural network, that is with the input layer, hidden layer, and an output layer. Remember the steps of a computation. First, you compute z_1 using this equation and then compute a_1, and then you compute z_2. Notice z_2 also depends on the parameters W_2 and b_2, and then based on z_2 you compute a_2. Then finally, that gives you the loss. What back-propagation does, is it will go backward to compute da_2 and then dz_2, then go back to compute dW_2 and db_2. Go back to compute da_1, dz_1, and so on. We don't need to take derivatives with respect to the input x, since input x for supervised learning because We're not trying to optimize x, so we won't bother to take derivatives, at least for supervised learning with respect to x. I'm going to skip explicitly computing da. If you want, you can actually compute da^2, and then use that to compute dz^2. But in practice, you could collapse both of these steps into one step. You end up that dz^2 is equal to a^2 minus y, same as before, and you have also going to write dw^2 and db^2 down here below. You have that dw^2 is equal to dz^2 times a^1 transpose, and db^2 equals dz^2. This step is quite similar for logistic regression, where we had that dw was equal to dz times x, except that now, a^1 plays the role of x, and there's an extra transpose there. Because the relationship between the capital matrix W and our individual parameters w was, there's a transpose there, because w is equal to a row vector. In the case of logistic regression with the single output, dw^2 is like that, whereas w here was a column vector. That's why there's an extra transpose for a^1, whereas we didn't for x here for logistic regression. This completes half of backpropagation.\"), CellFormat(cell_type='markdown', cell_no=42, cell_content=\"Then again, you can compute da^1, if you wish although in practice, the computation for da^1, and dz^1 are usually collapsed into one step. What you'd actually implement is that dz^1 is equal to w^2 transpose times dz^2 and then, times an element-wise product of g^1 prime of z^1. Just to do a check on the dimensions. If you have a neural network that looks like this, outputs y if so. If you have n^0 and x equals n^0, and for features, n^1 hidden units, and n^2 so far, and n^2 in our case, just one output unit, then the matrix w^2 is n^2 by n^1 dimensional, z^2, and therefore, dz^2 are going to be n^2 by one-dimensional. There's really going to be a one by one when we're doing binary classification, and z^1, and therefore also dz^1 are going to be n^1 by one-dimensional. Note that for any variable, foo and dfoo always have the same dimensions. That's why, w and dw always have the same dimension. Similarly, for b and db, and z and dz, and so on. To make sure that the dimensions of these all match up, we have that dz^1 is equal to w^2 transpose, times dz^2. Then, this is an element-wise product times g^1 prime of z^1. Mashing the dimensions from above, this is going to be n^1 by 1, is equal to w^2 transpose, we transpose of this. It is just going to be, n^1 by n^2-dimensional, dz^2 is going to be n^2 by one-dimensional. Then, this is same dimension as z^. This is also, n^1 by one-dimensional, so element-wise product. The dimensions do make sense. N^1 by one-dimensional vector can be obtained by n^1 by n^2 dimensional matrix, times n^2 by n^1, because the product of these two things gives you an n^1 by one-dimensional matrix. This becomes the element-wise product of 2, n^1 by one-dimensional vectors, so the dimensions do match up. One tip when implementing backprop, if you just make sure that the dimensions of your matrices match up, if you think through, what are the dimensions of your various matrices including w^1, w^2, z^1, z^2, a^1, a^2, and so on, and just make sure that the dimensions of these matrix operations may match up, sometimes that will already eliminate quite a lot of bugs in backprop. This gives us dz^1. Then finally, just to wrap up, dw^1 and db^1, we should write them here, I guess. But since I'm running out of space, I'll write them on the right of the slide, dw^1 and db^1 are given by the following formulas. This is going to equal to dz^1 times x transpose, and this is going to be equal to dz. You might notice a similarity between these equations and these equations, which is really no coincidence, because x plays the role of a^0. X transpose is a^0 transpose. Those equations are actually very similar. That gives a sense for how backpropagation is derived. We have six key equations here for dz_2, dw_2, db_2, dz_1, dw_1, and db_1. Let me just take these six equations and copy them over to the next slide. Here they are. So far we've derived that propagation for training on a single training example at a time. But it should come as no surprise that rather than working on a single example at a time, we would like to vectorize across different training examples. You remember that for a propagation when we're operating on one example at a time, we had equations like this, as well as say a^1 equals g^1 plus z^1. In order to vectorize, we took say, the z's and stack them up in columns like this, z^1m, and call this capital Z. Then we found that by stacking things up in columns and defining the capital uppercase version of these, we then just had z^1 equals to the w^1x plus b and a^1 equals g^1 of z^1. We defined the notation very carefully in this course to make sure that stacking examples into different columns of a matrix makes all this workout. It turns out that if you go through the math carefully, the same trick also works for backpropagation. The vectorized equations are as follows. First, if you take this dzs for different training examples and stack them as different columns of a matrix, same for this, same for this. Then this is the vectorized implementation. Here's how you can compute dW^2. There is this extra 1 over n because the cost function J is this 1 over m of the sum from I equals 1 through m of the losses. When computing derivatives, we have that extra 1 over m term, just as we did when we were computing the weight updates for logistic regression. That's the update you get for db^2, again, some of the dz's. Then, we have 1 over m. Dz^1 is computed as follows. Once again, this is an element-wise product only, whereas previously, we saw on the previous slide that this was an n1 by one-dimensional vector. No w, this is n1 by m dimensional matrix. Both of these are also n1 by m dimensional. That's why that asterisk is the element-wise product. Finally, the remaining two updates perhaps shouldn't look too surprising. I hope that gives you some intuition for how the backpropagation algorithm is derived. In all of machine learning, I think the derivation of the backpropagation algorithm is actually one of the most complicated pieces of math I've seen. It requires knowing both linear algebra as well as the derivative of matrices to really derive it from scratch from first principles. If you are an expert in matrix calculus, using this process, you might want to derive the algorithm yourself. But I think that there actually plenty of deep learning practitioners that have seen the derivation at about the level you've seen in this video and are already able to have all the right intuitions and be able to implement this algorithm very effectively. If you are an expert in calculus do see if you can derive the whole thing from scratch. It is one of the hardest pieces of math on the very hardest derivations that I've seen in all of machine learning. But either way, if you implement this, this will work and I think you have enough intuitions to tune in and get it to work. There's just one last detail, my share of you before you implement your neural network, which is how to initialize the weights of your neural network. It turns out that initializing your parameters not to zero, but randomly turns out to be very important for training your neural network. In the next video, you'll see why.\"), CellFormat(cell_type='markdown', cell_no=43, cell_content='## Lesson 09: Random Initialization'), CellFormat(cell_type='markdown', cell_no=44, cell_content=\"When you change your neural network, it's important to initialize the weights randomly. For logistic regression, it was okay to initialize the weights to zero. But for a neural network of initialize the weights to parameters to all zero and then applied gradient descent, it won't work. Let's see why.\"), CellFormat(cell_type='markdown', cell_no=45, cell_content=\"So you have here two input features, so n0=2, and two hidden units, so n1=2. And so the matrix associated with the hidden layer, w 1, is going to be two-by-two. Let's say that you initialize it to all 0s, so 0 0 0 0, two-by-two matrix. And let's say B1 is also equal to 0 0. It turns out initializing the bias terms b to 0 is actually okay, but initializing w to all 0s is a problem. So the problem with this formalization is that for any example you give it, you'll have that a1,1 and a1,2, will be equal, right? So this activation and this activation will be the same, because both of these hidden units are computing exactly the same function. And then, when you compute backpropagation, it turns out that dz11 and dz12 will also be the same colored by symmetry, right? Both of these hidden units will initialize the same way. Technically, for what I'm saying, I'm assuming that the outgoing weights or also identical. So that's w2 is equal to 0 0. But if you initialize the neural network this way, then this hidden unit and this hidden unit are completely identical. Sometimes you say they're completely symmetric, which just means that they're completing exactly the same function. And by kind of a proof by induction, it turns out that after every single iteration of training your two hidden units are still computing exactly the same function. Since plots will show that dw will be a matrix that looks like this. Where every row takes on the same value. So we perform a weight update. So when you perform a weight update, w1 gets updated as w1- alpha times dw. You find that w1, after every iteration, will have the first row equal to the second row. So it's possible to construct a proof by induction that if you initialize all the ways, all the values of w to 0, then because both hidden units start off computing the same function. And both hidden the units have the same influence on the output unit, then after one iteration, that same statement is still true, the two hidden units are still symmetric. And therefore, by induction, after two iterations, three iterations and so on, no matter how long you train your neural network, both hidden units are still computing exactly the same function. And so in this case, there's really no point to having more than one hidden unit. Because they are all computing the same thing. And of course, for larger neural networks, let's say of three features and maybe a very large number of hidden units, a similar argument works to show that with a neural network like this. Let me draw all the edges, if you initialize the weights to zero, then all of your hidden units are symmetric. And no matter how long you're upgrading the center, all continue to compute exactly the same function. So that's not helpful, because you want the different hidden units to compute different functions.\"), CellFormat(cell_type='markdown', cell_no=46, cell_content=\"The solution to this is to initialize your parameters randomly. So here's what you do. You can set w1 = np.random.randn. This generates a gaussian random variable (2,2). And then usually, you multiply this by very small number, such as 0.01. So you initialize it to very small random values. And then b, it turns out that b does not have the symmetry problem, what's called the symmetry breaking problem. So it's okay to initialize b to just zeros. Because so long as w is initialized randomly, you start off with the different hidden units computing different things. And so you no longer have this symmetry breaking problem. And then similarly, for w2, you're going to initialize that randomly. And b2, you can initialize that to 0. So you might be wondering, where did this constant come from and why is it 0.01? Why not put the number 100 or 1000? Turns out that we usually prefer to initialize the weights to very small random values. Because if you are using a tanh or sigmoid activation function, or the other sigmoid, even just at the output layer. If the weights are too large, then when you compute the activation values, remember that z[1]=w1 x + b. And then a1 is the activation function applied to z1. So if w is very big, z will be very, or at least some values of z will be either very large or very small. And so in that case, you're more likely to end up at these fat parts of the tanh function or the sigmoid function, where the slope or the gradient is very small. Meaning that gradient descent will be very slow. So learning was very slow. So just a recap, if w is too large, you're more likely to end up even at the very start of training, with very large values of z. Which causes your tanh or your sigmoid activation function to be saturated, thus slowing down learning. If you don't have any sigmoid or tanh activation functions throughout your neural network, this is less of an issue. But if you're doing binary classification, and your output unit is a sigmoid function, then you just don't want the initial parameters to be too large. So that's why multiplying by 0.01 would be something reasonable to try, or any other small number. And same for w2, right? This can be random.random. I guess this would be 1 by 2 in this example, times 0.01. Missing an s there. So finally, it turns out that sometimes they can be better constants than 0.01. When you're training a neural network with just one hidden layer, it is a relatively shallow neural network, without too many hidden layers. Set it to 0.01 will probably work okay. But when you're training a very very deep neural network, then you might want to pick a different constant than 0.01. And in next week's material, we'll talk a little bit about how and when you might want to choose a different constant than 0.01. But either way, it will usually end up being a relatively small number. So that's it for this week's videos. You now know how to set up a neural network of a hidden layer, initialize the parameters, make predictions using. As well as compute derivatives and implement gradient descent, using backprop. So that, you should be able to do the quizzes, as well as this week's programming exercises. Best of luck with that. I hope you have fun with the problem exercise, and look forward to seeing you in the week four materials.\"), CellFormat(cell_type='markdown', cell_no=47, cell_content='## Lesson 10: Ian Goodfellow Interview'), CellFormat(cell_type='markdown', cell_no=48, cell_content=\"Hi, Ian. Thanks a lot for joining us today. Thank you for inviting me, Andrew. I am glad to be here. Today, you are one of the world's most visible deep learning researchers. Let us share a bit about your personal story. So, how do you end up doing this work that you now do? Yeah. That sounds great. I guess I first became interested in machine learning right before I met you, actually. I had been working on neuroscience and my undergraduate adviser, Jerry Cain, at Stanford encouraged me to take your Intro to AI class. Oh, I didn't know that. Okay. So I had always thought that AI was a good idea, but that in practice, the main, I think, idea that was happening was like game AI, where people have a lot of hard-coded rules for non-player characters in games to say different scripted lines at different points in time. And then, when I took your Intro to AI class and you covered topics like linear regression and the variance decomposition of the error of linear regression, I started to realize that this is a real science and I could actually have a scientific career in AI rather than neuroscience. I see. Great. And then what happened? Well, I came back and I was the TA to your course later. Oh, I see. Right. Like a TA. So a really big turning point for me was while I was TA-ing that course, one of the students, my friend Ethan Dreifuss, got interested in Geoff Hinton's deep belief net paper. I see. And the two of us ended up building one of the first GPU CUDA-based machines at Stanford in order to run Watson machines in our spare time over winter break. I see. And at that point, I started to have a very strong intuition that deep learning was the way to go in the future, that a lot of the other algorithms that I was working with, like support vector machines, didn't seem to have the right asymptotics, that you add more training data and they get slower, or for the same amount of training data, it's hard to make them perform a lot better by changing other settings. At that point, I started to focus on deep learning as much as possible. And I remember Richard Reyna's very old GPU paper acknowledges you for having done a lot of early work. Yeah. Yeah. That was written using some of the machines that we built. Yeah. The first machine I built was just something that Ethan and I built at Ethan's mom's house with our own money, and then later, we used lab money to build the first two or three for the Stanford lab. Wow that's great. I never knew that story. That's great. And then, today, one of the things that's really taken the deep learning world by storm is your invention of GANs. So how did you come up with that? I've been studying generative models for a long time, so GANs are a way of doing generative modeling where you have a lot of training data and you'd like to learn to produce more examples that resemble the trading data, but they're imaginary. They've never been seen exactly in that form before. There were several other ways of doing generative models that had been popular for several years before I had the idea for GANs. And after I'd been working on all those other methods throughout most of my Ph.D., I knew a lot about the advantages and disadvantages of all the other frameworks like Boltzmann machines and sparse coding and all the other approaches that have been really popular for years. I was looking for something that avoid all these disadvantages at the same time. And then finally, when I was arguing about generative models with my friends in a bar, something clicked into place, and I started telling them, You need to do, this, this, and this and I swear it will work. And my friends didn't believe me that it would work. I was supposed to be writing the deep learning textbook at the time, I see. But I believed strongly enough that it would work that I went home and coded it up the same night and it worked. So it take you one evening to implement the first version of GANs? I implemented it around midnight after going home from the bar where my friend had his going-away party. I see. And the first version of it worked, which is very, very fortunate. I didn't have to search for hyperparameters or anything. There was a story, I read it somewhere, where you had a near-death experience and that reaffirmed your commitment to AI. Tell me that one. So, yeah. I wasn't actually near death but I briefly thought that I was. I had a very bad headache and some of the doctors thought that I might have a brain hemorrhage. And during the time that I was waiting for my MRI results to find out whether I had a brain hemorrhage or not, I realized that most of the thoughts I was having were about making sure that other people would eventually try out the research ideas that I had at the time. I see. I see. In retrospect, they're all pretty silly research ideas. I see. But at that point, I realized that this was actually one of my highest priorities in life, was carrying out my machine learning research work. I see. Yeah. That's great, that when you thought you might be dying soon, you're just thinking how to get the research done. Yeah. Yeah. That's commitment. Yeah. Yeah. So today, you're still at the center of a lot of the activities with GANs, with Generative Adversarial Networks. So tell me how you see the future of GANs. Right now, GANs are used for a lot of different things, like semi-supervised learning, generating training data for other models and even simulating scientific experiments. In principle, all of these things could be done by other kinds of generative models. So I think that GANs are at an important crossroads right now. Right now, they work well some of the time, but it can be more of an art than a science to really bring that performance out of them. It's more or less how people felt about deep learning in general 10 years ago. And back then, we were using deep belief networks with Boltzmann machines as the building blocks, and they were very, very finicky. Over time, we switched to things like rectified linear units and batch normalization, and deep learning became a lot more reliable. If we can make GANs become as reliable as deep learning has become, then I think we'll keep seeing GANs used in all the places they're used today with much greater success. If we aren't able to figure out how to stabilize GANs, then I think their main contribution to the history of deep learning is that they will have shown people how to do all these tasks that involve generative modeling, and eventually, we'll replace them with other forms of generative models. So I spend maybe about 40 percent of my time right now working on stabilizing GANs. I see. Cool. Okay. Oh, and so just as a lot of people that joined deep learning about 10 years ago, such as yourself, wound up being pioneers, maybe the people that join GANs today, if it works out, could end up the early pioneers. Yeah. A lot of people already are early pioneers of GANs, and I think if you wanted to give any kind of history of GANs so far, you'd really need to mention other groups like Indico and Facebook and Berkeley for all the different things that they've done. So in addition to all your research, you also coauthored a book on deep learning. How is that going? That's right, with Yoshua Bengio and Aaron Courville, who are my Ph.D. co-advisers. We wrote the first textbook on the modern version of deep learning, and that has been very popular, both in the English edition and the Chinese edition. We've sold about, I think around 70,000 copies total between those two languages. And I've had a lot of feedback from students who said that they've learned a lot from it. One thing that we did a little bit differently than some other books is we start with a very focused introduction to the kind of math that you need to do in deep learning. I think one thing that I got from your courses at Stanford is that linear algebra and probability are very important, that people get excited about the machine learning algorithms, but if you want to be a really excellent practitioner, you've got to master the basic math that underlies the whole approach in the first place. So we make sure to give a very focused presentation of the math basics at the start of the book. That way, you don't need to go ahead and learn all that linear algebra, that you can get a very quick crash course in the pieces of linear algebra that are the most useful for deep learning. So even someone whose math is a little shaky or haven't seen the math for a few years will be able to start from the beginning of your book and get that background and get into deep learning? All of the facts that you would need to know are there. It would definitely take some focused effort to practice making use of them. Yeah. Yeah. Great. If someone's really afraid of math, it might be a bit of a painful experience. But if you're ready for the learning experience and you believe you can master it, I think all the tools that you need are there. As someone that worked in deep learning for a long time, I'd be curious, if you look back over the years. Tell me a bit about how you're thinking of AI and deep learning has evolved over the years. Ten years ago, I felt like, as a community, the biggest challenge in machine learning was just how to get it working for AI-related tasks at all. We had really good tools that we could use for simpler tasks, where we wanted to recognize patterns in how to extract features, where a human designer could do a lot of the work by creating those features and then hand it off to the computer. Now, that was really good for different things like predicting which ads a user would click on or different kinds of basic scientific analysis. But we really struggled to do anything involving millions of pixels in an image or a raw audio wave form where the system had to build all of its understanding from scratch. We finally got over the hurdle really thoroughly maybe five years ago. And now, we're at a point where there are so many different paths open that someone who wants to get involved in AI, maybe the hardest problem they face is choosing which path they want to go down. Do you want to make reinforcement learning work as well as supervised learning works? Do you want to make unsupervised learning work as well as supervised learning works? Do you want to make sure that machine learning algorithms are fair and don't reflect biases that we'd prefer to avoid? Do you want to make sure that the societal issues surrounding AI work out well, that we're able to make sure that AI benefits everyone rather than causing social upheaval and trouble with loss of jobs? I think right now, there's just really an amazing amount of different things that can be done, both to prevent downsides from AI but also to make sure that we leverage all of the upsides that it offers us. And so today, there are a lot of people wanting to get into AI. So, what advice would you have for someone like that? I think a lot of people that want to get into AI start thinking that they absolutely need to get a Ph.D. or some other kind of credential like that. I don't think that's actually a requirement anymore. One way that you could get a lot of attention is to write good code and put it on GitHub. If you have an interesting project that solves a problem that someone working at the top level wanted to solve, once they find your GitHub repository, they'll come find you and ask you to come work there. A lot of the people that I've hired or recruited at OpenAI last year or at Google this year, I first became interested in working with them because of something that I saw that they released in an open-source forum on the Internet. Writing papers and putting them on Archive can also be good. A lot of the time, it's harder to reach the point where you have something polished enough to really be a new academic contribution to the scientific literature, but you can often get to the point of having a useful software product much earlier. So read your book, practice the materials and post on GitHub and maybe on Archive. I think if you learned by reading the book, it's really important to also work on a project at the same time, to either choose some way of applying machine learning to an area that you are already interested in. Like if you're a field biologist and you want to get into deep learning, maybe you could use it to identify birds, or if you don't have an idea for how you'd like to use machine learning in your own life, you could pick something like making a Street View house numbers classifier, where all the data sets are set up to make it very straightforward for you. And that way, you get to exercise all of the basic skills while you read the book or while you watch Coursera videos that explain the concepts to you. So over the last couple of years, I've also seen you do one more work on adversarial examples. Tell us a bit about that. Yeah. I think adversarial examples are the beginning of a new field that I call machine learning security. In the past, we've seen computer security issues where attackers could fool a computer into running the wrong code. That's called application-level security. And there's been attacks where people can fool a computer into believing that messages on a network come from somebody that is not actually who they say they are. That's called network-level security. Now, we're starting to see that you can also fool machine-learning algorithms into doing things they shouldn't, even if the program running the machine-learning algorithm is running the correct code, even if the program running the machine-learning algorithm knows who all the messages on the network really came from. And I think, it's important to build security into a new technology near the start of its development. We found that it's very hard to build a working system first and then add security later. So I am really excited about the idea that if we dive in and start anticipating security problems with machine learning now, we can make sure that these algorithms are secure from the start instead of trying to patch it in retroactively years later. Thank you. That was great. There's a lot about your story that I thought was fascinating and that, despite having known you for years, I didn't actually know, so thank you for sharing all that. Oh, very welcome. Thank you for inviting me. It was a great shot. Okay. Thank you. Very welcome.\")])},\n",
       "  {'chapter_number': '4',\n",
       "   'chapter_name': 'Deep_neural_networks',\n",
       "   'lessons': [{'lesson_number': '01',\n",
       "     'lesson_name': 'deep-l-layer-neural-network',\n",
       "     'content': 'Welcome to the fourth week of this course.\\nBy now, you\\'ve seen forward propagation and back propagation in the context\\nof a neural network, with a single hidden layer, as well as logistic regression, and\\nyou\\'ve learned about vectorization, and\\nwhen it\\'s important to initialize the ways randomly.\\nIf you\\'ve done the past couple weeks homework, you\\'ve also implemented and\\nseen some of these ideas work for yourself.\\nSo by now,\\nyou\\'ve actually seen most of the ideas you need to implement a deep neural network.\\nWhat we\\'re going to do this week, is take those ideas and put them together so\\nthat you\\'ll be able to implement your own deep neural network.\\nBecause this week\\'s problem exercise is longer,\\nit just has been more work, I\\'m going to keep the videos for\\nthis week shorter as you can get through the videos a little bit more quickly, and\\nthen have more time to do a significant problem exercise at then end, which I hope\\nwill leave you having thoughts deep in neural network, that if you feel proud of.\\nSo what is a deep neural network?\\nYou\\'ve seen this picture for logistic regression and\\nyou\\'ve also seen neural networks with a single hidden layer.\\nSo here\\'s an example of a neural network with two hidden layers and\\na neural network with 5 hidden layers.\\nWe say that logistic regression is a very \"shallow\" model,\\nwhereas this model here is a much deeper model, and\\nshallow versus depth is a matter of degree.\\nSo neural network of a single hidden layer,\\nthis would be a 2 layer neural network.\\nRemember when we count layers in a neural network, we don\\'t count the input layer,\\nwe just count the hidden layers as was the output layer.\\nSo, this would be a 2 layer neural network is still quite shallow,\\nbut not as shallow as logistic regression.\\nTechnically logistic regression is a one layer neural network,\\nwe could then, but over the last several years the AI,\\non the machine learning community, has realized that there are functions that\\nvery deep neural networks can learn that shallower models are often unable to.\\nAlthough for any given problem, it might be hard to predict in advance exactly how\\ndeep in your network you would want.\\nSo it would be reasonable to try logistic regression, try one and\\nthen two hidden layers, and view the number of hidden layers as another hyper\\nparameter that you could try a variety of values of, and\\nevaluate on all that across validation data, or on your development set.\\nSee more about that later as well.\\nLet\\'s now go through the notation we used to describe deep neural networks.\\nHere\\'s is a one, two, three, four layer neural network,\\nWith three hidden layers, and the number of units in these hidden\\nlayers are I guess 5, 5, 3, and then there\\'s one one upper unit.\\nSo the notation we\\'re going to use,\\nis going to use capital L ,to denote the number of layers in the network.\\nSo in this case, L = 4, and so does the number of layers, and\\nwe\\'re going to use N superscript [l] to denote the number of nodes,\\nor the number of units in layer lowercase l.\\nSo if we index this, the input as layer \"0\".\\nThis is layer 1, this is layer 2, this is layer 3, and this is layer 4.\\nThen we have that, for example, n[1], that would be this,\\nthe first is in there will equal 5, because we have 5 hidden units there.\\nFor this one, we have the n[2],\\nthe number of units in the second hidden layer\\nis also equal to 5, n[3] = 3, and\\nn[4] = n[L] this number of upper units is 01,\\nbecause your capital L is equal to four,\\nand we\\'re also going to have here that for\\nthe input layer n[0] = nx = 3.\\nSo that\\'s the notation we use to describe the number of nodes we have in different\\nlayers.\\nFor each layer L, we\\'re also going to use\\na[l] to denote the activations in layer l.\\nSo we\\'ll see later that in for propagation,\\nyou end up computing a[l] as the activation g(z[l]) and\\nperhaps the activation is indexed by the layer l as well,\\nand then we\\'ll use W[l ]to denote, the weights for\\ncomputing the value z[l] in layer l, and\\nsimilarly, b[l] is used to compute z [l].\\nFinally, just to wrap up on the notation, the input features are called x,\\nbut x is also the activations of layer zero, so a[0] = x,\\nand the activation of the final layer, a[L] = y-hat.\\nSo a[L] is equal to the predicted output to prediction y-hat to the neural network.\\nSo you now know what a deep neural network looks like,\\nas was the notation we\\'ll use to describe and to compute with deep networks.\\nI know we\\'ve introduced a lot of notation in this video, but if you ever forget\\nwhat some symbol means, we\\'ve also posted on the course website, a notation sheet or\\na notation guide, that you can use to look up what these different symbols mean.\\nNext, I\\'d like to describe what forward propagation in this type of network\\nlooks like.\\nLet\\'s go into the next video.'},\n",
       "    {'lesson_number': '02',\n",
       "     'lesson_name': 'forward-propagation-in-a-deep-network',\n",
       "     'content': \"In the last video, we described what is\\na deep L-layer neural network and also talked\\nabout the notation we use to describe such networks.\\nIn this video, you see how you can perform forward propagation,\\nin a deep network.\\nAs usual, let's first go over\\nwhat forward propagation will look like for a single training example x,\\nand then later on we'll talk about the vectorized version,\\nwhere you want to carry out forward propagation\\non the entire training set at the same time.\\nBut given a single training example x,\\nhere's how you compute the activations of the first layer.\\nSo for this first layer,\\nyou compute z1 equals\\nw1 times x plus b1.\\nSo w1 and b1 are the parameters that affect the activations in layer one.\\nThis is layer one of the neural network,\\nand then you compute the activations for that layer to be equal to g of z1.\\nThe activation function g depends on what layer you're at and\\nmaybe what index set as the activation function from layer one.\\nSo if you do that, you've now computed the activations for layer one.\\nHow about layer two? Say that layer.\\nWell, you would then compute z2 equals\\nw2 a1 plus b2.\\nThen, so the activation of layer two is the y matrix times the outputs of layer one.\\nSo, it's that value,\\nplus the bias vector for layer two.\\nThen a2 equals the activation function applied to z2.\\nOkay? So that's it for layer two,\\nand so on and so forth.\\nUntil you get to the upper layer, that's layer four.\\nWhere you would have that z4 is equal\\nto the parameters for that layer times the activations from the previous layer,\\nplus that bias vector.\\nThen similarly, a4 equals g of z4.\\nSo, that's how you compute your estimated output, y hat.\\nSo, just one thing to notice,\\nx here is also equal to a0,\\nbecause the input feature vector x is also the activations of layer zero.\\nSo we scratch out x.\\nWhen I cross out x and put a0 here,\\nthen all of these equations basically look the same.\\nThe general rule is that zl is equal to\\nwl times a of l minus 1 plus bl.\\nIt's one there. And then,\\nthe activations for that layer is\\nthe activation function applied to the values of z.\\nSo, that's the general forward propagation equation.\\nSo, we've done all this for a single training example.\\nHow about for doing it in a vectorized way for the whole training set at the same time?\\nThe equations look quite similar as before.\\nFor the first layer, you would have capital Z1 equals\\nw1 times capital X plus b1.\\nThen, A1 equals g of Z1.\\nBear in mind that X is equal to A0.\\nThese are just the training examples stacked in different columns.\\nYou could take this, let me scratch out X,\\nthey can put A0 there.\\nThen for the next layer, looks similar,\\nZ2 equals w2\\nA1 plus b2 and A2 equals g of Z2.\\nWe're just taking these vectors z or a and so on,\\nand stacking them up.\\nThis is z vector for the first training example,\\nz vector for the second training example,\\nand so on, down to the nth training example,\\nstacking these and columns and calling this capital Z.\\nSimilarly, for capital A,\\njust as capital X.\\nAll the training examples are column vectors stack left to right.\\nIn this process, you end up with y hat which is equal to g of Z4,\\nthis is also equal to A4.\\nThat's the predictions on all of your training examples stacked horizontally.\\nSo just to summarize on notation,\\nI'm going to modify this up here.\\nA notation allows us to replace lowercase z and a with the uppercase counterparts,\\nis that already looks like a capital Z.\\nThat gives you the vectorized version of\\nforward propagation that you carry out on the entire training set at a time,\\nwhere A0 is X.\\nNow, if you look at this implementation of vectorization,\\nit looks like that there is going to be a For loop here.\\nSo therefore l equals 1-4.\\nFor L equals 1 through capital L. Then you have to compute the activations for layer one,\\nthen layer two, then for layer three,\\nand then the layer four.\\nSo, seems that there is a For loop here.\\nI know that when implementing neural networks,\\nwe usually want to get rid of explicit For loops.\\nBut this is one place where I don't think\\nthere's any way to implement this without an explicit For loop.\\nSo when implementing forward propagation,\\nit is perfectly okay to have a For loop to compute the activations for layer one,\\nthen layer two, then layer three, then layer four.\\nNo one knows, and I don't think there is any way to do\\nthis without a For loop that goes from one to capital L,\\nfrom one through the total number of layers in the neural network.\\nSo, in this place, it's perfectly okay to have an explicit For loop.\\nSo, that's it for the notation for deep neural networks,\\nas well as how to do forward propagation in these networks.\\nIf the pieces we've seen so far looks a little bit familiar to you,\\nthat's because what we're seeing is taking a piece very similar to what you've seen in\\nthe neural network with a single hidden layer and just repeating that more times.\\nNow, it turns out that we implement a deep neural network,\\none of the ways to increase your odds of having a bug-free implementation\\nis to think very systematic and\\ncarefully about the matrix dimensions you're working with.\\nSo, when I'm trying to debug my own code,\\nI'll often pull a piece of paper,\\nand just think carefully through,\\nso the dimensions of the matrix I'm working with.\\nLet's see how you could do that in the next video.\"},\n",
       "    {'lesson_number': '03',\n",
       "     'lesson_name': 'getting-your-matrix-dimensions-right',\n",
       "     'content': \"When implementing a deep neural network,\\none of the debugging tools I often\\nuse to check the correctness of my code\\nis to pull a piece of paper and just\\nwork through the dimensions in matrix I'm working with.\\nLet me show you how to do that since I hope this\\nwill make it easier for you\\nto implement your deep networks as well.\\nSo capital L is equal to 5.\\nI counted them quickly.\\nNot counting the input layer,\\nthere are five layers here,\\nfour hidden layers and one output layer.\\nIf you implement forward propagation,\\nthe first step will be Z1 equals\\nW1 times the input features x plus b1.\\nLet's ignore the bias terms B\\nfor now and focus on the parameters W. Now,\\nthis first hidden layer has three hidden units.\\nSo this is Layer 0,\\nLayer 1, Layer 2, Layer 3,\\nLayer 4, and Layer 5.\\nUsing the notation we had from the previous video,\\nwe have that n1, which is\\nthe number of hidden units in layer 1,\\nis equal to 3.\\nHere we would have that n2 is equal to 5,\\nn3 is equal to 4,\\nn4 is equal to 2,\\nand n5 is equal to 1.\\nSo far we've only seen\\nneural networks with a single output unit,\\nbut in later courses we'll talk about\\nneural networks with multiple output units as well.\\nFinally, for the input layer,\\nwe also have n0 equals nX is equal to 2.\\nNow, let's think about the dimensions of Z, W, and X.\\nZ is the vector of activations\\nfor this first hidden layer.\\nSo Z is going to be 3 by 1,\\nis going to be a three-dimensional vector.\\nI'm going to write it as, n1 by one-dimensional matrix,\\n3 by 1 in this case.\\nNow, how about the input features x?\\nX we have two input features.\\nSo x is, in this example,\\n2 by 1, but more generally it'll be n0 by 1.\\nWhat we need is for the matrix W1 to be something\\nthat when we multiply an n0 by 1 vector to it,\\nwe get an n1 by 1 vector.\\nSo you have a three-dimensional vector\\nequals something times a two-dimensional vector.\\nBy the rules of matrix multiplication,\\nthis has got to be a 3 by 2 matrix.\\nBecause a 3 by 2 matrix times a 2 by\\n1 matrix or times a 2 by 1 vector,\\nthat gives you a 3 by 1 vector.\\nMore generally, this is going to be\\nan n1 by n0 dimensional matrix.\\nSo what we figured out here is that the dimensions of W1\\nhas to be n1 by n0,\\nand more generally, the dimensions of\\nWL must be nL by nL minus 1.\\nFor example, the dimensions of W2,\\nfor this, it will have to be 5 by 3,\\nor it will be n2 by n1,\\nbecause we're going to compute Z2\\nas W2 times a1.\\nAgain, let's ignore the bias for now.\\nThis is going to be 3 by 1.\\nWe need this to be 5 by 1.\\nSo this had better be 5 by 3.\\nSimilarly, W3 is really the dimension of the next layer,\\nthe dimension of the previous layer.\\nSo this is going to be 4 by 5.\\nW4 is going to\\nbe 2 by 4,\\nand W5 is going to be 1 by 2.\\nThe general formula to check is that when you're\\nimplementing the matrix for a layer L,\\nthat the dimension of that matrix be nL by nL minus 1.\\nNow, let's think about the dimension of this vector B.\\nThis is going to be a 3 by 1 vector,\\nso you have to add that to another 3 by\\n1 vector in order to get a 3 by 1 vector as the output.\\nThis was going to be 5 by 1,\\nso there's going to be another 5 by 1 vector\\nin order for the sum\\nof these two things that I have in the boxes to\\nbe itself a 5 by 1 vector.\\nThe more general rule is that in the example on the left,\\nb^[1] is n^[1] by 1,\\nlike this 3 by 1.\\nIn the second example,\\nit is this is n^[2] by 1 and so\\nthe more general case is that b^[l]\\nshould be n^[l] by 1 dimensional.\\nHopefully, these two equations help you to\\ndouble-check that the dimensions of your matrices,\\nw, as well as of\\nyour vectors b are the correct dimensions.\\nOf course, if you're implementing back-propagation,\\nthen the dimensions of\\ndw should be the same as dimension of\\nw. So dw should be the same dimension as w,\\nand db should be the same dimension as b.\\nNow, the other key set of quantities\\nwhose dimensions to check are these z,\\nx, as well as a of l,\\nwhich we didn't talk too much about here.\\nBut because z of l is equal to g of a of l,\\napply element-wise then z and\\na should have the same dimension\\nin these types of networks.\\nNow, let's see what happens when you have\\na vectorized implementation that\\nlooks at multiple examples at a time.\\nEven for a vectorized implementation, of course,\\nthe dimensions of w,\\nb, dw, and db will stay the same.\\nBut the dimensions of za,\\nas well as x, will change a bit\\nin your vectorized implementation.\\nPreviously we had z^[1] equals\\nw^[1] times x plus b^]1],\\nwhere this was n^[1] by 1.\\nThis was n^[1] by n^[0],\\nx was n^[0] by 1,\\nand b was n^[1] by 1.\\nNow, in a vectorized implementation,\\nyou would have z^[1] equals w^[1] times x plus b^[1].\\nWhere now z^[1] is obtained by\\ntaking the z^[1] for the individual examples.\\nSo there's z^[1][1], z^[1][2] up to z^[1][m] and\\nstacking them as follows and this gives you z^[1].\\nThe dimension of z^[1] is that\\ninstead of being n^[1] by 1,\\nit ends up being n^[1] by m,\\nif m is decisive training set.\\nThe dimensions of w^[1] stays the same\\nso is the n^[1] by n^[0] and\\nx instead of being n^[0] by\\n1 is now all your training examples stamped horizontally,\\nso it's now n^[0] by m. You notice that when you take a,\\nn^[1] by n^[0] matrics and\\nmultiply that by an n^[0] by m matrics\\nthat together they actually give you an\\nn^[1] by m dimensional matrics as expected.\\nNow the final detail is that b^[1] is still n^[1] by 1.\\nBut when you take this and add it to b,\\nthen through python broadcasting this will get duplicated\\ninto an n^[1] by m matrics and then added element-wise.\\nOn the previous slide,\\nwe talked about the dimensions of w,\\nb, dw, and db.\\nHere what we see is that whereas z^[l],\\nas well as a^[l],\\nare of dimension n^[l] by 1,\\nwe have now instead that capital Z^[l],\\nas well as capital A^[l],\\nare n^[l] by m.\\nA special case of this is when l is equal to 0,\\nin which case A^[0],\\nwhich is equal to just your training\\nset input features x is going\\nto be equal to n^[0] by m as expected.\\nOf course, when you're\\nimplementing this in back-propagation,\\nwe'll see later you end up computing dz as well as da.\\nThis way, of course, has the same dimension as z and a.\\nHope the low exercise went through helps\\nclarify the dimensions of\\nthe various matrices you'll be working with.\\nWhen you implement back-propagation\\nfor a deep neural network,\\nso long as you work through your code and make sure that\\nall the matrices or dimensions are consistent,\\nthat will usually help you go some ways\\ntowards eliminating some class of possible bugs.\\nI hope that exercise for figuring out\\nthe dimensions of the various matrices\\nyou'd be working with is helpful.\\nWhen you implement a deep neural\\nnetwork if you keep straight\\nthe dimensions of these various matrices\\nand vectors you're working with,\\nhopefully, that will help you eliminate\\nsome class of possible bugs.\\nIt certainly helps me get my code right.\\nNext, we've now seen some of the mechanics of\\nhow to do the forward propagation in a neural network.\\nBut why are deep neural networks so effective and\\nwhy do they do better than shallow representations?\\nLet's spend a few minutes in the next video to discuss.\"},\n",
       "    {'lesson_number': '04',\n",
       "     'lesson_name': 'why-deep-representations',\n",
       "     'content': \"We've all been hearing that deep neural networks work really well for\\na lot of problems, and it's not just that they need to be big neural networks,\\nis that specifically, they need to be deep or to have a lot of hidden layers.\\nSo why is that?\\nLet's go through a couple examples and try to gain some intuition for\\nwhy deep networks might work well.\\nSo first, what is a deep network computing?\\nIf you're building a system for face recognition or\\nface detection, here's what a deep neural network could be doing.\\nPerhaps you input a picture of a face then the first layer of the neural network\\nyou can think of as maybe being a feature detector or an edge detector.\\nIn this example, I'm plotting what a neural network with maybe 20 hidden units,\\nmight be trying to compute on this image.\\nSo the 20 hidden units visualized by these little square boxes.\\nSo for example, this little visualization represents a hidden unit that's\\ntrying to figure out where the edges of that orientation are in the image.\\nAnd maybe this hidden unit might be trying to figure out\\nwhere are the horizontal edges in this image.\\nAnd when we talk about convolutional networks in a later course,\\nthis particular visualization will make a bit more sense.\\nBut the form, you can think of the first layer of the neural network as looking at the\\npicture and trying to figure out where are the edges in this picture.\\nNow, let's think about where the edges in this picture by grouping together\\npixels to form edges.\\nIt can then detect the edges and group edges together to form parts of faces.\\nSo for example, you might have a low neuron trying to see if it's finding an eye,\\nor a different neuron trying to find that part of the nose.\\nAnd so by putting together lots of edges,\\nit can start to detect different parts of faces.\\nAnd then, finally, by putting together different parts of faces,\\nlike an eye or a nose or an ear or a chin, it can then try to recognize or\\ndetect different types of faces.\\nSo intuitively, you can think of the earlier layers of the neural network as\\ndetecting simple functions, like edges.\\nAnd then composing them together in the later layers of a neural network so\\nthat it can learn more and more complex functions.\\nThese visualizations will make more sense when we talk about convolutional nets.\\nAnd one technical detail of this visualization,\\nthe edge detectors are looking in relatively small areas of an image,\\nmaybe very small regions like that.\\nAnd then the facial detectors you can look at maybe much larger areas of image.\\nBut the main intuition you take away from this is just finding simple things\\nlike edges and then building them up.\\nComposing them together to detect more complex things like an eye or a nose\\nthen composing those together to find even more complex things.\\nAnd this type of simple to complex hierarchical representation,\\nor compositional representation,\\napplies in other types of data than images and face recognition as well.\\nFor example, if you're trying to build a speech recognition system,\\nit's hard to revisualize speech but\\nif you input an audio clip then maybe the first level of a neural network might\\nlearn to detect low level audio wave form features, such as is this tone going up?\\nIs it going down?\\nIs it white noise or sniffling sound like [SOUND].\\nAnd what is the pitch?\\nWhen it comes to that, detect low level wave form features like that.\\nAnd then by composing low level wave forms,\\nmaybe you'll learn to detect basic units of sound.\\nIn linguistics they call phonemes.\\nBut, for example, in the word cat, the C is a phoneme, the A is a phoneme,\\nthe T is another phoneme.\\nBut learns to find maybe the basic units of sound and\\nthen composing that together maybe learn to recognize words in the audio.\\nAnd then maybe compose those together,\\nin order to recognize entire phrases or sentences.\\nSo deep neural network with multiple hidden layers might be able to have the earlier\\nlayers learn these lower level simple features and\\nthen have the later deeper layers then put together the simpler things it's detected\\nin order to detect more complex things like recognize specific words or\\neven phrases or sentences.\\nThe uttering in order to carry out speech recognition.\\nAnd what we see is that whereas the other layers are computing, what seems like\\nrelatively simple functions of the input such as where the edge is, by the time\\nyou get deep in the network you can actually do surprisingly complex things.\\nSuch as detect faces or detect words or phrases or sentences.\\nSome people like to make an analogy between deep neural networks and\\nthe human brain, where we believe, or neuroscientists believe,\\nthat the human brain also starts off detecting simple things like edges in what\\nyour eyes see then builds those up to detect more complex\\nthings like the faces that you see.\\nI think analogies between deep learning and\\nthe human brain are sometimes a little bit dangerous.\\nBut there is a lot of truth to, this being how we think that human brain works and\\nthat the human brain probably detects simple things like edges first\\nthen put them together to from more and more complex objects and so that\\nhas served as a loose form of inspiration for some deep learning as well.\\nWe'll see a bit more about the human brain or\\nabout the biological brain in a later video this week.\\nThe other piece of intuition about why deep networks seem to\\nwork well is the following.\\nSo this result comes from circuit theory of which pertains the thinking\\nabout what types of functions you can compute with\\ndifferent AND gates, OR gates, NOT gates, basically logic gates.\\nSo informally, their functions compute with a relatively small but deep neural\\nnetwork and by small I mean the number of hidden units is relatively small.\\nBut if you try to compute the same function with a shallow network,\\nso if there aren't enough hidden layers,\\nthen you might require exponentially more hidden units to compute.\\nSo let me just give you one example and illustrate this a bit informally.\\nBut let's say you're trying to compute the exclusive OR, or\\nthe parity of all your input features.\\nSo you're trying to compute X1, XOR, X2, XOR,\\nX3, XOR, up to Xn if you have n or n X features.\\nSo if you build in XOR tree like this, so for us it computes the XOR of X1 and\\nX2, then take X3 and X4 and compute their XOR.\\nAnd technically, if you're just using AND or NOT gate, you might need a\\ncouple layers to compute the XOR function rather than just one layer, but\\nwith a relatively small circuit, you can compute the XOR, and so on.\\nAnd then you can build, really, an XOR tree like so,\\nuntil eventually, you have a circuit here that outputs, well, lets call this Y.\\nThe outputs of Y hat equals Y.\\nThe exclusive OR, the parity of all these input bits.\\nSo to compute XOR, the depth of the network will be on the order of log N.\\nWe'll just have an XOR tree.\\nSo the number of nodes or the number of circuit components or\\nthe number of gates in this network is not that large.\\nYou don't need that many gates in order to compute the exclusive OR.\\nBut now, if you are not allowed to use a neural network with multiple\\nhidden layers with, in this case, order log and hidden layers,\\nif you're forced to compute this function with just one hidden layer,\\nso you have all these things going into the hidden units.\\nAnd then these things then output Y.\\nThen in order to compute this XOR function, this hidden layer\\nwill need to be exponentially large, because essentially,\\nyou need to exhaustively enumerate our 2 to the N possible configurations.\\nSo on the order of 2 to the N, possible configurations of the input\\nbits that result in the exclusive OR being either 1 or 0.\\nSo you end up needing a hidden layer that is exponentially large in\\nthe number of bits.\\nI think technically, you could do this with 2 to the N minus 1 hidden units.\\nBut that's the older 2 to the N, so it's going to be exponentially larger on the number of bits.\\nSo I hope this gives a sense that there are mathematical functions,\\nthat are much easier to compute with deep networks than with shallow networks.\\nActually, I personally found the result from circuit theory less useful for\\ngaining intuitions, but this is one of the results that people often cite\\nwhen explaining the value of having very deep representations.\\nNow, in addition to this reasons for\\npreferring deep neural networks, to be perfectly honest,\\nI think the other reasons the term deep learning has taken off is just branding.\\nThis things just we call neural networks with a lot of hidden layers, but\\nthe phrase deep learning is just a great brand, it's just so deep.\\nSo I think that once that term caught on that really neural networks rebranded or\\nneural networks with many hidden layers rebranded,\\nhelp to capture the popular imagination as well.\\nBut regardless of the PR branding, deep networks do work well.\\nSometimes people go overboard and insist on using tons of hidden layers.\\nBut when I'm starting out a new problem, I'll often really start out with\\neven logistic regression then try something with one or\\ntwo hidden layers and use that as a hyper parameter.\\nUse that as a parameter or hyper parameter that you tune in order to try to find\\nthe right depth for your neural network.\\nBut over the last several years there has been a trend toward people finding that\\nfor some applications, very, very deep neural networks here with maybe many\\ndozens of layers sometimes, can sometimes be the best model for a problem.\\nSo that's it for the intuitions for why deep learning seems to work well.\\nLet's now take a look at the mechanics of how to implement not just front\\npropagation, but also back propagation.\"},\n",
       "    {'lesson_number': '05',\n",
       "     'lesson_name': 'building-blocks-of-deep-neural-networks',\n",
       "     'content': \"In the earlier videos from this week,\\nas well as from the videos from the past several weeks,\\nyou've already seen the basic building blocks of forward propagation and\\nback propagation, the key components you need to implement a deep neural network.\\nLet's see how you can put these components together to build your deep net.\\nHere's a network of a few layers.\\nLet's pick one layer.\\nAnd look into the computations focusing on just that layer for now.\\nSo for layer L, you have some parameters wl and\\nbl and for the forward prop, you will input\\nthe activations a l-1 from your previous layer and\\noutput a l.\\nSo the way we did this previously was you compute z l =\\nw l times al - 1 + b l.\\nAnd then al = g of z l.\\nAll right.\\nSo, that's how you go from the input al minus one to the output al.\\nAnd, it turns out that for later use it'll be useful to also cache the value zl.\\nSo, let me include this on cache as well because storing the value zl\\nwould be useful for backward, for the back propagation step later.\\nAnd then for the backward step or for the back propagation step, again,\\nfocusing on computation for this layer l,\\nyou're going to implement a function that inputs da(l).\\nAnd outputs da(l-1), and just to flesh out the details,\\nthe input is actually da(l), as well as the cache so\\nyou have available to you the value of zl that you computed and\\nthen in addition, outputing da(l) minus 1 you bring the output or\\nthe gradients you want in order to implement gradient descent for\\nlearning, okay?\\nSo this is the basic structure of how you implement this forward step,\\nwhat we call the forward function as well as this backward step,\\nwhich we'll call backward function.\\nSo just to summarize, in layer l,\\nyou're going to have the forward step or the forward prop of the forward function.\\nInput al- 1 and output, al, and\\nin order to make this computation you need to use wl and bl.\\nAnd also output a cache, which contains zl, right?\\nAnd then the backward function, using the back prop step,\\nwill be another function that now\\ninputs da(l) and outputs da(l-1).\\nSo it tells you, given the derivatives respect to these activations,\\nthat's da(l), what are the derivatives?\\nHow much do I wish?\\nYou know, al- 1 changes the computed derivatives respect to deactivations\\nfrom a previous layer.\\nWithin this box, right?\\nYou need to use wl and bl, and it turns out along the way you end up\\ncomputing dzl, and then this box,\\nthis backward function can also output dwl and\\ndbl, but I was sometimes using red arrows to denote the backward iteration.\\nSo if you prefer, we could draw these arrows in red.\\nSo if you can implement these two functions\\nthen the basic computation of the neural network will be as follows.\\nYou're going to take the input features a0, feed that in, and\\nthat would compute the activations of the first layer, let's call that a1 and\\nto do that, you need a w1 and b1 and then will also,\\nyou know, cache away z1, right?\\nNow having done that, you feed that to the second layer and then using w2 and b2,\\nyou're going to compute deactivations in the next layer a2 and so on.\\nUntil eventually, you end up outputting\\na l which is equal to y hat.\\nAnd along the way, we cached all of these values z.\\nSo that's the forward propagation step.\\nNow, for the back propagation step, what we're going to do\\nwill be a backward sequence of iterations\\nin which you are going backwards and computing gradients like so.\\nSo what you're going to feed in here, da(l) and\\nthen this box will give us da(l- 1) and so on until we get da(2) da(1).\\nYou could actually get one more output to compute da(0) but\\nthis is derivative with respect to your\\ninput features, which is not useful at least for\\ntraining the weights of these supervised neural networks.\\nSo you could just stop it there. But along the way,\\nback prop also ends up outputting dwl, dbl.\\nI just used the prompt as wl and bl.\\nThis would output dw3, db3 and so on.\\nSo you end up computing all the derivatives you need.\\nAnd so just to maybe fill in the structure of this a little bit more,\\nthese boxes will use those parameters as well.\\nwl, bl and it turns out that\\nwe'll see later that inside these boxes we end up computing the dz's as well.\\nSo one iteration of training through a neural network involves: starting with\\na(0) which is x and going through forward prop as follows.\\nComputing y hat and then using that to compute this and\\nthen back prop, right, doing that and\\nnow you have all these derivative terms and so, you know,\\nw would get updated as w1 = the learning rate times dw, right?\\nFor each of the layers and similarly for b rate.\\nNow the computed back prop have all these derivatives.\\nSo that's one iteration of gradient descent for your neural network.\\nNow before moving on, just one more informational detail.\\nConceptually, it will be useful to think of the cache here as\\nstoring the value of z for the backward functions.\\nBut when you implement this, and you see this in the programming exercise,\\nWhen you implement this, you find that the cache may be\\na convenient way to get to this value of the parameters of w1, b1,\\ninto the backward function as well. So for\\nthis exercise you actually store in your cache to z as well as w\\nand b. So this stores z2, w2, b2. But from an implementation standpoint,\\nI just find it a convenient way to just get the parameters,\\ncopy to where you need to use them later when you're computing back propagation.\\nSo that's just an implementational detail that you see when\\nyou do the programming exercise.\\nSo you've now seen what are the basic building blocks for\\nimplementing a deep neural network.\\nIn each layer there's a forward propagation step and\\nthere's a corresponding backward propagation step.\\nAnd has a cache to pass information from one to the other.\\nIn the next video,\\nwe'll talk about how you can actually implement these building blocks.\\nLet's go on to the next video.\"},\n",
       "    {'lesson_number': '06',\n",
       "     'lesson_name': 'forward-and-backward-propagation',\n",
       "     'content': \"In a previous video, you saw\\nthe basic blocks of implementing a deep neural network,\\na forward propagation step for\\neach layer and a corresponding backward propagation step.\\nLet's see how you can actually implement these steps.\\nWe'll start with forward propagation.\\nRecall that what this will do is input a^l minus 1,\\nand outputs a^l and the cache, z^l.\\nWe just said that, from implementational point of view,\\nmaybe we'll cache w^l and b^l as well,\\njust to make the function's call a\\nbit easier in the program exercise.\\nThe equations for this should already look familiar.\\nThe way to implement a forward function is just this,\\nequals w^l times a^l minus 1 plus b^l,\\nand then a^l equals the activation function applied to z.\\nIf you want a vectorized implementation,\\nthen it's just that times a^l minus 1 plus b,\\nb being a Python broadcasting,\\nand a^l equals g,\\napplied element-wise to z.\\nYou remember, on the diagram for the 4th step,\\nremember we had this chain of boxes going forward,\\nso you initialize that with feeding\\nin a^0, which is equal to x.\\nSo you initialize this with,\\nwhat is the input to the first one?\\nIt's really a^0, which is the input\\nfeatures either for one training example\\nif you're doing one example at a time,\\nor a^0 if you're processing the entire training set.\\nThat's the input to\\nthe first forward function in the chain,\\nand then just repeating this allows you to compute\\nforward propagation from left to right.\\nNext, let's talk about the backward propagation step.\\nHere, your goal is to input da^l,\\nand output da^l minus 1 and dw^l and db^l.\\nLet me just write out\\nthe steps you need to compute these things.\\nDz^l is equal to da^l element-wise product,\\nwith g of l prime z of\\nl. Then to compute the derivatives,\\ndw^l equals dz^l times a of l minus 1.\\nI didn't explicitly put that in the cache,\\nbut it turns out you need this as well.\\nThen db^l is equal to dz^l, and finally,\\nda of l minus 1 is equal to\\nw^l transpose times dz^l.\\nI don't want to go through\\nthe detailed derivation for this,\\nbut it turns out that if you take this definition\\nfor da and plug it in here,\\nthen you get the same formula\\nas we had in there previously,\\nfor how you compute\\ndz^l as a function of the previous dz^l.\\nIn fact, well, if I just plug that in here,\\nyou end up that dz^l is equal\\nto w^l plus 1 transpose dz^l\\nplus 1 times g^l prime z\\nof l. I know this looks like a lot of algebra.\\nYou could actually double-check for\\nyourself that this is the equation\\nwe had written down for back propagation last week,\\nwhen we were doing a neural network\\nwith just a single hidden layer.\\nAs a reminder, this times is element-wise product,\\nso all you need is\\nthose four equations to implement your backward function.\\nThen finally, I'll just write out a vectorized version.\\nSo the first line becomes dz^l\\nequals da^l element-wise product with g^l prime of z^l,\\nmaybe no surprise there.\\nDw^l becomes 1 over m,\\ndz^l times a^l minus 1 transpose.\\nThen db^l becomes 1 over m np.sum dz^l.\\nThen axis equals 1, keepdims equals true.\\nWe talked about the use of\\nnp.sum in the previous week, to compute db.\\nThen finally, da^l minus 1 is w^l transpose times\\ndz of l. This\\nallows you to input this quantity, da, over here.\\nOutput dW^l, dp^l, the derivatives you need,\\nas well as da^l minus 1 as follows.\\nThat's how you implement the backward function.\\nJust to summarize, take the input x,\\nyou may have the first layer maybe\\nhas a ReLU activation function.\\nThen go to the second layer,\\nmaybe uses another ReLU activation function,\\ngoes to the third layer maybe has\\na sigmoid activation function\\nif you're doing binary classification,\\nand this outputs y-hat.\\nThen using y-hat, you can compute the loss.\\nThis allows you to start your backward iteration.\\nI'll draw the arrows first.\\nI guess I don't have to change pens too much.\\nWhere you will then\\nhave backprop compute the derivatives,\\ncompute dW^3,\\ndb^3, dW^2, db^2, dW^1, db^1.\\nAlong the way you would be computing against the cash.\\nWe'll transfer z^1, z^2, z^3.\\nHere you pass backwards da^2 and da^1.\\nThis could compute da^0,\\nbut we won't use that so you can just discard that.\\nThis is how you implement forward prop and back prop for\\na three-layer neural network.\\nThere's this one last detail that I didn't talk\\nabout which is for the forward recursion,\\nwe will initialize it with the input data x.\\nHow about the backward recursion?\\nWell it turns out that\\nda of l when you're using logistic regression,\\nwhen you're doing binary classification\\nis equal to y over\\na plus 1 minus y over 1 minus a.\\nIt turns out that the derivative\\nof the loss function respect to\\nthe output with respect to\\ny-hat can be shown to be what it is.\\nIf you're familiar with calculus,\\nif you look up the loss function l and\\ntake derivatives with respect to y-hat with respect to a,\\nyou can show that you get that formula.\\nThis is the formula you should use for da,\\nfor the final layer,\\ncapital L. Of course if you\\nwere to have a vectorized implementation,\\nthen you initialize\\nthe backward recursion, not with this,\\nbut with da capital A for\\nthe layer L which is going to be\\nthe same thing for the different examples.\\nOver a for the first train example plus 1 minus\\ny for the first train example over\\n1 minus A for the first train example,\\ndot-dot-dot down to the nth train example.\\nSo 1 minus a of M. That's how\\nyou'd implement the vectorized version.\\nThat's how you initialize\\nthe vectorized version of back propagation.\\nWe've now seen the basic building blocks of\\nboth forward propagation as well as back propagation.\\nNow if you implement these equations,\\nyou will get the correct implementation of\\nforward prop and backprop to\\nget you the derivatives you need.\\nYou might be thinking, well those are a lot of equations.\\nI'm slightly confused. I'm not quite sure I see\\nhow this works and if you're feeling that way,\\nmy advice is when you\\nget to this week's programming assignment,\\nyou will be able to implement these for\\nyourself and they will be much more concrete.\\nI know those are a lot of equations,\\nand maybe some of the equations\\ndidn't make complete sense,\\nbut if you work through\\nthe calculus and the linear algebra which is not easy,\\nso feel free to try,\\nbut that's actually one of the more\\ndifficult derivations in machine learning.\\nIt turns out the equations we wrote down are\\njust the calculus equations\\nfor computing the derivatives,\\nespecially in backprop, but once again,\\nif this feels a little bit abstract,\\na little bit mysterious to you,\\nmy advice is when you've done the programming exercise,\\nit will feel a bit more concrete to you.\\nAlthough I have to say, even\\ntoday when I implement a learning algorithm,\\nsometimes even I'm surprised when\\nmy learning algorithm implementation\\nworks and it's because\\na lot of the complexity of\\nmachine learning comes from\\nthe data rather than from the lines of codes.\\nSometimes you feel like\\nyou implement a few lines of code,\\nnot quite sure what it did,\\nbut it almost magically works,\\nand it's because a lot of magic is actually not in\\nthe piece of code you write which is often not too long.\\nIt's not exactly simple,\\nbut it's not 10,000,\\n100,000 lines of code,\\nbut you feed it so much data that\\nsometimes even though I've\\nworked with machine learning for a long time,\\nsometimes it still surprises\\nme a bit when my learning algorithm works,\\nbecause a lot of the complexity of\\nyour learning algorithm comes from the data\\nrather than necessarily from you\\nwriting thousands and thousands of lines of code.\\nThat's how you implement deep neural networks.\\nAgain this will become more\\nconcrete when you've done the programming exercising.\\nBefore moving on, in the next video,\\nI want to discuss hyper-parameters and parameters.\\nIt turns out that when you're training deep nets,\\nbeing able to organize your hyper-parameters well\\nwill help you be more efficient\\nin developing your networks.\\nIn the next video, let's talk\\nabout exactly what that means.\"},\n",
       "    {'lesson_number': '07',\n",
       "     'lesson_name': 'parameters-vs-hyperparameters',\n",
       "     'content': \"Being effective in developing your deep\\nNeural Nets requires that you not only\\norganize your parameters well but also\\nyour hyper parameters. So what are hyper\\nparameters? let's take a look! So the\\nparameters your model are W and B and\\nthere are other things you need to tell\\nyour learning algorithm, such as the\\nlearning rate alpha, because we need\\nto set alpha and that in turn will\\ndetermine how your parameters evolve or\\nmaybe the number of iterations of\\ngradient descent you carry out. Your\\nlearning algorithm has oth\\nnumbers that you need to set such as the\\nnumber of hidden layers, so we call that\\ncapital L, or the number of hidden units,\\nsuch as 0 and 1 and 2 and\\nso on. Then you also have the choice\\nof activation function. do you want to\\nuse a RELU, or tangent or a sigmoid\\nfunction especially in the\\nhidden layers. So all of these things\\nare things that you need to tell your\\nlearning algorithm and so these are\\nparameters that control the ultimate\\nparameters W and B and so we call all of\\nthese things below hyper parameters.\\nBecause these things like alpha, the\\nlearning rate, the number of iterations,\\nnumber of hidden layers, and so on, these\\nare all parameters that control W and B.\\nSo we call these things hyper parameters,\\nbecause it is the hyper parameters that\\nsomehow determine the final\\nvalue of the parameters W and B that you\\nend up with. In fact, deep learning has a\\nlot of different hyper parameters.\\nIn the later course, we'll see other\\nhyper parameters as well such as the\\nmomentum term, the mini batch size,\\nvarious forms of regularization\\nparameters, and so on. If none of\\nthese terms at the bottom make sense yet,\\ndon't worry about it! We'll talk about\\nthem in the second course. Because deep\\nlearning has so many hyper parameters in\\ncontrast to earlier errors of machine\\nlearning, I'm going to try to be very\\nconsistent in calling the learning rate\\nalpha a hyper parameter rather than\\ncalling the parameter. I think in earlier\\neras of machine learning when we didn't\\nhave so many hyper parameters, most of us\\nused to be a bit slow up here and just\\ncall alpha a parameter. Technically,\\nalpha is a parameter, but is a parameter\\nthat determines the real parameters. I'll\\ntry to be consistent in calling these\\nthings like alpha, the number of\\niterations, and so on hyper parameters. So\\nwhen you're training a deep net for your\\nown application you find that there may\\nbe a lot of possible settings for the\\nhyper parameters that you need to just\\ntry out. So applying deep learning today is\\na very intrictate process where often you\\nmight have an idea. For example, you might\\nhave an idea for the best value for the\\nlearning rate. You might say, well maybe\\nalpha equals 0.01 I want to try that.\\nThen you implement, try it out, and then\\nsee how that works. Based on\\nthat outcome you might say, you know what?\\nI've changed online, I want to increase\\nthe learning rate to 0.05. So, if\\nyou're not sure what the best value\\nfor the learning rate to use. You might\\ntry one value of the learning rate alpha\\nand see their cost function j go down\\nlike this, then you might try a larger\\nvalue for the learning rate alpha and\\nsee the cost function blow up and\\ndiverge. Then, you might try another\\nversion and see it go down really fast.\\nit's inverse to higher value. You might\\ntry another version and\\nsee the cost function J do that then.\\nI'll be trying to set the values. So you might\\nsay, okay looks like this the value of\\nalpha. It gives me a pretty fast learning\\nand allows me to converge to a lower\\ncost function j and so I'm going to use\\nthis value of alpha. You saw in a\\nprevious slide that there are a lot of\\ndifferent hybrid parameters. It turns\\nout that when you're starting on the new\\napplication, you should find it very\\ndifficult to know in advance exactly\\nwhat is the best value of the hyper\\nparameters. So, what often happens is you\\njust have to try out many different\\nvalues and go around this cycle your\\ntry out some values, really try five hidden\\nlayers. With this many number of hidden\\nunits implement that, see if it works, and\\nthen iterate. So the title of this slide\\nis that applying deep learning is a very\\nempirical process, and empirical process\\nis maybe a fancy way of saying you just\\nhave to try a lot of things and see what\\nworks. Another effect I've seen is that\\ndeep learning today is applied to so\\nmany problems ranging from computer\\nvision, to speech recognition, to natural\\nlanguage processing, to a lot of\\nstructured data applications such as\\nmaybe a online advertising, or web search,\\nor product recommendations, and so on.\\nWhat I've seen is that first, I've seen\\nresearchers from one discipline, any one\\nof these, and try to go to a different one.\\nAnd sometimes the intuitions about hyper\\nparameters carries over and sometimes it\\ndoesn't, so I often advise people,\\nespecially when starting on a new\\nproblem, to just try out a range of\\nvalues and see what w. In the next\\ncourse we'll\\nsee some systematic ways for trying out\\na range of values. Second,\\neven if you're working on one\\napplication for a long time, you know\\nmaybe you're working on online\\nadvertising, as you make progress on the\\nproblem it is quite possible that the best\\nvalue for the learning rate, a number of\\nhidden units, and so on might change. So\\neven if you tune your system to the best\\nvalue of hyper parameters today it's\\npossible you'll find that the best value\\nmight change a year from now maybe\\nbecause the computer infrastructure,\\nbe it you know CPUs, or the type of GPU\\nrunning on, or something has changed.\\nSo maybe one rule of thumb is\\nevery now and then, maybe every few\\nmonths, if you're working on a problem\\nfor an extended period of time for many\\nyears just try a few values for the\\nhyper parameters and double check if\\nthere's a better value for the hyper\\nparameters. As you do so you slowly\\ngain intuition as well about the hyper\\nparameters that work best for your\\nproblems.\\nI know that this might seem like an\\nunsatisfying part of deep learning that\\nyou just have to try on all the values\\nfor these hyper parameters, but maybe\\nthis is one area where deep learning\\nresearch is still advancing, and maybe\\nover time we'll be able to give better\\nguidance for the best hyper parameters\\nto use. It's also possible that\\nbecause CPUs and GPUs and networks and\\ndata sets are all changing, and it is\\npossible that the guidance won't\\nconverge for some time. You just need\\nto keep trying out different values and\\nevaluate them on a hold on\\ncross-validation set or something and\\npick the value that works for your\\nproblems. So that was a brief discussion\\nof hyper parameters. In the second course,\\nwe'll also give some suggestions for how\\nto systematically explore the space of\\nhyper parameters but by now you actually\\nhave pretty much all the tools you need\\nto do their programming exercise before\\nyou do that adjust or share view one\\nmore set of ideas which is I often ask\\nwhat does deep learning have to do the\\nhuman brain?\"},\n",
       "    {'lesson_number': '08',\n",
       "     'lesson_name': 'what-does-this-have-to-do-with-the-brain',\n",
       "     'content': \"So what does deep learning have to do with the brain?\\nAt the risk of giving away\\nthe punchline I would say, not a whole lot,\\nbut let's take a quick look at why people keep making\\nthe analogy between deep learning and the human brain.\\nWhen you implement a neural network,\\nthis is what you do,\\nforward prop and back prop.\\nI think because it's been difficult to convey\\nintuitions about what these equations are doing,\\nreally creating the sense on a very complex function,\\nthe analogy that it's like the brain has\\nbecome an oversimplified explanation\\nfor what this is doing.\\nBut the simplicity of this makes it seductive\\nfor people to just say it\\npublicly as well as for media to report it,\\nand it is certainly called the popular imagination.\\nThere is a very loose analogy between, let's say,\\na logistic regression unit\\nwith a sigmoid activation function.\\nHere's a cartoon of a single neuron in the brain.\\nIn this picture of a biological neuron, this neuron,\\nwhich is a cell in your brain,\\nreceives electric signals from other neurons,\\nX1, X2, X3, or maybe from other neurons, A1, A2,\\nA3, does a simple thresholding computation,\\nand then if this neuron fires,\\nit sends a pulse of electricity down the axon,\\ndown this long wire,\\nperhaps to other neurons.\\nThere is a very simplistic analogy\\nbetween a single neuron in a neural network,\\nand a biological neuron like that shown on the right.\\nBut I think that today even neuroscientists have\\nalmost no idea what even a single neuron is doing.\\nA single neuron appears to be much more complex\\nthan we are able to characterize with neuroscience,\\nand while some of what it's\\ndoing is a little bit like logistic regression,\\nthere's still a lot about what even a single neuron\\ndoes that no one human today understands.\\nFor example, exactly how neurons in\\nthe human brain learns is still\\na very mysterious process,\\nand it's completely unclear today\\nwhether the human brain uses an algorithm,\\ndoes anything like back propagation\\nor gradient descent or if there's\\nsome fundamentally different learning principle\\nthat the human brain uses.\\nWhen I think of deep-learning,\\nI think of it as being very\\ngood and learning very flexible functions,\\nvery complex functions, to learn X to Y mappings,\\nto learn input-output mappings in supervised learning.\\nWhereas this is like the brain analogy,\\nmaybe that was useful once,\\nI think the field has moved to\\nthe point where that analogy is breaking down,\\nand I tend not to use that analogy much anymore.\\nSo that's it for neural networks and the brain.\\nI do think that maybe the field of\\ncomputer vision has taken\\na bit more inspiration from the human brain than\\nother disciplines that also apply deep learning,\\nbut I personally use\\nthe analogy to the human brain less than I used to.\\nThat's it for this video.\\nYou now know how to implement forward prop and back\\nprop and gradient descent even for deep neural networks.\\nBest of luck with the programming exercise,\\nand I look forward to sharing more of\\nthese ideas with you in the second course.\"}],\n",
       "   'generated_content': Code(topic='Chapter 4: Deep Neural Networks', cell=[CellFormat(cell_type='markdown', cell_no=1, cell_content='# Chapter 4: Deep Neural Networks'), CellFormat(cell_type='markdown', cell_no=2, cell_content='## Lesson 01: Deep L-Layer Neural Network'), CellFormat(cell_type='markdown', cell_no=3, cell_content=\"Welcome to the fourth week of this course. By now, you've seen forward propagation and back propagation in the context of a neural network, with a single hidden layer, as well as logistic regression, and you've learned about vectorization, and when it's important to initialize the ways randomly. If you've done the past couple weeks homework, you've also implemented and seen some of these ideas work for yourself.\"), CellFormat(cell_type='markdown', cell_no=4, cell_content=\"So by now, you've actually seen most of the ideas you need to implement a deep neural network. What we're going to do this week, is take those ideas and put them together so that you'll be able to implement your own deep neural network.\"), CellFormat(cell_type='markdown', cell_no=5, cell_content=\"Because this week's problem exercise is longer, it just has been more work, I'm going to keep the videos for this week shorter as you can get through the videos a little bit more quickly, and then have more time to do a significant problem exercise at the end, which I hope will leave you having built a deep neural network that you feel proud of.\"), CellFormat(cell_type='markdown', cell_no=6, cell_content='### What is a Deep Neural Network?'), CellFormat(cell_type='markdown', cell_no=7, cell_content=\"You've seen this picture for logistic regression and you've also seen neural networks with a single hidden layer. Here's an example of a neural network with two hidden layers and a neural network with 5 hidden layers.\"), CellFormat(cell_type='markdown', cell_no=8, cell_content='We say that logistic regression is a very \"shallow\" model, whereas a model with multiple hidden layers is a much deeper model, and shallow versus depth is a matter of degree.'), CellFormat(cell_type='markdown', cell_no=9, cell_content=\"A neural network of a single hidden layer would be a 2-layer neural network. Remember when we count layers in a neural network, we don't count the input layer; we just count the hidden layers as well as the output layer. So, this would be a 2-layer neural network is still quite shallow, but not as shallow as logistic regression. Technically, logistic regression is a one-layer neural network.\"), CellFormat(cell_type='markdown', cell_no=10, cell_content='Over the last several years, the AI and machine learning community has realized that very deep neural networks can learn functions that shallower models are often unable to. Although for any given problem, it might be hard to predict in advance exactly how deep a network you would want. So it would be reasonable to try logistic regression, then one, then two hidden layers, and view the number of hidden layers as another hyperparameter that you could try a variety of values of, and evaluate on cross-validation data, or on your development set. See more about that later as well.'), CellFormat(cell_type='markdown', cell_no=11, cell_content='### Notation for Deep Neural Networks'), CellFormat(cell_type='markdown', cell_no=12, cell_content=\"Let's now go through the notation we use to describe deep neural networks. Here's a 4-layer neural network (with three hidden layers), and the number of units in these hidden layers are, for example, 5, 5, 3, and then there's one output unit.\"), CellFormat(cell_type='markdown', cell_no=13, cell_content=\"The notation we're going to use:\"), CellFormat(cell_type='markdown', cell_no=14, cell_content='*   $L$: denotes the number of layers in the network. In this case, $L = 4$ (input layer is not counted in $L$).\\n*   $n^{[l]}$: denotes the number of nodes or units in layer $l$. We index the input layer as layer \"0\".\\n    *   $n^{[0]} = n_x = 3$ (input layer)\\n    *   $n^{[1]} = 5$ (first hidden layer)\\n    *   $n^{[2]} = 5$ (second hidden layer)\\n    *   $n^{[3]} = 3$ (third hidden layer)\\n    *   $n^{[4]} = n^{[L]} = 1$ (output layer)'), CellFormat(cell_type='markdown', cell_no=15, cell_content='For each layer $l$:'), CellFormat(cell_type='markdown', cell_no=16, cell_content='*   $a^{[l]}$: denotes the activations in layer $l$. In forward propagation, $a^{[l]} = g^{[l]}(z^{[l]})$, where $g^{[l]}$ is the activation function for layer $l$.\\n*   $W^{[l]}$: denotes the weights for computing $z^{[l]}$ in layer $l$.\\n*   $b^{[l]}$: denotes the biases for computing $z^{[l]}$ in layer $l$.'), CellFormat(cell_type='markdown', cell_no=17, cell_content='Finally, to wrap up on the notation:'), CellFormat(cell_type='markdown', cell_no=18, cell_content='*   The input features are called $x$, but $x$ is also the activations of layer zero, so $a^{[0]} = x$.\\n*   The activation of the final layer, $a^{[L]} = \\text{y-hat}$. So $a^{[L]}$ is equal to the predicted output, the prediction $\\text{y-hat}$ of the neural network.'), CellFormat(cell_type='markdown', cell_no=19, cell_content=\"You now know what a deep neural network looks like, as well as the notation we'll use to describe and compute with deep networks. If you ever forget what some symbol means, refer to the course notation guide.\"), CellFormat(cell_type='markdown', cell_no=20, cell_content=\"Next, we'll describe forward propagation in this type of network.\"), CellFormat(cell_type='markdown', cell_no=21, cell_content='## Lesson 02: Forward Propagation in a Deep Network'), CellFormat(cell_type='markdown', cell_no=22, cell_content=\"In the last video, we described what is a deep L-layer neural network and also talked about the notation we use to describe such networks. In this video, you'll see how you can perform forward propagation in a deep network.\"), CellFormat(cell_type='markdown', cell_no=23, cell_content='### Forward Propagation for a Single Training Example ($x$)'), CellFormat(cell_type='markdown', cell_no=24, cell_content=\"Given a single training example $x$, here's how you compute the activations of the first layer:\"), CellFormat(cell_type='markdown', cell_no=25, cell_content='$$z^{[1]} = W^{[1]}x + b^{[1]}$$'), CellFormat(cell_type='markdown', cell_no=26, cell_content='$$a^{[1]} = g^{[1]}(z^{[1]})$$'), CellFormat(cell_type='markdown', cell_no=27, cell_content=\"($W^{[1]}$ and $b^{[1]}$ are the parameters that affect the activations in layer one. The activation function $g^{[1]}$ depends on what layer you're at).\"), CellFormat(cell_type='markdown', cell_no=28, cell_content='For layer two:'), CellFormat(cell_type='markdown', cell_no=29, cell_content='$$z^{[2]} = W^{[2]}a^{[1]} + b^{[2]}$$'), CellFormat(cell_type='markdown', cell_no=30, cell_content='$$a^{[2]} = g^{[2]}(z^{[2]})$$'), CellFormat(cell_type='markdown', cell_no=31, cell_content='This continues until the output layer (e.g., layer $L=4$):'), CellFormat(cell_type='markdown', cell_no=32, cell_content='$$z^{[L]} = W^{[L]}a^{[L-1]} + b^{[L]}$$'), CellFormat(cell_type='markdown', cell_no=33, cell_content='$$a^{[L]} = g^{[L]}(z^{[L]})$$'), CellFormat(cell_type='markdown', cell_no=34, cell_content='This $a^{[L]}$ is your estimated output, $\\text{y-hat}$.'), CellFormat(cell_type='markdown', cell_no=35, cell_content='Just one thing to notice: $x$ here is also equal to $a^{[0]}$, because the input feature vector $x$ is also the activations of layer zero. If we replace $x$ with $a^{[0]}$, then all these equations look the same.'), CellFormat(cell_type='markdown', cell_no=36, cell_content='The general rule for layer $l$ is:'), CellFormat(cell_type='markdown', cell_no=37, cell_content='$$z^{[l]} = W^{[l]}a^{[l-1]} + b^{[l]}$$'), CellFormat(cell_type='markdown', cell_no=38, cell_content='$$a^{[l]} = g^{[l]}(z^{[l]})$$'), CellFormat(cell_type='markdown', cell_no=39, cell_content='This is the general forward propagation equation for a single training example.'), CellFormat(cell_type='markdown', cell_no=40, cell_content='### Vectorized Forward Propagation for the Entire Training Set'), CellFormat(cell_type='markdown', cell_no=41, cell_content='How about doing it in a vectorized way for the whole training set at the same time? The equations look quite similar:'), CellFormat(cell_type='markdown', cell_no=42, cell_content='For the first layer:'), CellFormat(cell_type='markdown', cell_no=43, cell_content='$$Z^{[1]} = W^{[1]}X + B^{[1]}$$'), CellFormat(cell_type='markdown', cell_no=44, cell_content='$$A^{[1]} = g^{[1]}(Z^{[1]})$$'), CellFormat(cell_type='markdown', cell_no=45, cell_content='(Here $X$ is equal to $A^{[0]}$, where $X$ is your training examples stacked in different columns).'), CellFormat(cell_type='markdown', cell_no=46, cell_content='For the next layer:'), CellFormat(cell_type='markdown', cell_no=47, cell_content='$$Z^{[2]} = W^{[2]}A^{[1]} + B^{[2]}$$'), CellFormat(cell_type='markdown', cell_no=48, cell_content='$$A^{[2]} = g^{[2]}(Z^{[2]})$$'), CellFormat(cell_type='markdown', cell_no=49, cell_content='We are stacking the individual $z$ vectors for each training example in columns to form $Z$, and similarly for $A$ (just as capital $X$ has all training examples as column vectors stacked left to right).'), CellFormat(cell_type='markdown', cell_no=50, cell_content='The process continues until the final layer:'), CellFormat(cell_type='markdown', cell_no=51, cell_content='$$A^{[L]} = g^{[L]}(Z^{[L]})$$'), CellFormat(cell_type='markdown', cell_no=52, cell_content='This $A^{[L]}$ is your predictions $\\text{y-hat}$ on all of your training examples stacked horizontally.'), CellFormat(cell_type='markdown', cell_no=53, cell_content='So, to summarize notation, replacing lowercase $z$ and $a$ with their uppercase counterparts gives you the vectorized version of forward propagation, where $A^{[0]}$ is $X$.'), CellFormat(cell_type='markdown', cell_no=54, cell_content='### For Loops in Forward Propagation'), CellFormat(cell_type='markdown', cell_no=55, cell_content='When implementing vectorization, it looks like there is going to be a \"for loop\" here. For $l = 1$ to $L$ (total number of layers), you compute activations for layer one, then layer two, and so on. This is one place where an explicit for loop is necessary and perfectly okay. No one knows a way to implement this without a for loop that goes from one to capital $L$.'), CellFormat(cell_type='markdown', cell_no=56, cell_content=\"That's it for the notation for deep neural networks, as well as how to do forward propagation in these networks. What we're seeing is taking pieces very similar to what you've seen in a neural network with a single hidden layer and just repeating that more times.\"), CellFormat(cell_type='markdown', cell_no=57, cell_content=\"One of the ways to increase your odds of having a bug-free implementation is to think very systematically and carefully about the matrix dimensions you're working with. We'll see how you could do that in the next video.\"), CellFormat(cell_type='markdown', cell_no=58, cell_content='## Lesson 03: Getting Your Matrix Dimensions Right'), CellFormat(cell_type='markdown', cell_no=59, cell_content=\"When implementing a deep neural network, one of the debugging tools often used to check the correctness of code is to work through the dimensions of the matrices. Let's see how.\"), CellFormat(cell_type='markdown', cell_no=60, cell_content='Consider a 5-layer neural network ($L=5$, which means 4 hidden layers and 1 output layer).'), CellFormat(cell_type='markdown', cell_no=61, cell_content=\"Let's define the number of units in each layer using $n^{[l]}$ notation:\"), CellFormat(cell_type='markdown', cell_no=62, cell_content='*   $n^{[0]} = n_x = 2$ (input features)\\n*   $n^{[1]} = 3$ (layer 1)\\n*   $n^{[2]} = 5$ (layer 2)\\n*   $n^{[3]} = 4$ (layer 3)\\n*   $n^{[4]} = 2$ (layer 4)\\n*   $n^{[5]} = 1$ (layer 5 - output)'), CellFormat(cell_type='markdown', cell_no=63, cell_content='### Dimensions for Single Example (Non-Vectorized)'), CellFormat(cell_type='markdown', cell_no=64, cell_content='For forward propagation, the first step is $z^{[1]} = W^{[1]}x + b^{[1]}$.'), CellFormat(cell_type='markdown', cell_no=65, cell_content='**Dimensions of $z^{[l]}$ and $a^{[l]}$:**'), CellFormat(cell_type='markdown', cell_no=66, cell_content='*   $z^{[1]}$ will be $n^{[1]} \\\\times 1$ (e.g., $3 \\\\times 1$).\\n*   In general, $z^{[l]}$ and $a^{[l]}$ are $n^{[l]} \\\\times 1$ dimensional vectors.'), CellFormat(cell_type='markdown', cell_no=67, cell_content='**Dimensions of $x$:**'), CellFormat(cell_type='markdown', cell_no=68, cell_content='*   $x$ (input features) is $n^{[0]} \\\\times 1$ (e.g., $2 \\\\times 1$).\\n'), CellFormat(cell_type='markdown', cell_no=69, cell_content='**Dimensions of $W^{[l]}$:**'), CellFormat(cell_type='markdown', cell_no=70, cell_content='*   For $z^{[1]} = W^{[1]}x + b^{[1]}$ to work (where $z^{[1]}$ is $n^{[1]} \\\\times 1$ and $x$ is $n^{[0]} \\\\times 1$), $W^{[1]}$ must be $n^{[1]} \\\\times n^{[0]}$ (e.g., $3 \\\\times 2$).\\n*   The general rule: $W^{[l]}$ is $n^{[l]} \\\\times n^{[l-1]}$.\\n    *   $W^{[2]}$ will be $n^{[2]} \\\\times n^{[1]}$ (e.g., $5 \\\\times 3$).\\n    *   $W^{[3]}$ will be $n^{[3]} \\\\times n^{[2]}$ (e.g., $4 \\\\times 5$).\\n    *   $W^{[4]}$ will be $n^{[4]} \\\\times n^{[3]}$ (e.g., $2 \\\\times 4$).\\n    *   $W^{[5]}$ will be $n^{[5]} \\\\times n^{[4]}$ (e.g., $1 \\\\times 2$).\\n'), CellFormat(cell_type='markdown', cell_no=71, cell_content='**Dimensions of $b^{[l]}$:**'), CellFormat(cell_type='markdown', cell_no=72, cell_content='*   For $z^{[1]} = W^{[1]}x + b^{[1]}$ to work (where $z^{[1]}$ is $n^{[1]} \\\\times 1$), $b^{[1]}$ must be $n^{[1]} \\\\times 1$ (e.g., $3 \\\\times 1$).\\n*   The general rule: $b^{[l]}$ is $n^{[l]} \\\\times 1$ dimensional.'), CellFormat(cell_type='markdown', cell_no=73, cell_content='These equations help double-check that the dimensions of your matrices $W$ and vectors $b$ are correct.'), CellFormat(cell_type='markdown', cell_no=74, cell_content='When implementing back-propagation, the dimensions of $dW^{[l]}$ should be the same as $W^{[l]}$, and $db^{[l]}$ should be the same as $b^{[l]}$.'), CellFormat(cell_type='markdown', cell_no=75, cell_content='Since $z^{[l]} = g^{[l]}(a^{[l]})$ is applied element-wise, $z^{[l]}$ and $a^{[l]}$ should have the same dimensions.'), CellFormat(cell_type='markdown', cell_no=76, cell_content='### Dimensions for Vectorized Implementation'), CellFormat(cell_type='markdown', cell_no=77, cell_content='Even for a vectorized implementation, the dimensions of $W$, $b$, $dW$, and $db$ stay the same.'), CellFormat(cell_type='markdown', cell_no=78, cell_content='The dimensions of $Z$, $A$, and $X$ will change. Previously, for a single example: $z^{[1]}$ ($n^{[1]} \\\\times 1$), $W^{[1]}$ ($n^{[1]} \\\\times n^{[0]}$), $x$ ($n^{[0]} \\\\times 1$), $b^{[1]}$ ($n^{[1]} \\\\times 1$).'), CellFormat(cell_type='markdown', cell_no=79, cell_content='Now, in a vectorized implementation (with $m$ training examples):'), CellFormat(cell_type='markdown', cell_no=80, cell_content='$$Z^{[1]} = W^{[1]}X + B^{[1]}$$'), CellFormat(cell_type='markdown', cell_no=81, cell_content='*   $Z^{[1]}$ (containing $z^{[1](1)}, z^{[1](2)}, \\\\dots, z^{[1](m)}$ stacked horizontally) becomes $n^{[1]} \\\\times m$.\\n*   $W^{[1]}$ stays $n^{[1]} \\\\times n^{[0]}$.\\n*   $X$ (containing $x^{(1)}, x^{(2)}, \\\\dots, x^{(m)}$ stacked horizontally) becomes $n^{[0]} \\\\times m$. \\n*   Notice: A $(n^{[1]} \\\\times n^{[0]})$ matrix multiplied by an $(n^{[0]} \\\\times m)$ matrix yields an $(n^{[1]} \\\\times m)$ matrix, which is correct for $Z^{[1]}$.'), CellFormat(cell_type='markdown', cell_no=82, cell_content='$b^{[1]}$ is still $n^{[1]} \\\\times 1$. When you take $W^{[1]}X$ (an $n^{[1]} \\\\times m$ matrix) and add it to $b^{[1]}$, then through Python broadcasting, $b^{[1]}$ will get duplicated into an $n^{[1]} \\\\times m$ matrix and then added element-wise.'), CellFormat(cell_type='markdown', cell_no=83, cell_content='To summarize for vectorized operations:'), CellFormat(cell_type='markdown', cell_no=84, cell_content=\"*   Capital $Z^{[l]}$ and capital $A^{[l]}$ are $n^{[l]} \\\\times m$ dimensional matrices.\\n*   A special case of this is when $l$ is equal to 0, in which case $A^{[0]}$, which is equal to just your training set input features $X$, is going to be $n^{[0]} \\\\times m$ as expected.\\n*   Of course, when you're implementing back-propagation, you end up computing $dZ^{[l]}$ as well as $dA^{[l]}$. These will, of course, have the same dimension as $Z^{[l]}$ and $A^{[l]}$ respectively.\"), CellFormat(cell_type='markdown', cell_no=85, cell_content=\"Keeping the dimensions of these various matrices and vectors straight will help eliminate a class of possible bugs. This exercise for figuring out the dimensions of the various matrices you'd be working with is helpful when you implement a deep neural network.\"), CellFormat(cell_type='markdown', cell_no=86, cell_content=\"Next, we've now seen some of the mechanics of how to do the forward propagation in a neural network. But why are deep neural networks so effective and why do they do better than shallow representations? Let's spend a few minutes in the next video to discuss.\"), CellFormat(cell_type='markdown', cell_no=87, cell_content='## Lesson 04: Why Deep Representations?'), CellFormat(cell_type='markdown', cell_no=88, cell_content=\"We've all been hearing that deep neural networks work really well for a lot of problems, and it's not just that they need to be big neural networks, but specifically, they need to be deep or to have a lot of hidden layers. So why is that? Let's go through a couple examples and try to gain some intuition for why deep networks might work well.\"), CellFormat(cell_type='markdown', cell_no=89, cell_content='### Hierarchical Feature Detection'), CellFormat(cell_type='markdown', cell_no=90, cell_content=\"What is a deep network computing? If you're building a system for face recognition or face detection, here's what a deep neural network could be doing:\"), CellFormat(cell_type='markdown', cell_no=91, cell_content='*   **Input Layer:** A picture of a face.\\n*   **First Layer:** Can be thought of as a feature detector or an edge detector. It identifies simple features like horizontal or vertical edges in small regions of the image.\\n*   **Middle Layers:** Group edges together to form parts of faces (e.g., an eye, a nose, an ear, a chin).\\n*   **Later Layers (Deep):** By putting together different parts of faces, it can then try to recognize or detect different types of faces.'), CellFormat(cell_type='markdown', cell_no=92, cell_content='Intuitively, the earlier layers detect simple functions (like edges) and then compose them together in later layers to learn more and more complex functions. This type of simple-to-complex hierarchical or compositional representation applies to other types of data as well.'), CellFormat(cell_type='markdown', cell_no=93, cell_content='**Example: Speech Recognition System**'), CellFormat(cell_type='markdown', cell_no=94, cell_content='*   **Input Layer:** An audio clip.\\n*   **First Layer:** Might learn to detect low-level audio waveform features (e.g., tone going up/down, white noise, pitch).\\n*   **Middle Layers:** By composing low-level waveforms, it might learn to detect basic units of sound (phonemes, e.g., \\'C\\', \\'A\\', \\'T\\' in \"cat\").\\n*   **Later Layers (Deep):** Composes phonemes to recognize entire words, and then words to recognize phrases or sentences.'), CellFormat(cell_type='markdown', cell_no=95, cell_content='While earlier layers compute relatively simple functions, by the time you get deep in the network, you can do surprisingly complex things (detect faces, recognize words/phrases/sentences).'), CellFormat(cell_type='markdown', cell_no=96, cell_content='**Analogy to Human Brain (Loose):** Some people make an analogy to the human brain, where neuroscientists believe the brain also starts by detecting simple things (like edges in vision) and builds them up to detect more complex objects like faces. This has served as a loose inspiration for some deep learning, though such analogies can be dangerous.'), CellFormat(cell_type='markdown', cell_no=97, cell_content='### Circuit Theory Intuition'), CellFormat(cell_type='markdown', cell_no=98, cell_content='Another intuition about why deep networks seem to work well comes from circuit theory (related to computing functions with logic gates like AND, OR, NOT).'), CellFormat(cell_type='markdown', cell_no=99, cell_content='**Informal Result:** There are functions computable with a relatively small but deep neural network that would require exponentially more hidden units if computed with a shallow network (not enough hidden layers).'), CellFormat(cell_type='markdown', cell_no=100, cell_content='**Example: Exclusive OR (XOR) / Parity Function**'), CellFormat(cell_type='markdown', cell_no=101, cell_content='Consider computing $X_1 \\\\oplus X_2 \\\\oplus X_3 \\\\oplus \\\\dots \\\\oplus X_n$ (the parity of all input features).'), CellFormat(cell_type='markdown', cell_no=102, cell_content='*   **Deep Network:** An XOR tree structure (where each node computes an XOR of its inputs) would have a depth on the order of $\\\\log N$. The number of nodes or gates in this network would not be excessively large.\\n*   **Shallow Network (one hidden layer):** If forced to compute this function with just one hidden layer, that hidden layer would need to be exponentially large (on the order of $2^N$ hidden units) because it essentially needs to exhaustively enumerate all $2^N$ possible input configurations (e.g., $2^{N-1}$ hidden units, which is on the order of $2^N$).\\n'), CellFormat(cell_type='markdown', cell_no=103, cell_content='This illustrates that some mathematical functions are much easier to compute with deep networks than with shallow networks. This result from circuit theory is often cited to explain the value of deep representations.'), CellFormat(cell_type='markdown', cell_no=104, cell_content='### The \"Deep Learning\" Branding'), CellFormat(cell_type='markdown', cell_no=105, cell_content='To be perfectly honest, another reason the term \"deep learning\" has taken off is branding. \"Deep learning\" is a great brand, and once it caught on, it helped capture the popular imagination. But regardless of the PR, deep networks do work well.'), CellFormat(cell_type='markdown', cell_no=106, cell_content=\"However, one shouldn't always insist on using tons of hidden layers. When starting a new problem, it's often good to start with logistic regression, then try one or two hidden layers, and treat the number of hidden layers as a hyperparameter to tune.\"), CellFormat(cell_type='markdown', cell_no=107, cell_content='Over the last several years, there has been a trend toward finding that for some applications, very, very deep neural networks (many dozens of layers) can sometimes be the best model for a problem.'), CellFormat(cell_type='markdown', cell_no=108, cell_content='## Lesson 05: Building Blocks of Deep Neural Networks'), CellFormat(cell_type='markdown', cell_no=109, cell_content=\"You've already seen the basic building blocks of forward and back propagation. Let's see how to put them together to build a deep net.\"), CellFormat(cell_type='markdown', cell_no=110, cell_content='### Forward and Backward Functions for a Single Layer'), CellFormat(cell_type='markdown', cell_no=111, cell_content='Consider a single layer $l$ in the network:'), CellFormat(cell_type='markdown', cell_no=112, cell_content='**Forward Propagation for Layer $l$:**'), CellFormat(cell_type='markdown', cell_no=113, cell_content='*   **Input:** Activations $a^{[l-1]}$ from the previous layer.\\n*   **Parameters:** $W^{[l]}, b^{[l]}$.\\n*   **Computation:**\\n    *   $z^{[l]} = W^{[l]}a^{[l-1]} + b^{[l]}$ \\n    *   $a^{[l]} = g^{[l]}(z^{[l]})$ \\n*   **Output:** Activations $a^{[l]}$.\\n*   **Cache:** For later use in backpropagation, we cache the value $z^{[l]}$. (From an implementation standpoint, it can also be convenient to cache $W^{[l]}$ and $b^{[l]}$ here).'), CellFormat(cell_type='markdown', cell_no=114, cell_content='**Backward Propagation for Layer $l$:**'), CellFormat(cell_type='markdown', cell_no=115, cell_content='*   **Input:** $da^{[l]}$ (derivative of the cost with respect to $a^{[l]}$), and the `cache` from the forward pass for this layer (which includes $z^{[l]}$).\\n*   **Output:** $da^{[l-1]}$ (derivative with respect to activations of the previous layer), $dW^{[l]}$ (derivative with respect to weights), and $db^{[l]}$ (derivative with respect to biases).\\n*   Within this backward function, $dz^{[l]}$ will also be computed.'), CellFormat(cell_type='markdown', cell_no=116, cell_content='### Overall Structure of a Deep Neural Network Training'), CellFormat(cell_type='markdown', cell_no=117, cell_content='If we can implement these two functions (forward and backward for a single layer), the basic computation of the neural network will be as follows:'), CellFormat(cell_type='markdown', cell_no=118, cell_content='**Forward Propagation Step (Left to Right):**'), CellFormat(cell_type='markdown', cell_no=119, cell_content=\"*   Start with $a^{[0]} = x$ (input features).\\n*   Feed $a^{[0]}$ into the first layer's forward function (using $W^{[1]}, b^{[1]}$) to compute $a^{[1]}$. Cache $z^{[1]}$ (and possibly $W^{[1]}, b^{[1]}$).\\n*   Feed $a^{[1]}$ into the second layer's forward function (using $W^{[2]}, b^{[2]}$) to compute $a^{[2]}$. Cache $z^{[2]}$.\\n*   Continue this process through all layers until the final layer $L$, outputting $a^{[L]} = \\\\hat{y}$.\"), CellFormat(cell_type='markdown', cell_no=120, cell_content='**Back Propagation Step (Right to Left):**'), CellFormat(cell_type='markdown', cell_no=121, cell_content=\"*   Start with $da^{[L]}$ (derivative of the loss with respect to $\\\\hat{y}$). \\n*   Feed $da^{[L]}$ and cached values for layer $L$ into the $L$-th layer's backward function to compute $da^{[L-1]}$, $dW^{[L]}$, $db^{[L]}$.\\n*   Feed $da^{[L-1]}$ and cached values for layer $L-1$ into the $(L-1)$-th layer's backward function to compute $da^{[L-2]}$, $dW^{[L-1]}$, $db^{[L-1]}$.\\n*   Continue this process backwards until layer 1, computing $dW^{[1]}, db^{[1]},$ and $da^{[0]}$ (though $da^{[0]}$ is typically not useful for training weights).\"), CellFormat(cell_type='markdown', cell_no=122, cell_content='**Parameter Update:**'), CellFormat(cell_type='markdown', cell_no=123, cell_content='*   After computing all $dW^{[l]}$ and $db^{[l]}$ values:\\n    *   $W^{[l]} = W^{[l]} - \\\\alpha \\\\cdot dW^{[l]}$ \\n    *   $b^{[l]} = b^{[l]} - \\\\alpha \\\\cdot db^{[l]}$ \\n    (where $\\\\alpha$ is the learning rate).'), CellFormat(cell_type='markdown', cell_no=124, cell_content='This entire sequence constitutes one iteration of gradient descent for your neural network.'), CellFormat(cell_type='markdown', cell_no=125, cell_content='The concept of \"cache\" is crucial for passing information ($z^{[l]}$, and for convenience $W^{[l]}, b^{[l]}$) from the forward pass to the corresponding backward pass function. From an implementation standpoint, the cache can store $z^{[l]}$ as well as $W^{[l]}$ and $b^{[l]}$ for convenience.'), CellFormat(cell_type='markdown', cell_no=126, cell_content=\"You've now seen what are the basic building blocks for implementing a deep neural network. In each layer, there's a forward propagation step and a corresponding backward propagation step, and a cache to pass information from one to the other.\"), CellFormat(cell_type='markdown', cell_no=127, cell_content=\"In the next video, we'll talk about how you can actually implement these building blocks.\"), CellFormat(cell_type='markdown', cell_no=128, cell_content='## Lesson 06: Forward and Backward Propagation: Implementation Details'), CellFormat(cell_type='markdown', cell_no=129, cell_content=\"In a previous video, you saw the basic blocks of implementing a deep neural network: a forward propagation step for each layer and a corresponding backward propagation step. Let's see how to actually implement these steps.\"), CellFormat(cell_type='markdown', cell_no=130, cell_content='### Forward Propagation Implementation'), CellFormat(cell_type='markdown', cell_no=131, cell_content='The forward function for layer $l$ takes $A^{[l-1]}$ as input and outputs $A^{[l]}$ and `cache` ($Z^{[l]}$, and optionally $W^{[l]}, b^{[l]}$ for convenience).'), CellFormat(cell_type='markdown', cell_no=132, cell_content='**Equations (Vectorized):**'), CellFormat(cell_type='markdown', cell_no=133, cell_content='$$Z^{[l]} = W^{[l]}A^{[l-1]} + b^{[l]}$$'), CellFormat(cell_type='markdown', cell_no=134, cell_content='$$A^{[l]} = g^{[l]}(Z^{[l]})$$'), CellFormat(cell_type='markdown', cell_no=135, cell_content='Note: Python broadcasting handles adding the $n^{[l]} \\\\times 1$ bias vector $b^{[l]}$ to the $n^{[l]} \\\\times m$ matrix $W^{[l]}A^{[l-1]}$.'), CellFormat(cell_type='markdown', cell_no=136, cell_content='Initialization: The first forward function in the chain takes $A^{[0]} = X$ (input features for one or multiple training examples) as its input.'), CellFormat(cell_type='markdown', cell_no=137, cell_content='Repeating this process from left to right allows you to compute forward propagation across all layers.'), CellFormat(cell_type='markdown', cell_no=138, cell_content='### Backward Propagation Implementation'), CellFormat(cell_type='markdown', cell_no=139, cell_content='The backward function for layer $l$ takes $dA^{[l]}$ as input and outputs $dA^{[l-1]}$, $dW^{[l]}$, and $db^{[l]}$.'), CellFormat(cell_type='markdown', cell_no=140, cell_content='**Equations (Vectorized):**'), CellFormat(cell_type='markdown', cell_no=141, cell_content=\"1. Compute $dZ^{[l]}$:$$dZ^{[l]} = dA^{[l]} * g'^{[l]}(Z^{[l]}) \\\\quad \\\\text{(element-wise product)}$$\"), CellFormat(cell_type='markdown', cell_no=142, cell_content='2. Compute $dW^{[l]}$:$$dW^{[l]} = \\\\frac{1}{m} dZ^{[l]} (A^{[l-1]})^T$$'), CellFormat(cell_type='markdown', cell_no=143, cell_content='3. Compute $db^{[l]}$: '), CellFormat(cell_type='code', cell_no=144, cell_content='db_l = (1 / m) * np.sum(dZ_l, axis=1, keepdims=True)'), CellFormat(cell_type='markdown', cell_no=145, cell_content='4. Compute $dA^{[l-1]}$:$$dA^{[l-1]} = (W^{[l]})^T dZ^{[l]}$$'), CellFormat(cell_type='markdown', cell_no=146, cell_content=\"These four equations are sufficient to implement your backward function for a single layer. These equations were derived in the previous week's content for a single hidden layer and are generalized here.\"), CellFormat(cell_type='markdown', cell_no=147, cell_content='### Initializing Backward Recursion'), CellFormat(cell_type='markdown', cell_no=148, cell_content='The forward recursion is initialized with the input data $X$ (i.e., $A^{[0]}$). How about the backward recursion?'), CellFormat(cell_type='markdown', cell_no=149, cell_content='You need to initialize $dA^{[L]}$ (the derivative of the loss function with respect to the final activation $A^{[L]}$ or $\\\\hat{Y}$). For binary classification with logistic regression (using cross-entropy loss), $dA^{[L]}$ can be shown to be:'), CellFormat(cell_type='markdown', cell_no=150, cell_content='$$dA^{[L]} = -\\\\left(\\\\frac{Y}{A^{[L]}} - \\\\frac{1-Y}{1-A^{[L]}}\\\\right)$$'), CellFormat(cell_type='markdown', cell_no=151, cell_content=\"This formula is for a vectorized implementation where $Y$ and $A^{[L]}$ contain the values for all training examples. You can discard $dA^{[0]}$ as it's not used for training weights.\"), CellFormat(cell_type='markdown', cell_no=152, cell_content='If these equations seem abstract, they will become more concrete when you implement them in the programming assignment. The derivations involve calculus and linear algebra, which can be challenging, but the resulting equations are what you need for implementation.'), CellFormat(cell_type='markdown', cell_no=153, cell_content='Often, a lot of the complexity of machine learning comes from the data rather than the code itself, leading to seemingly magical results even with relatively short implementations. This is how you implement deep neural networks.'), CellFormat(cell_type='markdown', cell_no=154, cell_content='Before moving on, in the next video, we want to discuss hyperparameters and parameters. Being able to organize your hyperparameters well will help you be more efficient in developing your networks.'), CellFormat(cell_type='markdown', cell_no=155, cell_content='## Lesson 07: Parameters vs. Hyperparameters'), CellFormat(cell_type='markdown', cell_no=156, cell_content='Being effective in developing deep neural networks requires organizing not only your parameters but also your hyperparameters.'), CellFormat(cell_type='markdown', cell_no=157, cell_content='### Parameters'), CellFormat(cell_type='markdown', cell_no=158, cell_content='**Parameters** are the values that the learning algorithm learns from the data. These are:'), CellFormat(cell_type='markdown', cell_no=159, cell_content='*   $W$ (weights)\\n*   $b$ (biases)'), CellFormat(cell_type='markdown', cell_no=160, cell_content='### Hyperparameters'), CellFormat(cell_type='markdown', cell_no=161, cell_content='**Hyperparameters** are values that you need to set *before* the learning algorithm starts, which control the learning process and the ultimate values of the parameters ($W$ and $b$). These include:'), CellFormat(cell_type='markdown', cell_no=162, cell_content='*   Learning rate ($\\\\alpha$)\\n*   Number of iterations of gradient descent\\n*   Number of hidden layers ($L$)\\n*   Number of hidden units ($n^{[l]}$) in each layer\\n*   Choice of activation function (e.g., ReLU, tanh, sigmoid) for hidden layers'), CellFormat(cell_type='markdown', cell_no=163, cell_content=\"In later courses, you'll encounter more hyperparameters such as:\"), CellFormat(cell_type='markdown', cell_no=164, cell_content='*   Momentum term\\n*   Mini-batch size\\n*   Regularization parameters (e.g., $\\\\lambda$)'), CellFormat(cell_type='markdown', cell_no=165, cell_content=\"It's important to distinguish parameters from hyperparameters. The learning algorithm tunes parameters, while you, the developer, tune hyperparameters. I will try to be consistent in calling things like $\\\\alpha$, the number of iterations, etc., hyperparameters.\"), CellFormat(cell_type='markdown', cell_no=166, cell_content='### Applying Deep Learning is an Empirical Process'), CellFormat(cell_type='markdown', cell_no=167, cell_content=\"Deep learning involves many hyperparameters, and it's often difficult to know in advance the best values for them. Therefore, applying deep learning is a very **empirical process**, meaning you often have to try out many different values and see what works.\"), CellFormat(cell_type='markdown', cell_no=168, cell_content='**Tuning Process:**'), CellFormat(cell_type='markdown', cell_no=169, cell_content='*   You might have an idea for a learning rate, say $\\\\alpha = 0.01$. Implement and try it out, observe the cost function $J$.\\n*   Based on the outcome, you might change it (e.g., increase to $\\\\alpha = 0.05$).\\n*   You typically plot the cost function vs. iterations to find a learning rate that causes $J$ to decrease smoothly and quickly without diverging.'), CellFormat(cell_type='markdown', cell_no=170, cell_content='**Challenges in Hyperparameter Tuning:**'), CellFormat(cell_type='markdown', cell_no=171, cell_content=\"*   **Problem Specificity:** Intuitions about hyperparameters might not carry over from one application domain (e.g., computer vision) to another (e.g., speech recognition). It's advisable to try out a range of values for new problems.\\n*   **Evolving Best Values:** Even within the same application, the optimal hyperparameter values might change over time (e.g., due to changes in data, compute infrastructure). It's a good practice to periodically re-check hyperparameter values (e.g., every few months).\"), CellFormat(cell_type='markdown', cell_no=172, cell_content=\"This empirical nature can be unsatisfying, but it's an area of ongoing research. For now, it involves systematically exploring the hyperparameter space and evaluating performance on a development set or cross-validation set. In the next course, we'll see some systematic ways for trying out a range of values.\"), CellFormat(cell_type='markdown', cell_no=173, cell_content='## Lesson 08: What Does This Have to Do With the Brain?'), CellFormat(cell_type='markdown', cell_no=174, cell_content=\"So what does deep learning have to do with the brain? At the risk of giving away the punchline, I would say, not a whole lot, but let's take a quick look at why people keep making the analogy between deep learning and the human brain.\"), CellFormat(cell_type='markdown', cell_no=175, cell_content='### Oversimplified Analogy'), CellFormat(cell_type='markdown', cell_no=176, cell_content='When you implement a neural network, you\\'re doing forward propagation and back propagation using mathematical equations. Because it can be difficult to convey the intuitions behind these complex functions, the analogy that \"it\\'s like the brain\" has become an oversimplified, yet seductive, explanation that has captured the popular imagination.'), CellFormat(cell_type='markdown', cell_no=177, cell_content='### Biological Neuron vs. Artificial Neuron'), CellFormat(cell_type='markdown', cell_no=178, cell_content='There is a simplistic analogy between a logistic regression unit (an artificial neuron) and a cartoon of a single biological neuron.'), CellFormat(cell_type='markdown', cell_no=179, cell_content='*   **Biological Neuron:** A cell in your brain receives electrical signals from other neurons (dendrites), does a simple computation, and if it \"fires,\" it sends an electrical pulse down its axon (a long wire) to other neurons.\\n*   **Artificial Neuron:** Receives inputs ($X_1, X_2, X_3$, or $A_1, A_2, A_3$), computes a weighted sum plus bias ($Z$), and applies an activation function ($g(Z)$) to produce an output ($A$).'), CellFormat(cell_type='markdown', cell_no=180, cell_content='However, neuroscientists today have almost no idea what even a single biological neuron is truly doing. A single neuron appears to be much more complex than what we can characterize with current neuroscience.'), CellFormat(cell_type='markdown', cell_no=181, cell_content=\"For example, the exact learning mechanism of neurons in the human brain is still a mystery. It's completely unclear if the brain uses an algorithm like backpropagation or gradient descent, or if there's a fundamentally different learning principle at play.\"), CellFormat(cell_type='markdown', cell_no=182, cell_content='### Deep Learning: Flexible Function Learning'), CellFormat(cell_type='markdown', cell_no=183, cell_content='When I think of deep learning, I think of it as being very good at learning very flexible, complex functions, to learn X to Y mappings, to learn input-output mappings in supervised learning. While the brain analogy was perhaps useful once for intuition or popular appeal, I think the field has moved to the point where that analogy is breaking down, and I tend not to use that analogy much anymore.'), CellFormat(cell_type='markdown', cell_no=184, cell_content='The field of computer vision has taken a bit more inspiration from the human brain than other disciplines that also apply deep learning, but personally, the analogy to the human brain is used less than it used to be.'), CellFormat(cell_type='markdown', cell_no=185, cell_content=\"That's it for this video. You now know how to implement forward prop and back prop and gradient descent even for deep neural networks. Best of luck with the programming exercise, and I look forward to sharing more of these ideas with you in the second course.\")])}]}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "e6b1d3bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content=\"Here are a few appropriate themes, ranging from direct to more evocative:\\n\\n**Simple & Direct:**\\n*   Sunset Serenity\\n*   Lakeside Glow\\n*   Orange Dusk\\n\\n**More Evocative/Poetic:**\\n*   Golden Hour Tranquility\\n*   Dusk's Gentle Embrace\\n*   Still Waters, Warm Light\\n*   A Calm Canvas\\n\\n**My top recommendation would be:**\\n\\n**Sunset Serenity** or **Golden Hour Tranquility**\" additional_kwargs={} response_metadata={'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash', 'safety_ratings': [], 'model_provider': 'google_genai'} id='lc_run--019b3f93-7201-7c71-9b5f-4ebf1219114c-0' usage_metadata={'input_tokens': 28, 'output_tokens': 836, 'total_tokens': 864, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 737}}\n",
      "content='Here are several appropriate themes for an image of a \"Cyberpunk city with neon lights and rain at night,\" ranging from direct to more evocative:\\n\\n1.  **Neon Noir:** (Classic and highly fitting, combining the visual style with the atmospheric mood)\\n2.  **Rain-Slicked Future:** (Direct and paints a clear picture)\\n3.  **Electric Dystopia:** (Highlights the technological aspect and the often grim underlying reality)\\n4.  **Urban Luminescence:** (Focuses on the beauty of the city lights in the dark)\\n5.  **Glow and Grit:** (Emphasizes the contrast between the vibrant lights and the harsh, wet environment)\\n6.  **Cyberpunk Rainscape:** (A descriptive and genre-specific term)\\n7.  **Melancholy Metropolis:** (Suggests a deeper mood of perhaps loneliness or introspection amidst the busy city)' additional_kwargs={} response_metadata={'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash', 'safety_ratings': [], 'model_provider': 'google_genai'} id='lc_run--019b3f93-7202-79f1-95e5-904b6ea21643-0' usage_metadata={'input_tokens': 28, 'output_tokens': 1083, 'total_tokens': 1111, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 894}}\n",
      "content=\"Here are a few appropriate theme suggestions, playing on different aspects of the description:\\n\\n**Focusing on Youth & Dreams:**\\n*   **Childhood Dreams, Mountain Views**\\n*   **Grassroots Aspirations**\\n*   **Where Dreams Begin**\\n*   **Youthful Spirit, Grand Stage**\\n\\n**Focusing on Setting & Simplicity:**\\n*   **Nature's Pitch**\\n*   **Village Vibes, Mountain Majesty**\\n*   **Rustic Rhythms of Play**\\n*   **The Beautiful Game, Simply Played**\\n\\n**More Evocative/Poetic:**\\n*   **Echoes of Joy, Whispers of the Wind**\\n*   **Unfettered Play, Horizon Bound**\\n\\n**My Top Recommendations:**\\n\\n1.  **Childhood Dreams, Mountain Views** (Captures the kid's potential and the grand backdrop)\\n2.  **Nature's Pitch** (Simple, highlights the natural setting for play)\\n3.  **Grassroots Aspirations** (Connects the humble village setting with future potential)\" additional_kwargs={} response_metadata={'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash', 'safety_ratings': [], 'model_provider': 'google_genai'} id='lc_run--019b3f93-7203-71a1-bb20-249fbbc2a73c-0' usage_metadata={'input_tokens': 29, 'output_tokens': 1039, 'total_tokens': 1068, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 814}}\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=\"\"\"Generate an appropriate theme for the given description about an image.\n",
    "Description:\n",
    "{code_content}\n",
    "\"\"\",\n",
    "    input_variables=[\"code_content\"]\n",
    ")\n",
    "\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-2.5-flash\")\n",
    "\n",
    "# Create chain\n",
    "chain = prompt | llm\n",
    "\n",
    "# Multiple inputs -> each becomes one LLM call (in parallel)\n",
    "inputs = [\n",
    "    {\"code_content\": \"A calm sunset over a lake with soft orange light\"},\n",
    "    {\"code_content\": \"Cyberpunk city with neon lights and rain at night\"},\n",
    "    {\"code_content\": \"A kid playing football in a village ground with mountains behind\"},\n",
    "]\n",
    "\n",
    "responses = chain.batch(inputs)\n",
    "\n",
    "for r in responses:\n",
    "    print(r)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40abe349",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "path = \"./transcripts\"\n",
    "folders =  [entry.name for entry in os.scandir(path) if entry.is_dir()]\n",
    "\n",
    "chapters = []\n",
    "for folder in folders:\n",
    "    \n",
    "    chapter_no,chapter_name = folder.split('.',1)\n",
    "    lesson_path = f\"{path}//{folder}\"\n",
    "    lessons = [f for f in os.listdir(lesson_path) if os.path.isfile(os.path.join(lesson_path, f))]\n",
    "\n",
    "    chapter: Chapter = {}\n",
    "    chapter[\"chapter_number\"]=chapter_no\n",
    "    chapter[\"chapter_name\"]=chapter_name\n",
    "    chapter[\"lessons\"] = []\n",
    "    for lesson in lessons:\n",
    "        lesson_no,lesson_name_raw = lesson.split(\"-\",1)\n",
    "        lesson_name,_ = lesson_name_raw.split(\".\",1)\n",
    "        with open(f\"{lesson_path}//{lesson}\",\"r\") as f:\n",
    "            content = f.read()\n",
    "        chapter[\"lessons\"].append({\"lesson_number\":lesson_no,\"lesson_name\":lesson_name,\"content\":content})\n",
    "    chapters.append(chapter)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
